\documentclass{article}
\newcommand{\hwnumber}{1}

\newcommand{\norm}[1]{\| #1 \|}
\newcommand{\abs}[1]{| #1 |}

\usepackage{fullpage,amsthm,amsmath,amssymb}
\usepackage{algorithm,algorithmic}
\usepackage{mathtools}
\usepackage{bbm,bm}
\usepackage{enumerate}
\usepackage{xspace}
\usepackage[textsize=tiny,
%disable
]{todonotes}
\newcommand{\todot}[1]{\todo[color=blue!20!white]{T: #1}}
\newcommand{\todoc}[1]{\todo[color=orange!20!white]{Cs: #1}}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=black]{hyperref}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator*{\Exp}{\mathbf{E}}
\DeclareMathOperator*{\1}{\mathbbm{1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\E}{\mathbb E}
\newcommand{\V}{\mathbb V}
\renewcommand{\P}[1]{P\left\{ #1 \right\}}
\newcommand{\Prob}[1]{\mathbb{P}( #1 )}
\newcommand{\real}{\mathbb{R}}
\renewcommand{\b}[1]{\mathbf{#1}}
\newcommand{\EE}[1]{\E[#1]}
\newcommand{\bfone}{\1}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\cF}{\mathcal{F}}
\usepackage[capitalize]{cleveref}
\usepackage{xifthen}

% total number of points that can be collected
\newcounter{DocPoints} % counter reset to zero upon creation

% counting points per question (not user facing)
\newcounter{QuestionPoints} % counter reset to zero upon creation

% Points for a subquestion of a question; 
% Adds the points to the total for the question and the document.
\newcommand{\points}[1]{%
	\par\mbox{}\par\noindent\hfill {\bf #1 points}%
	\addtocounter{DocPoints}{#1}
	\addtocounter{QuestionPoints}{#1}
}
% Points for a question; call with no params if the question
% had subquestions. In this case it prints the total for the question (see \points).
% Otherwise call with the points that the question is worth.
% In this case, the total is added to the document total.
% It is a semantic error to call this with a non-empty parameter
% when a question had subquestions with individual scores.
\newcommand{\tpoints}[1]{        %
	\ifthenelse{\isempty{#1}}%
	{%
	}%
	{%
		\addtocounter{DocPoints}{#1}
		\addtocounter{QuestionPoints}{#1}
	}													 %
	\par\mbox{}\par\noindent\hfill {Total: \bf \arabic{QuestionPoints}\xspace points}\par\mbox{}\par\hrule\hrule
	\setcounter{QuestionPoints}{0}
}
\newcommand{\tpoint}[1]{
	\tpoints{#1}
}

\theoremstyle{definition}
\newtheorem{question}{Question}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem*{remark*}{Remark}
\newtheorem{solution}{Solution}
\newtheorem*{solution*}{Solution}

\newcommand{\hint}{\noindent \textbf{Hint}:\xspace}

\usepackage{hyperref}

\newcommand{\epssub}{\delta}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\sA}{\mathcal{A}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}


\begin{document}

\begin{center}
{\Large \textbf{CMPUT 605: Theoretical Foundations of Reinforcement Learning, Winter 2023\\ Homework \#\hwnumber}}
\end{center}

\section*{Instructions}
\textbf{Submissions}
You need to submit a single PDF file, named {\tt p0\hwnumber\_<name>.pdf} where {\tt <name>} is your name.
The PDF file should include your typed up solutions (we strongly encourage to use pdf\LaTeX). 
Write your name in the title of your PDF file.
We provide a \LaTeX template that you are encouraged to use.
To submit your PDF file you should send the PDF file via private message to Vlad Tkachuk on Slack before the deadline.

\textbf{Collaboration and sources}
Work on your own. You can consult the problems with your classmates, use books
or web, papers, etc.
Also, the write-up must be your own and you must acknowledge all the
sources (names of people you worked with, books, webpages etc., including class notes.) 
Failure to do so will be considered cheating.  
Identical or similar write-ups will be considered cheating as well.
Students are expected to understand and explain all the steps of their proofs.

\textbf{Scheduling}
Start early: It takes time to solve the problems, as well as to write down the solutions. Most problems should have a short solution (and you can refer to results we have learned about to shorten your solution). Don't repeat calculations that we did in the class unnecessarily.

\vspace{0.3cm}

\textbf{Deadline:} January 29 at 11:55 pm

\newcommand{\cM}{\mathcal{M}}
\newcommand{\nS}{\mathrm{S}}
\newcommand{\nA}{\mathrm{A}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ip}[1]{\langle #1 \rangle}

\section*{Problems}
Unless otherwise stated, for the problem described below all policies, value functions, etc. are for
a discounted, finite MDP $\cM=(\cS,\cA,P,r,\gamma)$. That is, $\cS$ and $\cA$ are finite, $0\le \gamma<1$.
Also, without the loss of generality, $\cS = [\nS]=\{1,\dots,\nS\}$ and $\cA = [\nA]=\{1,\dots,\nA\}$.
Below we use notation introduced in the lecture without redefining it, e.g., $\PP_\mu^\pi$, $\E_\mu^\pi$, $v^\pi$, $v^*$, $T_\pi$, $T$, etc. All these objects are to be understood in the context of the fixed $\cM$.

\begin{question}
Show that for any policy $\pi$ (not necessarily memoryless) and distribution $\mu\in \cM_1(\cS)$ over the states,
 $v^\pi(\mu) = \sum_{s\in \cS} \mu(s) v^{\pi}(s)$.
 
\hint Read the end-notes to Lecture 2. Use the canonical probability space for MDPs and the cylinder sets to show that $\PP_\mu = \sum_{s\in \cS} \mu(s) \PP_s$.
\tpoints{10}
\end{question}
\begin{solution*}
We follow the hint.
Let $((\cS\times\cA )^{\NN},\cF)$ be the canonical probability space
of trajectories
over which the measures $(\PP_\mu^\pi)_{\mu,\pi}$ are defined. 
Fix $\pi$.
We let $\PP_s = \PP_s^\pi$ and let $\PP_\mu = \PP_\mu^\pi$ (suppressing dependence on $\pi$).
The statement will follow from 
\begin{align}
\PP_\mu= \sum_{s\in \cS} \mu(s) \PP_s\,,
\label{eq:decompose}
\end{align}
since if this holds then
\begin{align*}
v^\pi(\mu) 
& = \int R(\omega) \PP_\mu(d\omega)  \tag{\text{definition of $v^\pi(\mu)$}} \\
& = \sum_{s\in \cS} \mu(s) \int R(\omega) \PP_s(d\omega)  \tag{\text{\cref{eq:decompose} and linearity of integrals}}\\
& = \sum_{s\in \cS} \mu(s) v^\pi(s)\,,
\tag{\text{definition of $v^\pi(s)$}}
\end{align*}
where $R(\omega)$ is the return on $\omega\in \Omega$.
To show \cref{eq:decompose} recall that a measure over the product measurable space $(\Omega,\cF)$ is uniquely defined based on what probabilities it assigns to the cylinder sets that take either the form 
\begin{align*}
C   & = \{ s_0 \} \times \{ a_0 \} \times \{ s_1 \} \dots \{ s_t \} \times \Omega\,, \quad \text{or}\\
C' & = \{ s_0 \} \times \{ a_0 \} \times  \{ s_1 \} \dots \{ s_t \} \times \{a_t\}\times \Omega\,.
\end{align*}
By the properties of $\PP_\mu$, 
\begin{align*}
\PP_\mu(C) &= \mu(s_0) \pi_0(a_0|s_0) P_{a_0}(s_0,s_1) \dots P_{a_{t-1}}(s_{t-1},s_t)\,,\\
\PP_\mu(C') &= \mu(s_0) \pi_0(a_0|s_0) P_{a_0}(s_0,s_1) \dots P_{a_{t-1}}(s_{t-1},s_t)
\pi_t(a_t|s_0,a_0,\dots,s_t)
\,,
\end{align*}
Applying this with $\mu = \delta_s$, we get that indeed
\begin{align*}
\PP_\mu(C) &= \sum_s \mu(s) \PP_s(C) \qquad \text{and }\\
\PP_\mu(C') &= \sum_s \mu(s) \PP_s(C') \,.
\end{align*}
Since $C$ and $C'$ were arbitrary cylinder sets of the above form, \cref{eq:decompose} holds.
\qed\par\smallskip\hrule
\end{solution*}

\begin{question}
Recall that for a memoryless policy $\pi$, $P_\pi$ 
is the $\nS \times \nS$ matrix whose $(s,s')$th entry is 
\[
\sum_{a\in \cA} \pi(a|s) P_a(s,s')\,.
\] 
Show that for any $s,s'\in \cS$ and $t\ge 1$, $(P_\pi^t)_{s,s'} = \PP_s^\pi(S_t=s')$.

\hint Use the properties of $\PP_s$ (the tower rule of conditional expectations may be useful, too, especially if you do not want to write a lot).
\tpoints{10}
\end{question}
\begin{solution*}
Fix any $t\ge 0$. Fix also $\pi$ and $s_0\in \cS$. We abbreviate $\PP_{s_0}^\pi$ to $\PP$ in what follows (we changed $s$ to $s_0$ so that there is no clash with indexing of states below while we can reduce clutter).
Detto for $\E_{s_0}^\pi$ and $\mathbb{E}$.
Recall that $H_t = (S_0,A_0,\dots,S_{t-1},A_{t-1},S_t)$.
Fix $s'\in \cS$.
By the tower rule of conditional expectations (applied twice),
\begin{align*}
\PP(S_{t+1}=s')
=\EE{ \EE{ \PP(S_{t+1}=s'|H_t,A_t)  | H_t } }\,.
\end{align*}
For the innermost expectation (probability, actually) we have
\begin{align*}
\PP(S_{t+1}=s'|H_t,A_t) = P_{A_t}(S_t,s')
\end{align*}
by the construction of $\PP$.
Now,
\begin{align*}
\EE{ P_{A_t}(S_t,s') | H_t } = \sum_{a\in \cA} \pi_t(a|H_t) P_a(S_t,s')
\end{align*}
and since $\pi$ is memoryless, $\pi_t(a|H_t) = \pi(a|S_t)$. 
Hence, the expression in the right-hand side is $P_\pi(S_t,s')$. 
Plugging this in, using the law of total expectations,
\begin{align*}
\EE{ P_{\pi}(S_t,s') } = \sum_{s\in \cS} P_{\pi}(s,s') \PP(S_t=s)\,.
\end{align*}
Putting everything together we see that
\begin{align*}
\PP(S_{t+1}=s') = \sum_{s\in \cS} P_{\pi}(s,s') \PP(S_t=s)
\end{align*}
which, together with $\PP(S_0=s)=\delta_{s_0}(s)$ implies the desired statement.
Indeed, for $t=0$ we get
\begin{align*}
\PP(S_{1}=s') = P_{\pi}(s_0,s') = P_{\pi}(s_0,s')\,,
\end{align*}
and hence, by induction,
\begin{align*}
\PP(S_{t+1}=s') = \sum_{s\in \cS}  P_\pi^t(s_0,s) P_{\pi}(s,s')
= P_\pi^{t+1}(s_0,s')\,.
\end{align*}
\qed\par\smallskip\hrule
\end{solution*}

\begin{question}
Prove that for any memoryless policy $\pi$, 
$v^\pi = \sum_{t\ge 0} \gamma^t P_\pi^t r_\pi$.

\hint You may want to reuse the result of the previous exercise.
\tpoints{10}
\end{question}
\begin{solution*}
Let $\PP = \PP_s^\pi$.
By the result of the previous exercise, for $t\ge 1$, $\PP(S_t=s') = P_\pi^t(s,s')$.
By the tower rule and Lebesgue's dominated convergence theorem,
\begin{align*}
v^\pi(s) =\sum_{t\ge 0}\gamma^t  \EE{ \EE{ r_{A_{t}}(S_{t}) | H_t } }\,.
\end{align*}
For the innermost expectation we have
\begin{align*}
\EE{ r_{A_{t}}(S_{t}) | H_t }
= \sum_{a\in \cA} \pi_t(a|H_t) r_a(S_t)
= \sum_{a\in \cA} \pi(a|S_t) r_a(S_t) = r_\pi(S_t)\,,
\end{align*}
because $\pi$ is memoryless.
Plugging this in and using the law of total expectations we get
\begin{align*}
 \EE{ \EE{ r_{A_{t}}(S_{t}) | H_t } }  = \sum_{s'\in \cS} \PP(S_t=s') r_\pi(s')\,.
\end{align*}
Since for $t=0$, $\PP(S_t=s') = 1$ iff $s'=s$, this together with the result of the previous problem gives
that
\begin{align*}
v^\pi(s) = \sum_{t\ge 0} \gamma^t \sum_{s'\in \cS} P_\pi^t(s,s') r_\pi(s')
\end{align*}
(recall that $A^0$ is the identity matrix for any square matrix $A$).
Using a matrix-vector notation we can write the above display as 
\begin{align*}
v^\pi = \sum_{t\ge 0} \gamma^t P_\pi^t r_\pi
\end{align*}
\qed\par\smallskip\hrule
\end{solution*}

\begin{question}
Prove that for any memoryless policy $\pi$, $v^\pi$ is the fixed point of $T_\pi$: $v^\pi = T_\pi v^\pi$.
\tpoints{5}
\end{question}
\begin{solution*}
Fix $s\in \cS$ and $\pi$. From the solution of the previous problem,
\begin{align*}
v^\pi
 = r_\pi + \gamma P_\pi \underbrace{\sum_{t\ge 0} \gamma^t P_\pi^t r_\pi}_{v^\pi} = r_\pi + \gamma P_\pi v^\pi\,,
\end{align*}
finishing the proof. 
\qed\par\smallskip\hrule
\end{solution*}

\begin{question}
Let $w\in (0,\infty)^\nS$ be an $\nS$-dimensional vector whose entries are all positive.
Let $\tilde{v}^*$ be a solution to the optimization problem
\begin{align}
\label{eq:badlp}
\max_{v\in \RR^\nS} w^\top v \qquad \text{s.t.} \qquad v \le T v\,.
\end{align}
Show that $\tilde{v}^*=v^*$. That is, the unique solution to the problem stated in \eqref{eq:badlp} is $v^*$. 
\tpoints{5}
\end{question}

\begin{solution*}
Let $v$ be any feasible point: $v \le T v$. Let $\pi$ be greedy w.r.t. $v$. Hence, $T_\pi v = T v \ge v$ and by induction  on $k\ge 0$, for any $k\ge 0$, $T_\pi^k v \ge v$ and hence $v^\pi \ge v$.
Since $\pi$ was arbitrary memoryless, by the fundamental theorem, $v^* = \sup_{\pi \in \text{ML}} v^\pi \ge v$.

Now, let $v$ be the solution of the optimization problem. Hence, $v$ is feasible and thus $v^*\ge v$.
If there is a state $s_0\in \cS$ such that $v^*(s_0)>v(s_0)$ then $\1^\top v^* > \1^\top v$, 
contradicts that $v$ is an optimal solution since $v^*$ is also a feasible point of the optimization problem.
Therefore $v^*=v$.
\qed\par\smallskip\hrule
\end{solution*}

\begin{question}\label{ex:lp}
Let $w\in (0,\infty)^\nS$ be an $\nS$-dimensional vector whose entries are all positive.
Let $\tilde{v}^*$ be a solution to the optimization problem
\begin{align}
\label{eq:prelp}
\MoveEqLeft
\min_{v\in \RR^\nS} w^\top v \qquad \text{s.t.} \qquad v \ge T v\,.
\end{align}
Show that $\tilde{v}^*=v^*$. That is, the unique solution to the problem stated in \eqref{eq:prelp} is $v^*$.
\tpoints{5}
\end{question}

\begin{solution*}
Let $v$ be any feasible point: $v \ge T v$. Iterating with $T$ we get that $v\ge v^*$.
Now, let $v$ be the solution of the optimization problem. Hence, $v$ is feasible and thus $v\ge v^*$.
If there is a state $s_0\in \cS$ such that $v(s_0)>v^*(s_0)$ then $\1^\top v > \1^\top v^*$, which contradicts that $v$ is an optimal solution since $v^*$ is also a feasible point. 
Therefore $v^*=v$.
\qed\par\smallskip\hrule
\end{solution*}

\begin{question}
A linear program is a constrained optimization problem with a linear objective and linear constraints.
Which of \eqref{eq:badlp} or \eqref{eq:prelp} is equivalent to a linear program? Give the linear program and show the equivalence.
\tpoints{5}
\end{question}

\begin{solution*}
It is \eqref{eq:prelp}.
The linear program takes the form
\begin{align}
\label{eq:lp}
\MoveEqLeft
\min_{v\in \RR^\nS} w^\top v \qquad \text{s.t.} \qquad v \ge T_a v, \qquad a\in \cA\,.
\end{align}
This is equivalent to \eqref{eq:prelp} because
they have the same objective and same feasibility sets.
In particular, if $v$ is feasible for \eqref{eq:prelp} then $v \ge T v \ge T_a v$ for any $a\in \cA$.
Hence, $v \ge T_a v$ holds for all $a\in \cA$ and $v$ is feasible for \eqref{eq:lp}.
In the reverse direction, if $v$ is feasible for \eqref{eq:lp}, $v(s) \ge (T_a v)(s)$ holds for all $a\in \cA$ and $s\in \cS$.
Taking the maximum of both sides over $a$, we get $v(s) \ge \max_{a\in \cA} (T_a v)(s) = (Tv)(s)$ where the last equality used the definitions of $T_a$ and $T$.  Since this holds for any $s\in \cS$, $v \ge T v$, i.e., we get that $v$ is also feasible for \eqref{eq:prelp}.
\qed\par\smallskip\hrule
\end{solution*}


\begin{question}
Show that for any policy $\pi$ and distribution $\mu \in \cM_1(\cS)$ there is a memoryless policy $\pi'$ such that $\nu_\mu^\pi = \nu_\mu^{\pi'}$ (i.e., memoryless policies exhaust the set of all discounted state-action occupancy measures).
\hint For arbitrary $\pi,\mu$, let $\tilde \nu_\mu^\pi(s) = \sum_{a\in \cA} \nu_\mu^\pi(s,a)$.
Define $\pi'(a|s) = \nu_\mu^\pi(s,a)/\tilde \nu_\mu^\pi(s)$ when the denominator is nonzero, and otherwise let $\pi'(\cdot|s)$ be an arbitrary distribution. Show that
$\tilde \nu_\mu^\pi = \mu + \gamma \tilde \nu_\mu^\pi P_{\pi'}$ (treating $\tilde \nu_\mu^\pi$ and $\mu$ as row-vectors) to conclude that 
$\tilde \nu_\mu^\pi = \tilde \nu_\mu^{\pi'}$. To conclude, use the definition of $\pi'$ and that for memoryless policies $\pi''$, $\tilde \nu_\mu^{\pi''}(s) \pi''(a|s) = \nu_\mu^{\pi''}(s,a)$.
\tpoints{15}
\end{question}

\begin{solution*}
For arbitrary $\mu,\pi$,
define
\begin{align*}
\tilde \nu_\mu^\pi(s) = \sum_{a\in \cA} \nu_\mu^\pi(s,a)\,,
\end{align*}
the `marginal' of $\nu_\mu^\pi\in \cM_1(\cS\times \cA)$ over the states.
Clearly, 
\begin{align*}
\tilde \nu_\mu^\pi(s)  = \sum_{t\ge 0} \gamma^t \PP_\mu^\pi(S_t=s)\,.
\end{align*}
When $\pi$ is a memoryless policy, since $\PP_\mu^\pi(S_t=s) = \mu P_\pi^t e_s$, 
\begin{align} \label{eq:discstateocmem}
\tilde \nu_\mu^\pi =  \mu \sum_{t\ge 0} (\gamma P_\pi)^t\,.
\end{align}

Now fix $\mu,\pi$ as in the theorem 
and pick an arbitrary distribution $\pi_0\in \cM_1(\cA)$ over the actions. 
Define $\pi'$ as follows:
\begin{align*}
\pi'(a|s) = 
\begin{cases}
\frac{\nu_\mu^\pi(s,a)}{{\tilde{\nu}}_\mu^\pi(s)}\,, & \text{if } \tilde \nu_\mu^\pi(s)\ne 0\,;\\
\pi_0(a)\,, & \text{otherwise}\,.
\end{cases}
\end{align*}
We will now argue that this policy is indeed suitable. In particular, we will show that 
\begin{align}
\label{eq:statemargeq}
\tilde \nu_\mu^\pi = \mu + \gamma \tilde \nu_\mu^\pi P_{\pi'},
\end{align}
which implies the result since viewing this as a (linear) 
equation in $\tilde \nu_\mu^\pi$, the unique solution to this equation is $\tilde \nu_\mu^{\pi'}$, the discounted occupancy measure of $\pi'$ over the states (see \cref{eq:discstateocmem}).
Thus, $\tilde \nu_\mu^\pi = \tilde\nu_\mu^{\pi'}$ and thus, 
\begin{align*}
\nu^\pi_\mu(s,a) = \tilde \nu_\mu^\pi(s) \pi'(a|s) = \tilde \nu_\mu^{\pi'}(s) \pi'(a|s) = \nu_\mu^{\pi'}(s,a)\,,
\end{align*}
where the last equality follows from the definitions of $\tilde \nu_\mu^{\pi'}$ and $\nu_\mu^\pi$ and the fact that $\pi'$ is memoryless.

It remains to show that \eqref{eq:statemargeq} holds. For this, we have
\begin{align*}
\tilde \nu_\mu^\pi(s)
& = \sum_{t\ge 0} \gamma^t \PP_\mu^\pi(S_t=s) \\
& = \mu(s) + \gamma \sum_{t\ge 0} \gamma^t \PP_\mu^\pi(S_{t+1}=s) \\
& = \mu(s) + \gamma \sum_{s_{\text{prev}},a} \underbrace{\sum_{t\ge 0} \gamma^t \PP_\mu^\pi(S_{t}=s_{\text{prev}},A_t=a)}_{\nu_\mu^\pi(s_{\text{prev}},a)} P_{a}(s_{\text{prev}},s) \\
& = \mu(s) + \gamma \sum_{s_{\text{prev}}} \tilde \nu_\mu^\pi(s_{\text{prev}}) \sum_a \pi'(a|s_{\text{prev}}) P_{a}(s_{\text{prev}},s) \\
& = \mu(s) + \gamma \sum_{s_{\text{prev}}} \tilde \nu_\mu^\pi(s_{\text{prev}}) P_{\pi'}(s_{\text{prev}},s)\,,
\end{align*}
which is equivalent to \eqref{eq:statemargeq}. Here, the last equality follows from 
the definition of $P_{\pi'}$.
\qed\par\smallskip\hrule
\end{solution*}

For the next questions, define the operators
\begin{align*}
P:\RR^\cS \to \RR^{\cS \times\cA}\,, \quad
M:\RR^{\cS \times \cA} \to \RR^{\cS}\,, \quad
M_\pi:\RR^{\cS \times \cA} \to \RR^{\cS}
\end{align*}
via
\begin{align*}
(P v)(s,a) = \ip{P_a(s),v}\,, \qquad 
(M q)(s) = \max_{a\in \cA} q(s,a)\,, \qquad 
(M_\pi q)(s) = \sum_{a\in \cA} \pi(a|s)q(s,a)\,,
\end{align*}
where $(s,a)\in \cS \times \cA$, $v\in \RR^{\cS}$, $q\in \RR^{\cS\times \cA}$
and $\pi$ is an arbitrary memoryless policy.
Further,
let $r\in \RR^{\cS\times \cA}$ be defined by $r(s,a) = r_a(s)$, $(s,a)\in \cS \times \cA$.
It is easy to see that for any $v\in \RR^{\cS}$ the following hold:
\begin{align}
T v & = M(r+\gamma P v)\,, \label{eq:tdec} \\
T_\pi v &= M_\pi (r+\gamma P v)\,. \label{eq:tpidec} 
\end{align}

\par\hrule

\begin{question}
Let $\pi$ be a memoryless policy. Show that $T_\pi$ is a $\gamma$-contraction with respect to the max-norm.
\tpoints{5}
\end{question}

\begin{solution*}
Let $v,v'\in \RR^{\nS}$. By the triangle inequality we
have $|(P_\pi(v-v')) (s)\le \sum_{s'\in \cS} P_\pi(s,s') |v(s)-v(s')| \le \norm{v-v'}_\infty\,  \sum_{s'\in \cS} P_\pi(s,s')  = \norm{v-v'}_\infty$.
Taking the maximum over $s$, $\norm{P_\pi(v-v')}_\infty \le \norm{v-v'}_\infty$.
Finally,
$\norm{T_\pi v - T_\pi v'}_\infty  = \norm{ r_\pi + \gamma P_\pi v - (r_\pi + \gamma P_\pi v') }_\infty 
=\gamma \norm{P_{\pi} (v-v')}_\infty \le \gamma \norm{v-v'}_\infty$.
\qed\par\smallskip\hrule
\end{solution*}

\begin{question}
Show that $M,M_\pi$ and $P$ as defined above are non-expansion when there domains and ranges are equipped with the maximum norm.
That is, show that for all $q,q'\in \R^{\cS \times \cA}$ and $v,v'\in \R^{\cS}$,
\begin{align*}
\norm{M q - M q'}_\infty & \le \norm{q-q'}_\infty\,,\\
\norm{M_\pi q - M_\pi q'}_\infty & \le \norm{q-q'}_\infty\,,\\
\norm{P v - P v'}_\infty & \le \norm{v-v'}_\infty\,.
\end{align*}
\hint To show that $M$ is a non-expansion, consider proving that $|\max_a q(a) - \max_b q'(b) |\le \norm{q-q'}_\infty$ holds for any $q,q'\in \R^{\cA}$.
\tpoints{10}
\end{question}

\begin{solution*}
Let us start with $P$.
This follows because $P$ is a stochastic operator, i.e., that $P \1 = \1$ and $P$ is monotone and linear.
Thus, with $c= \norm{v-v'}_\infty$,
 from $-c \1 \le v-v' \le c \1$, applying $P$ we get $-c \1 \le P(v-v') \le c \1$, which implies that $\norm{P(v-v')}_\infty \le c$.
 
That $M$ is a non-expansion follows by the following elementary argument:
Let $q,q':\cA \to \R$ be arbitrary. We want to prove that 
\begin{align}
|\max_a q(a) - \max_b q'(b) | \le \norm{q-q'}_\infty\,.
\label{eq:max}
\end{align}
If this is proven, we can apply this result
$q(s,\cdot)$ and $q'(s,\cdot)$ and then taking a maximum over $s$ to get that for any $q,q'$, $\norm{Mq - M q'}_\infty \le \norm{q-q'}_\infty$.
To prove \eqref{eq:max} assume without the loss of generality that 
\begin{align}
\max_a q(a) - \max_b q'(b) \ge 0\,.
\label{eq:qass}
\end{align}
Take any $a\in \cA$. Then,
\begin{align*}
|q(a)-q'(a)| \ge q(a) - q'(a) \ge q(a) - \max_b q'(b)\,.
\end{align*}
Taking the maximum of both sides over $a\in \cA$ gives
\begin{align*}
\norm{q-q'}_\infty \ge \max_a q(a) - \max_b q'(b) = |\max_a q(a) - \max_b q'(b)|\,,
\end{align*}
where the equality follows from \eqref{eq:qass}.

The proof of $\norm{M_\pi q - M_\pi q'}_\infty \le \norm{q-q'}_\infty$ follows by using the definition of $M_\pi$, expanding $\norm{M_\pi q - M_\pi q'}_\infty$ and applying a few inequalities as follows:
\begin{align*}
    \norm{M_\pi q - M_\pi q'}_\infty
    &= \max_s \left|\sum_{a \in \cA} \pi(a | s) \left[q(s, a) - q'(s, a)\right]\right| \\ 
    &\le \max_s \sum_{a \in \cA} \pi(a | s) |q(s, a) - q'(s, a)| & \text{Since $\pi(a|s) \geq 0$}\\ 
    &\leq \max_s \sum_{a \in \cA} \pi(a | s) \max_a |q(s, a) - q'(s, a)| \\ 
    &= \max_s \sum_{a \in \cA} \pi(a | s) \max_a |q(s, a) - q'(s, a)| \\ 
    &= \max_s \max_a |q(s, a) - q'(s, a)| \\ 
    &= \norm{q - q'}_\infty 
\end{align*}
\qed\par\smallskip\hrule
\end{solution*}

\if0
\begin{question}
Show that $T$, the Bellman optimality operator, is a $\gamma$-contraction with respect to the max-norm.

\hint Use \eqref{eq:tdec}.
\tpoints{5}
\end{question}

\begin{solution*}
Following the hint we have
\begin{align*}
T v = M( r + \gamma P v)\,, \qquad v\in \RR^{\cS}\,.
\end{align*}
Hence, for $v,v'\in \RR^{\cS}$,
\begin{align*}
\norm{ T v  - T v' }_\infty
&= \norm{ M( r + \gamma P v) - M( r + \gamma P v') }_\infty \\
&\le \norm{ ( r + \gamma P v) - ( r + \gamma P v') }_\infty \\
&= \norm{ \gamma P (v - v' ) }_\infty \\
&\le \gamma \norm{ v - v' }_\infty \,.
\end{align*}
Here, the last inequality follows because $P$ is a stochastic operator, i.e., that $P \1 = \1$ and $P$ is monotone and linear and thus with $c= \norm{v-v'}_\infty$,
 from $-c \1 \le v-v' \le c \1$, applying $P$ we get $-c \1 \le P(v-v') \le c \1$, which implies that $\norm{P(v-v')}_\infty \le c$.
Furthermore, the first inequality follows from the following elementary argument:
Let $q,q':\cA \to \R$ be arbitrary. We want to prove that 
\begin{align}
|\max_a q(a) - \max_b q'(b) | \le \norm{q-q'}_\infty\,.
\label{eq:max}
\end{align}
If this is proven, we can apply this result
$q(s,\cdot)$ and $q'(s,\cdot)$ and then taking a maximum over $s$ to get that for any $q,q'$, $\norm{Mq - M q'}_\infty \le \norm{q-q'}_\infty$.
To prove \eqref{eq:max} assume without the loss of generality that 
\begin{align}
\max_a q(a) - \max_b q'(b) \ge 0\,.
\label{eq:qass}
\end{align}
Take any $a\in \cA$. Then,
\begin{align*}
|q(a)-q'(a)| \ge q(a) - q'(a) \ge q(a) - \max_b q'(b)\,.
\end{align*}
Taking the maximum of both sides over $a\in \cA$ gives
\begin{align*}
\norm{q-q'}_\infty \ge \max_a q(a) - \max_b q'(b) = |\max_a q(a) - \max_b q'(b)|\,,
\end{align*}
where the equality follows from \eqref{eq:qass}.
\qed\par\smallskip\hrule
\end{solution*}
\fi

\begin{question}\label{q:qcontr}
Let $\tilde T: \R^{\cS \times \cA} \to \R^{\cS \times \cA}$ be defined using
$\tilde T q = r+ \gamma P M q$.
Show that $\tilde T$ is a $\gamma$-contraction with respect to the max-norm.
\tpoints{5}
\end{question}

\begin{solution*}
Follows immediately from the previous solutions and the properties of norms.
Defining $v = M q$ and $v' = M q'$ we have
\begin{align*}
    \norm{\tilde T q - \tilde T q'}_\infty
    &= \norm{r + \gamma P M q - r - \gamma P M q'}_\infty \\
    &= \gamma \norm{P v - P v'}_\infty \\
    &\le \gamma \norm{v - v'}_\infty \\
    &= \gamma \norm{M q - M q'}_\infty \\
    &\le \gamma \norm{q - q'}_\infty \\
\end{align*} 
\qed\par\smallskip\hrule
\end{solution*}

\begin{question}\label{q:qv}
Let $q^*$ be the fixed point of $\tilde T$ defined in Question~\ref{q:qcontr}.
Show that $v^* = M q^*$.
\tpoints{8}
\end{question}

\begin{solution*}
Let  $v = M q^*$. 
By the Fundamental Theorem of MDPs and since $T$ is a contraction, $v^*$ is the unique fixed-point of $T$. Hence, it suffices to show that $Tv = v$ also holds.
By definition,
\begin{align*}
q^*=  \tilde T q^* = r+\gamma P v\,,
\end{align*}
where the last equality used the definition of $\tilde T$ and $v$.
Applying $M$ on both sides, we get 
\begin{align*}
v = M q^* = 
M(r+\gamma P v)=T v\,,
\end{align*}
where the last equality used \eqref{eq:tdec}. 
\qed\par\smallskip\hrule
\end{solution*}

\begin{question}
Let $q^*$ be the fixed point of $\tilde T$ as before. Show that $q^* = r+\gamma P v^*$.
\tpoints{5}
\end{question}

\begin{solution*}
By Question~\ref{q:qv}, $v^* = M q^*$.
By the definition of $\tilde T$ and $q^*$, we have
$q^* =\tilde T q^* =  r + \gamma P Mq^* =r + \gamma P v^*$.
\qed\par\smallskip\hrule
\end{solution*}


\begin{question}
Show that if $q^*\in \R^{\cS \times \cA}$ is the fixed-point of $\tilde T$ and if $\pi$ is a memoryless policy that chooses actions maximizing $q^*$ (i.e. $M_\pi q^*=M q^*$) 
then $\pi$ is an optimal policy and any memoryless optimal policy can be found this way.
\tpoints{5}
\end{question}

\begin{solution*}
Let $\pi$ be greedy with respect to $q^*$. Hence, $M_\pi q^* = M q^*$.
We have
\begin{align*}
M_\pi q^* = M q^*\,.
\end{align*}
By the previous question, $q^* = r+\gamma P v^*$. Plugging this in, we get
\begin{align*}
T_\pi v^* = M_\pi (r+\gamma P v^*) = M  (r+\gamma P v^*) = T v^*\,.
\end{align*}
The result follows by the fundamental theorem of MDPs.
\qed\par\smallskip\hrule
\end{solution*}

\begin{question}\label{q:eopt}
Let $\pi$ be a memoryless policy and $\epsilon>0$. 
Call $\pi$ \emph{$\epsilon$-optimizing} $M_\pi q^* \ge v^* - \epsilon \1$.
Show that if $\pi$ is $\epsilon$-optimizing then $\pi$ is $\epsilon/(1-\gamma)$-optimal,
that is, $v^\pi \ge v^* - \frac{\epsilon}{1-\gamma} \1$. % \todoc{tightness?}
\tpoints{10}
\end{question}

\begin{solution*}
We have
\begin{align*}
0 
 \le v^* - v^\pi 
& \le M_\pi q^* - v^\pi  + \epsilon \1
=  (r_\pi + \gamma P_\pi v^*) - (r_\pi + \gamma P_\pi v^\pi)+ \epsilon \1\\
& = \gamma P_\pi(v^*-v^\pi)+ \epsilon \1\,, \numberthis
\label{eq:subbasic}
\end{align*}
where the first inequality uses the assumption on $\pi$, the second uses that $q^* = r+ \gamma P v^*$ and that $M_\pi$ is linear and $M_\pi r = r_\pi$ and $M_\pi P = P_\pi$, by definition. The last equality follows by algebra.
Let $\Delta = \norm{v^* - v^\pi}_\infty$.
Now, taking the absolute value of both sides in \cref{eq:subbasic} and then the maximum of both sides with respect to the states, using that $P_\pi$ is stochastic hence $\norm{P_\pi v }_\infty \le \norm{v}_\infty$,  we get that 
\begin{align*}
\Delta \le \gamma \Delta + \epsilon\,.
\end{align*}
Solving for $\Delta$ then gives the result.



\qed\par\smallskip\hrule
\end{solution*}

\begin{question}
Show that if $q\in \R^{\cS\times \cA}$ is such that $\norm{q-q^*}_{\infty}\le \epsilon$ and $\pi$ is greedy with respect to $q$ (i.e., $M_\pi q = Mq$)
then $\pi$ is $2\epsilon/(1-\gamma)$ optimal.

\hint Aim for reusing the answer to Question~\ref{q:eopt}.
\tpoints{5}
\end{question}

\begin{solution*}
Following the hint, we want to show that $M_\pi q^* \ge v^* - 2\epsilon \1$.
We have
\begin{align*}
M_\pi q^* \ge 
M_\pi (q - \epsilon \1)=
M_\pi q - \epsilon \1 = M q - \epsilon \1 \ge M(q^*-\epsilon\1) - \epsilon \1
\ge M q^* - 2 \epsilon \1\,,
\end{align*}
where we used that $M_\pi$ is linear, and $M(v+c \1) = M v + c\1$ by its definition ($c\in \R$) and that $-\epsilon \1 \le q^* - q \le \epsilon \1$.
The result then follows from the claim made in Question~\ref{q:eopt}.
\qed\par\smallskip\hrule
\end{solution*}

\begin{question}
Let $\pi$ be a memoryless policy that selects $\epsilon$-optimal actions with probability at least $1-\zeta$ in each state (i.e., $\sum_{a: q^*(s,a)\ge v^*(s)-\epsilon} \pi(a|s) \ge 1-\zeta$). Show that $\pi$ is at least $(\epsilon + 2\zeta \norm{q^*}_\infty)/(1-\gamma)$ optimal. 
Only assume that the reward is deterministic and bounded (i.e. do not assume it is in $[0, 1]$).
\hint Aim for showing first that $\pi$ is $(\epsilon+2\zeta \norm{q^*}_{\infty})$-optimizing.
\tpoints{5}
\end{question}

\begin{solution*}
Fix $s\in \cS$.
Let $\cA(s,\epsilon) = \{ a\in \cA \,:\, q^*(s,a)\ge v^*(s) - \epsilon \}$ be the set of $\epsilon$-optimal actions in state $s$.
By our assumption, $\sum_{a\in \cA(s,\epsilon)} \pi(a|s) \ge 1-\zeta$ and, conversely, 
$\sum_{a\in \cA \setminus \cA(s,\epsilon)} \pi(a|s)
=1- \sum_{a\in \cA(s,\epsilon)} \pi(a|s)
\le 1-(1-\zeta)=\zeta$.
Thus,
\begin{align*}
(M_\pi q^*)(s) 
%& = \sum_{a\in \cA} \pi(a|s) q^*(s,a)\\
& = \sum_{a\in \cA(s,\epsilon)} \pi(a|s) q^*(s,a) + \sum_{a\not\in \cA(s,\epsilon)} \pi(a,s) q^*(s,a)\\
& \ge \sum_{a\in \cA(s,\epsilon)} \pi(a|s) (v^*(s)-\epsilon) - \norm{q^*}_\infty \sum_{a\not\in \cA(s,\epsilon)} \pi(a,s)\\
& = (v^*(s)-\epsilon) \sum_{a\in \cA(s,\epsilon)} \pi(a|s)  - \norm{q^*}_\infty \sum_{a\not\in \cA(s,\epsilon)} \pi(a,s)\\
& = (v^*(s)-\epsilon)\left( \sum_{a\in \cA} \pi(a|s) 
	-  \sum_{a\not\in \cA(s,\epsilon)} \pi(a,s)\right)
	 - \norm{q^*}_\infty \sum_{a\not\in \cA(s,\epsilon)} \pi(a,s)\\
& = v^*(s)-\epsilon
	-  (v^*(s)-\epsilon+\norm{q^*}_\infty) \sum_{a\not\in \cA(s,\epsilon)} \pi(a,s)\\
& \ge v^*(s)-\epsilon
	-  2\norm{q^*}_\infty \sum_{a\not\in \cA(s,\epsilon)} \pi(a,s)\\
& \ge v^*(s)-\epsilon
	-  2\norm{q^*}_\infty \zeta\,.
\end{align*}
The result then follows from the claim made in Question~\ref{q:eopt}.
\qed\par\smallskip\hrule
\end{solution*}


\bigskip
\bigskip

\noindent
\textbf{
Total for all questions: \arabic{DocPoints}}.
Of this, 23 are bonus marks. 
Your assignment will be marked out of 100.
% Of this, up to 23 can be bonus marks You can receive bonus marks by asking/upvoting questions, for a total of 23 bonus marks! You must ask at least one question in one of the Lecture Discussion Threads by the Assignment 1 deadline to receive 13 bonus marks. You can also receive 2 bonus marks for upvoting at least one question before 8am on the day of each lecture, for a maximum of 2 marks x 5 lectures = 10 marks for upvoting.
% Your assignment will be marked out of \arabic{DocPoints} minus the bonus marks you received.


\end{document}




