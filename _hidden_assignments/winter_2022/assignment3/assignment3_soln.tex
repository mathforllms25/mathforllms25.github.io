% !TEX encoding = UTF-8 Unicode
\documentclass{article}
\newcommand{\hwnumber}{3}

\newcommand{\norm}[1]{\| #1 \|}
\newcommand{\abs}[1]{| #1 |}


\usepackage{fullpage,amsthm,amsmath,amssymb}
\usepackage{algorithm,algorithmic}
\usepackage{mathtools}
\usepackage{bbm,bm}
\usepackage{enumerate}
\usepackage{cancel}
\usepackage{xspace}
\usepackage[textsize=tiny,
%disable
]{todonotes}
\newcommand{\todot}[1]{\todo[color=blue!20!white]{T: #1}}
\newcommand{\todoc}[1]{\todo[color=orange!20!white]{Cs: #1}}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=black]{hyperref}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\usepackage[capitalize]{cleveref}


\usepackage{comment}

\newcommand{\R}{\mathbb{R}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\cX}{\mathcal{X}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator*{\Exp}{\mathbf{E}}
\DeclareMathOperator*{\1}{\mathbbm{1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\E}{\mathbb E}
\newcommand{\bbP}{\mathbb P}
\newcommand{\V}{\mathbb V}
\renewcommand{\P}[1]{P\left\{ #1 \right\}}
\newcommand{\Prob}[1]{\mathbb{P}( #1 )}
\newcommand{\real}{\mathbb{R}}
\renewcommand{\b}[1]{\mathbf{#1}}
\newcommand{\EE}[1]{\E[#1]}
\newcommand{\bfone}{\1}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\0}{\mathbf{0}}
\newcommand{\one}[1]{\mathbb{I}\{#1\}}
\usepackage{xifthen}

% total number of points that can be collected
\newcounter{DocPoints} % counter reset to zero upon creation

% counting points per question (not user facing)
\newcounter{QuestionPoints} % counter reset to zero upon creation

% Points for a subquestion of a question;
% Adds the points to the total for the question and the document.
\newcommand{\points}[1]{%
	\par\mbox{}\par\noindent\hfill {\bf #1 points}%
	\addtocounter{DocPoints}{#1}
	\addtocounter{QuestionPoints}{#1}
}
% Points for a question; call with no params if the question
% had subquestions. In this case it prints the total for the question (see \points).
% Otherwise call with the points that the question is worth.
% In this case, the total is added to the document total.
% It is a semantic error to call this with a non-empty parameter
% when a question had subquestions with individual scores.
\newcommand{\tpoints}[1]{        %
	\ifthenelse{\isempty{#1}}%
	{%
	}%
	{%
		\addtocounter{DocPoints}{#1}
		\addtocounter{QuestionPoints}{#1}
	}													 %
	\par\mbox{}\par\noindent\hfill {Total: \bf \arabic{QuestionPoints}\xspace points}\par\mbox{}\par\hrule\hrule
	\setcounter{QuestionPoints}{0}
}
\newcommand{\tpoint}[1]{
	\tpoints{#1}
}

\theoremstyle{definition}
\newtheorem{question}{Question}
\newtheorem{assumption}{Assumption}
\newtheorem*{assumption*}{Assumption}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem*{remark*}{Remark}
\newtheorem{solution}{Solution}
\newtheorem*{solution*}{Solution}

\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\excludecomment{solution}
%\excludecomment{solution*}

\newcommand{\hint}{\noindent \textbf{Hint}:\xspace}


\usepackage{hyperref}

\newcommand{\epssub}{\delta}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\sA}{\mathcal{A}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}


\begin{document}

\begin{center}
{\Large \textbf{CMPUT 605: Theoretical Foundations of Reinforcement Learning, Winter 2023\\ Homework \#\hwnumber}}
\end{center}

\section*{Instructions}
\textbf{Submissions}
You need to submit a single PDF file, named {\tt p0\hwnumber\_<name>.pdf} where {\tt <name>} is your name.
The PDF file should include your typed up solutions (we strongly encourage to use pdf\LaTeX). 
Write your name in the title of your PDF file.
We provide a \LaTeX template that you are encouraged to use.
To submit your PDF file you should send the PDF file via private message to Vlad Tkachuk on Slack before the deadline.

\textbf{Collaboration and sources}
Work on your own. You can consult the problems with your classmates, use books
or web, papers, etc.
Also, the write-up must be your own and you must acknowledge all the
sources (names of people you worked with, books, webpages etc., including class notes.)
Failure to do so will be considered cheating.
Identical or similar write-ups will be considered cheating as well.
Students are expected to understand and explain all the steps of their proofs.

\textbf{Scheduling}
Start early: It takes time to solve the problems, as well as to write down the solutions. Most problems should have a short solution (and you can refer to results we have learned about to shorten your solution). Don't repeat calculations that we did in the class unnecessarily.

\vspace{0.3cm}

\textbf{Deadline:} March 12 at 11:55 pm

\newcommand{\cM}{\mathcal{M}}
\newcommand{\nS}{\mathrm{S}}
\newcommand{\nA}{\mathrm{A}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ip}[1]{\langle #1 \rangle}

\section*{Tightness of performance bounds of greedy policies}

Error bounds for greedy policies are at the heart of many of the upper bounds we obtained.
Here you will be asked to show that these bounds are unimprovable.
For example, in
\href{http://rltheory.github.io/lecture-notes/planning-in-mdps/lec6/}{Lecture 6},
the following is stated in
Part II of the ``Policy error bound - I.'' lemma:
\begin{lemma}
Let $\pi$ be a memoryless policy and choose a function $q:\mathcal{S}\times\mathcal{A} \to \mathbb{R}$ and $\epsilon\ge 0$. Then,
if $\pi$ is greedy with respect to $q$ then
\begin{align*}
v^\pi \ge v^* - \frac{2\|q-q^*\|_\infty}{1-\gamma} \boldsymbol{1}\,.
\end{align*}
\end{lemma}
The first problem is to show that this bound is tight:
\begin{question}
Show that for any $\gamma\in [0,1)$ and $\varepsilon>0$ there is a finite
discounted MDP $M=(\cS,\cA,P,r,\gamma)$ and $q:\cS \times \cA \to \R$ such that the following hold:
\begin{enumerate}
\item $\norm{q-q^*}_\infty  =\varepsilon$;
\item There is policy $\pi$ that is greedy with respect to $q$ such that $\|v^\pi-v^*\|_\infty = \frac{2\varepsilon}{1-\gamma}$.
\end{enumerate}
\tpoints{10}
\end{question}
\begin{solution*}
% Source: Ronald Williams, Leemon C. Baird. 1993. “Tight Performance Bounds on Greedy Policies Based on Imperfect Value Functions.” http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.3281.
The MDP will have a single state, call it $s$ (i.e., $\cS = \{s\}$) and two actions, say, $\cA = \{1,2\}$.
Since there is only a single state, of course
$p_a(s|s)=1$ for both actions $a$.
Let
\begin{align*}
0\le r_1(s)\le r_2(s)=1\,.
\end{align*}
Then
\begin{align*}
v^*(s) = \frac{1}{1-\gamma}\,.
\end{align*}
For the policy $\pi$ that chooses action $1$ in state $s$, we have
\begin{align*}
v^\pi(s)=\frac{r_1(s)}{1-\gamma}\,.
\end{align*}
Hence,
\begin{align*}
\norm{v^\pi-v^*}_\infty=\frac{1-r_1(s)}{1-\gamma}
\end{align*}
and thus
$\norm{v^\pi-v^*}_\infty=\frac{2\varepsilon}{1-\gamma}$
if $r_1(s) = 1-2\varepsilon$.
Hence, choose this for the value of $r_1(s)$.
Thus,
\begin{align*}
q^*(s,1) &= r_1(s) + \gamma \frac{1}{1-\gamma} = \frac{1}{1-\gamma} - 2 \varepsilon\,,\\
q^*(s,2) &= \frac{1}{1-\gamma}
\end{align*}
and hence if
\begin{align*}
q(s,1) = q^*(s,1)+\varepsilon \text{ and }
q(s,2) = q^*(s,2)-\varepsilon
\end{align*}
then $q(s,1)=q(s,2) = \frac{1}{1-\gamma}-\epsilon$, $\norm{q-q^*}_\infty = \varepsilon$
and policy $\pi$ is greedy with respect to $q$ (because any policy is greedy with respect to $q$ as it assigns the same value for both actions). By our previous argument, $\norm{v^\pi-v^*}_\infty = 2\varepsilon/(1-\gamma)$, thus, finishing the proof.
\qed\par\smallskip\hrule
\end{solution*}

\section*{Average vs. mixed policies}
Fix policies $\pi^{(1)},\dots,\pi^{(k)}$ of some finite discounted MDP $M=(\cS,\cA,P,r,\gamma)$.
There are two ways of combining these policies with
some weights $\alpha\in \cM_1([k])$.
The first way is to choose one of the policies at random from the multinomial parameterized by $\alpha$
and then follow the resulting policy for all the time steps.
Formally, one would choose
an index $I\in [k]$ at random such that $\Prob{I=i} = \alpha_i$
and then follow the policy $\pi^{(I)}$ for whichever state one encounters.
The second way is to choose the policy to follow at random in each time step.
Call the policy that is obtained following the first method the ($\alpha$-weighted) \textbf{mixture of $\pi^{(1)},\dots,\pi^{(k)}$}.
Call the policy that is obtained following the second method the ($\alpha$-weighted)
\textbf{average of $\pi^{(1)},\dots,\pi^{(k)}$}.

Intuitively,
a distribution $\mu\in \cM_1(\cS)$ over the states and
the interconnection of a mixture policy and $M$ gives rise to a probability space $(\Omega,\cF,\PP)$ that carries the random elements
$I,S_0,A_0,S_1,A_1,\dots$ with $I\in [k]$, $S_t\in \cS$ and $A_t\in \cA$ for $t\ge 0$ and such that
for $H_t = (S_0,A_0,S_1,\dots,A_{t-1},S_t)$,
\begin{enumerate}
\item  $\mathbb{P}(S_0 = s|I) = \mu(s)$ for all $s \in \mathcal{S}$,
\item  $\mathbb{P}(A_t = a | I,H_t)
			= \pi^{(I)}_t(a | H_t)$ for all $a \in \mathcal{A}, t \geq 0$,
\item  $\mathbb{P}(S_{t+1} = s' | I, H_t, A_t) = P_{A_t}(S_t, s')$ for all $s' \in \mathcal{S}$, and
\item $\PP(I=i)=\alpha_i$ for all $i\in [k]$.
\end{enumerate}
Note that all first three criteria are modified to express that the laws that govern $S_0$, the action distribution and the next state distribution are as before even when conditioning on $I$.
A new, fourth criterion is added that expresses that
the distribution of $I$ follows the multinomial distribution with parameter $\alpha$.
That the probability distribution $\PP$ with the above properties
exists is guaranteed again by the Ianescu-Tulcea theorem.
As usual, when needed, we use $\PP_\mu$ to indicate the dependence of $\PP$ on $\mu$.

\newcommand{\N}{\mathbb{N}}
\newcommand{\cG}{\mathcal{G}}

Finally some notation:
For a probability measure $\PP$ on a measurable space $(\Omega,\cF)$ and a sub-sigma algebra $\cG$ of $\cF$, let $\PP|_{\cG}$ be the probability measure on $(\Omega,\cG)$ obtained from $\PP$ by restricting it to $\cG$: $\PP|_{\cG}(U) = \PP(U)$ for any $U\in \cG$.

\begin{question}
Unless otherwise specified let
 $\pi^{(1)},\dots,\pi^{(k)}$ be arbitrary policies of $M$ and let $\alpha\in \cM_1([k])$, $\mu \in \cM_1(\cS)$ be also arbitrary.
 Also, let $(\Omega,\cF,\PP)$ as above (we shall also use $\PP_\mu$ when the dependence on $\mu$ is important).
 Let
 $Z = (S_0,A_0,S_1,A_1,\dots)$.
Show that the following hold:
\begin{enumerate}
\item $Z$ is random element between $(\Omega,\cF)$ and $((\cS\times \cA)^{\N},\cG')$ where $\cG'$ is the product $\sigma$-algebra on $(\cS\times \cA)^{\N}$
induced by the discrete topology on $\cS \times \cA$.
\points{5}
\item
\label{q1:a3:1}
Show that there is a policy $\bar \pi$ of the MDP $M$ such that for any $\mu \in \cM_1(\cS)$,
the pushforward of $\PP_\mu$ under $Z$, $(\PP_\mu)_Z$ satisfies
\[
(\PP_\mu)_Z =\PP_\mu^{\bar \pi}
\]
where $\PP_\mu^{\bar \pi}$ is the unique probability measure on
the canonical space $((\cS\times \cA)^{\N},\cG')$
induced
by the interconnection of  $\bar \pi$ and the MDP, given the initial state distribution $\mu$.
That is, a mixture policy induces a policy $\bar \pi$ of the MDP $M$.
\points{20}
\item
Let $R=\sum_{t=0}^\infty \gamma^t r_{A_t}(S_t)$ and
let $\PP$ be as above with the choice $\mu= \delta_s$. Let $\E$ be the expectation operator
corresponding to $\PP$.
Show that $v(s)=\E[R]$ is well-defined:
That is,
for any $(\Omega,\cF,\PP)$ and $(\Omega,\cF,\PP')$ as long as $\PP$ and $\PP'$ satisfy the above four properties, $\E[R]=\E'[R]$ where $\E'$ is the expectation operator underlying $\PP'$.
\points{10}
\item
Show that $v(s) = v^{\bar \pi}(s)$.
\points{5}
\item Let $\PP_\mu^{\pi^{(i)}}$ ($\PP_{\mu}^{\bar \pi}$) be the
probability measures induced on the canonical space
$((\cS \times \cA)^{\N},\cG')$ by the initial state distribution $\mu$ and the interconnection of
$\pi^{(i)}$ (respectively, $\bar \pi$) with the MDP $M$. Show that
$\PP_{\mu}^{\bar \pi} = \sum_{i=1}^k \alpha_i \PP_{\mu}^{\pi^{(i)}}$.
\points{10}
\item Mixing is guaranteed to keep performance bounds:
if for some
$v:\cS \to \R$ and for all $i\in [k]$,
$v^{\pi^{(i)}}\ge v$ then $v^{\bar \pi}\ge v$.
\points{5}
\item Averaging is not guaranteed to keep performance bounds:
For any $\gamma>1/2$
there exists an MDP with state space $\cS$, $k\ge 2$, policies $\pi_1,\dots,\pi_k$, a function $v:\cS \to \R$ and $\alpha\in \cM_1([k])$ such that $v^{\pi_i}\ge v$ holds for all $i\in [k]$, yet if $\pi$ is the $\alpha$-average of $\pi_1,\dots,\pi_k$ then $v^\pi<v$.
\points{10}
\item The state-wise uniform average of all deterministic ML policies and the uniform mixture of all deterministic ML policies both give the policy that is uniform over all the actions.
\points{5}
\end{enumerate}
\hint
Recall the change-of-variables formula:
For a random element $X$ taking values in some measurable set $\cX$,
the pushforward $\PP_X$ of $X$ satisfies
\begin{align*}
\EE{ f(X) } = \int f(x) \PP_X(dx)\,.
\end{align*}
%Read the \href{https://en.wikipedia.org/wiki/Ionescu-Tulcea_theorem}{Ionescu-Tulcea theorem}. The uniqueness part should help.
Recall also that integration is linear in measures.
In particular,
for any measures $\PP_i$ and nonnegative coefficients $\alpha_i$, $i\in[k]$
and $f$ which is $(\sum_{i=1}^k \alpha \PP_i)$-integrable,
 $\int f d(\sum_{i=1}^k \alpha \PP_i) = \sum_{i=1}^k \alpha_i \int f d\PP_i$
 (this also extends to signed measures, but we won't need this extension).
\tpoints{}
\end{question}

\begin{solution*}
Let $s,a,s_0,a_0,s_1,a_1,\dots$ be an arbitrary sequence of state-actions pairs.
\begin{enumerate}
\item We need to check that for $U\in \cG'$, $Z^{-1}(U)\in \cF$.
Since $\cG'$ is a product $\sigma$-algebra, it suffices to check this for the ``simple'' cylinder sets, i.e., when $U$
is either of the form
\begin{align*}
C   & = \{ s_0 \} \times \{ a_0 \} \times \{ s_1 \} \dots \{ s_t \} \times \Omega\,, \quad \text{or, of the form}\\
C' & = \{ s_0 \} \times \{ a_0 \} \times  \{ s_1 \} \dots \{ s_t \} \times \{a_t\}\times \Omega\,.
\end{align*}
For the first case, $Z^{-1}(C) = \{S_0=s_0,A_0=a_0,S_1=s_1,\dots,S_t=s_t\}$, which is in $\cF$ because $S_0,\dots,S_t$ and $A_0,\dots,A_{t-1}$ are $\cF$-measurable.
The same holds for the second case for identical reasons, just add that $A_t$ is also $\cF$-measurable.
In this case,
$Z^{-1}(C') = \{S_0=s_0,A_0=a_0,S_1=s_1,\dots,S_t=s_t,A_t=a_t\}$.
\item
Fix $\mu$ and let $\PP=\PP_\mu$.
We show that $\PP$ satisfies the criteria that define
the probability measure $\PP_{\mu}^{\bar \pi}$ with a suitable policy $\bar \pi$.
It follows that $\PP_Z$ also satisfies these criteria (because the criteria are concerned with events in $\sigma(Z)$).
Hence, $\PP_Z = \PP_{\mu}^{\bar \pi}$ follows
by the uniqueness of the canonical probability space.
Fix any $t\ge 0$.
For the first criterion, by the tower rule,
\begin{align*}
\Prob{S_0=s} = \EE{ \Prob{S_0=s|I} } = \EE{ \mu(s) } = \mu(s)\,.
\end{align*}
The second criterion will be verified by defining $\bar \pi_t$ as
\begin{align*}
\bar \pi_t(a|h_t)=
\begin{cases}
\PP(A_t=a|H_t=h_t)\,, & \text{if } \PP(H_t=h_t)>0\,;\\
\pi_0(a)\,, & \text{otherwise}\,,
\end{cases}
\end{align*}
where $h_t = (s_0,a_0,\dots,a_{t-1},s_t)$ is arbitrary and $\pi_0$ is an arbitrary distribution over the actions.
This indeed defines a policy: $\bar \pi = (\bar \pi_t)$; $\bar \pi_t$ maps histories to distributions. Indeed,
this is clear when $\PP(H_t=h_t)=0$. Otherwise,
\begin{align*}
\MoveEqLeft\sum_{a\in \cA} \bar \pi_t(a|h_t) \\
&= \sum_{a\in \cA} \PP(A_t=a|H_t=h_t) \\
&=\PP(A_t\in \cA|H_t=h_t) =1\,.
\end{align*}
We now claim that $\bar \pi_t$ is independent of $\mu$ ($\PP$ hides its dependence on $\mu$).
Again, this is clear when $\PP(H_t = h_t)=0$ since $\pi_0$ does not depend on $\mu$.
When $\PP(H_t=h_t)>0$ we have
\begin{align*}
\bar\pi_t(a|h_t)
&= \Prob{A_t=a|H_t=h_t}\\
&= \sum_i \Prob{A_t=a|H_t=h_t,I=i} \Prob{I=i|H_t=h_t}
=
\sum_i \pi^{(i)}_t(a|h_t) \Prob{I=i|H_t=h_t}\,,
\end{align*}
where the last equality follows because if $\PP(H_t=h_t,I=i)=0$ then, by definition,
$\PP(I=i|H_t=h_t)=0$, and hence
$\Prob{A_t=a|H_t=h_t,I=i} \Prob{I=i|H_t=h_t} = 0 = \pi^{(i)}_t(a|h_t) \Prob{I=i|H_t=h_t}$.

It remains to show that $ \Prob{I=i|H_t=h_t}$ does not depend on $\mu$.
Again, this is clear when $\Prob{H_t=h_t}=0$ since in this case $ \Prob{I=i|H_t=h_t}=0$.
For the case when $ \Prob{I=i|H_t=h_t}>0$, we have
\begin{align*}
\Prob{I=i|H_t=h_t} = \frac{\Prob{H_t=h_t,I=i}}{\Prob{H_t=h_t}}\,.
\end{align*}
Based on the properties of $\PP$, with repeated conditioning, we calculate,
\begin{equation}
\begin{split}
\Prob{H_t=h_t,I=i}
&= \alpha_i\mu(s_0) \, \pi_0^{(i)}(a_0|s_0) \pi_1^{(i)}(a_1|s_0,a_0,s_1) \dots
\pi_{t-1}^{(i)}(a_{t-1}|s_0,a_0,\dots,s_{t-1}) \,\, \times \\
& \qquad  \qquad \,\,\, P_{a_0}(s_0,s_1) \dots P_{a_{t-1}}(s_{t-1},s_t)\,.
\end{split}
\label{eq:prodp}
\end{equation}
Hence,
\begin{align*}
\MoveEqLeft \Prob{I=i|H_t=h_t} =\\
& \frac{
\pi_0^{(i)}(a_0|s_0) \pi_1^{(i)}(a_1|s_0,a_0,s_1) \dots
\pi_{t-1}^{(i)}(a_{t-1}|s_0,a_0,\dots,s_{t-1}) \,\,
\cancel{\mu(s_0)P_{a_0}(s_0,s_1) \dots P_{a_{t-1}}(s_{t-1},s_t)}
}{
\sum_i \alpha_i\pi_0^{(i)}(a_0|s_0) \pi_1^{(i)}(a_1|s_0,a_0,s_1) \dots
\pi_{t-1}^{(i)}(a_{t-1}|s_0,a_0,\dots,s_{t-1}) \,\,
\cancel{\mu(s_0)P_{a_0}(s_0,s_1) \dots P_{a_{t-1}}(s_{t-1},s_t)}
}\,,
\end{align*}
which is independent of $\mu$ as required.

For the third criterion,
we have
\begin{align*}
\MoveEqLeft
\Prob{S_{t+1}=s|H_t, A_t}
 =
\EE{ \Prob{S_{t+1}=s|H_t, A_t, I} |H_t, A_t}
 =
\EE{ P_{A_t}(S_t,s)  |H_t, A_t}
 =
P_{A_t}(S_t,s)\,,
\end{align*}
where the first equality uses the tower rule, the second uses Property 2 of $\PP$, the third uses that $P_{A_t}(S_t,s)$ is a constant given $H_t, A_t$, hence it can be moved outside of the expectation (formally, $P_{A_t}(S_t,s)$ is $\sigma(H_t \times A_t)$ measurable).
Hence $\PP$ satisfies the three criteria of measures induced by the interconnection of $\bar \pi$, the MDP $M$ and the initial distribution $\mu$, finishing the proof.

\item
Noting that $R=f(Z)$ where $f$ is defined via
$f(s_0,a_0,s_1,a_1,\dots) = \sum_{t=0}^{\infty} \gamma^t r_{a_t}(s_t)$ is a measurable function from $((\cS\times\cA)^{\N}, \cG')$ to $(\R,\mathfrak{B}(\R))$,
it suffices to show that $\PP_Z = \PP'_Z$ because then, by the change-of-variables-formula,
\begin{align*}
\EE{R}=\EE{f(Z)}=\int f(z) \PP_Z(dz) = \int f(z) \PP_Z'(dz) = \E'[f(Z)] = \E'[R]\,.
\end{align*}
Now, for $U\in \cG'$ we have
\begin{align*}
\PP_Z(U) = \PP(Z\in U) = \PP'(Z\in U) = \PP'_Z(U)\,,
\end{align*}
where the second equality follows because, as it can be easily seen, equality here holds
for all simple cylinder sets $U$, hence $\PP_Z=\PP'_Z$ also holds and the proof is finished.

\item
By Part~\ref{q1:a3:1},  $\PP_Z = \PP_s^{\bar \pi}$.
Then, with $f$ as above,
\begin{align*}
v(s) = \int f(z) \PP_Z(dz) = \int f(z) \PP_s^{\bar \pi}(dz) = v^{\bar \pi}(s)\,.
\end{align*}

\item
By \cref{eq:prodp} and the construction of $\PP_{\mu}^{\pi^{(i)}}$,
\begin{align*}
\PP(H_t=h_t,I=i)
& =
\alpha_i \mu(s_0)
\prod_{j=0}^{t-1} \pi_j^{(i)}(a_j|s_0,a_0,\dots,s_j)
\prod_{j=0}^{t-1} P_{a_j}(s_j,s_{j+1}) \\
& = \alpha_i \PP_{\mu}^{\pi^{(i)}}( H_t=h_t)\,,
\end{align*}
and, similarly,
\begin{align*}
\PP(H_t=h_t,A_t=a_t,I=i)
 = \alpha_i \PP_{\mu}^{\pi^{(i)}}( H_t=h_t,A_t=a_t)\,.
\end{align*}
Summing these up for $i\in [k]$, we get
\begin{align*}
\PP(H_t=h_t)  &= \sum_{i=1}^k \alpha_i \PP_{\mu}^{\pi^{(i)}}( H_t=h_t)\,,\\
\PP(H_t=h_t,A_t=a_t)  &=  \sum_{i=1}^k \alpha_i \PP_{\mu}^{\pi^{(i)}}( H_t=h_t,A_t=a_t)\,.
\end{align*}
Since $h_t,a_t$ are arbitrary,
$\PP_Z =  \sum_{i=1}^k \alpha_i \PP_{\mu}^{\pi^{(i)}}$ (again, verifying this for simple cylinder sets).
By Part~\ref{q1:a3:1},
$\PP_Z = \PP_\mu^{\bar \pi}$.
Putting things together, we get
$\PP_\mu^{\bar \pi} = \sum_{i=1}^k \alpha_i \PP_{\mu}^{\pi^{(i)}}$.

\item With $f$ as in the previous parts,
\begin{align*}
v^{\bar \pi}(s) = \int f(z) \PP_s^{\bar \pi}(dz)
= \sum_i \alpha_i \int f(z) \PP_{s}^{\pi^{(i)}}(dz) = \sum_i \alpha_i v^{\pi^{(i)}}(s)\,.
\end{align*}
Hence, if $v^{\pi^{(i)}}\ge v$ then multiplying both sides by $\alpha_i\ge 0$,
integrating with respect to $\PP_s^{\pi^{(i)}}$ and summing up we get
$v^{\bar \pi} \ge v$.

% $\nu_\mu^\pi = \sum_i \alpha_i \nu_\mu^{\pi_i}$.
\item It is enough to consider a $2$-state, $2$-action MDP with $\cS = \cA = [2]$ such that action $i\in [2]$ sets the next state to $i$ (deterministically). Further, make staying at any of the states incur a reward of $1$, while make transitioning between the states incur a reward of zero.
Choose $k=2$. Policy $\pi_i$ uses action $i$ (moving to state $i$) everywhere.
The value of both $\pi_1$ and $\pi_2$ is above $\gamma/(1-\gamma)$. The uniform average chooses the actions at random at both states.
The value of the averaged policy $\pi$ at both states is $\frac{1}{2(1-\gamma)}$, which is lower than $\gamma/(1-\gamma)$ provided that $\gamma> 1/2$.

\item
This question was incorrect, it did not hold for the mixing case.
For averaging it is trivial.

For mixing we show a counter example.
Our goal is to show $\mathbb{P}(A_t = a |H_t = h_t) = 1/A$ if we restrict ourselves to uniform mixtures of all deterministic "ML" policies.
Let $\mathcal{S} = \{s_1\}$ and $\mathcal{A} = \{a_1, a_2\}$, then pick $h_t = (s_1, a_1, s_1)$.
Only two deterministic ML policies exist, define them such that $\pi^{(1)}(a_1|s_1) = 1$ and $\pi^{(2)}(a_2|s_1) = 1$. Then we have

\begin{align*}
\mathbb{P}(A_1 = a_1 |H_1 = (s_1, a_1, s_1))
&= \sum_i \mathbb{P}(A_1 = a_1 |H_1 = (s_1, a_1, s_1), I=i) \mathbb{P}(I=i| H_1 = (s_1, a_1, s_1)) \\
&= \sum_i \pi^{(i)} (A_1 = a_1 |H_1 = (s_1, a_1, s_1)) \mathbb{P}(I=i| H_1 = (s_1, a_1, s_1)) \\
&= \pi^{(1)} (A_1 = a_1 |H_1 = (s_1, a_1, s_1)) \mathbb{P}(I=1| H_1 = (s_1, a_1, s_1)) + \\
& \ \ \pi^{(2)} (A_1 = a_1 |H_1 = (s_1, a_1, s_1)) \mathbb{P}(I=2| H_1 = (s_1, a_1, s_1)) \\
&= 1 * 1 + 0 * 0 = 1 \neq 1/2
\end{align*}

Basically, if it is not the first time encountering a state in the trajectory then the action that that will be selected is deterministic (exactly the same action that was selected in that state the last time is was seen).
% Define $\Pi$ to be the set of all deterministic policies, and $\Pi_{a|h}$ to be the set of all deterministic policies that take action $a$ conditioned on the history $h$.
% We first note a useful result.
% $$\sum_\PP^\pi$$

% For averaging we have that

% For mixing, we have
% \begin{align*}
% \PP(A_t=a|H_t=h_t)
% & = \frac{1}{A^S} \sum_{\pi} \PP^{\pi}(A_t=a|H_t=h_t)  \\
% & = \frac{1}{A^S} \sum_{\pi} \mathbb{I}( a= \pi(s_t) )   \\
% & = \frac{1}{A^S} A^{S-1} = \frac{1}{A}\,.
% \end{align*}
\end{enumerate}
\qed\par\smallskip\hrule
\end{solution*}

\section*{Finding needles with high probability}

The high-probability needle lemma is as follows:
\begin{lemma}[High-probability needle lemma]
\label{lem:hpn}
Any algorithm that
correctly identifies the single nonzero entry in any binary array of length $k$
with probability at least $0.91$
has the property that
on some input
the expected number of queries that the algorithm uses is
at least $\Omega(k)$.
\end{lemma}

\begin{question}
Prove~\cref{lem:hpn}.
Note that the algorithms are allowed to randomize.
\tpoints{30}
\end{question}
\begin{solution*}
\newcommand{\Perm}{\mathrm{Perm}}
We give two solutions, each of which have their own merits. The idea of the first solution is rather simple:
by repeatedly running it, any algorithm that is correct with positive probability can be turned into an algorithm which is always correct at the expense of only increasing the runtime inversely proportionally to the success probability.
However,
the formal argument relies on familiarity with Wald's identity.
In contrast, the second solution is direct and elementary, but it is special to the problem at hand.

\noindent \underline{Solution 1}:
In what follows we will identify the possible inputs over $k$ element arrays with the integers $i\in [k]$.
We prove a stronger claim that for any algorithm that returns solutions that are correct with at least probability $p$,
for any $k\ge 2$, if $q_{k.i}$ is the runtime of algorithm when it is used on input $i\in [k]$,
\begin{align*}
\max_{i\in [k]} q_{k,i} \ge p \left(\frac{k+1}{2}-\frac{1}{k} \right)-1\,,
\end{align*}

Fix $k\ge 2$.
Fix any algorithm $A$.
This algorithm gives rise to an algorithm $A'$ that knows when it is correct and $A'$ uses at most one extra query compared to $A$: When $A$ stops and chooses item $I$, at the expense of at most one extra query, $A'$ can verify whether $I=i$. Thus, $A'$ will know whether it was successful and not.
Since the number of queries issued by $A$ is at best one less than that of $A'$, it suffices to show that $A'$ uses $\Omega(k)$ queries on inputs of length $k$.
Hence, in what follows, we restrict ourselves to algorithm that also output an indicator of their own success.

Let $Q\in \{0,1,\dots\}$ denote the random number of queries used and let $S\in \{0,1\}$ be the indicator whether $A$ finds the nonzero entry in its input. As agreed, we may assume that $S$ is the output of $A$.
On input $i\in [k]$, algorithm $A$ induces some distribution $P_{k,i}\in \cM_1( \{0,1,\dots\} \times \{0,1\})$ over these pairs.
Let $q_{k,i}$ be the expected number of queries used by $A$ on input $i$.
Further, by assumption, $p_{k,i}$, the probability that algorithm $A$ succeeds on input $i$ is at least $p$:
\begin{align}
p_{k,i} \ge p\,.
\label{eq:plb}
\end{align}

Let $\mathbb{P}_{k,i}$ be the probability distribution over interaction sequences of infinitely many independent runs of $A$ on input $i$.
Define $A''$ as the algorithm that runs $A$ (every time freshly initialized) until $A$ succeeds
when it returns the item returned by $A$ on it last call.
Clearly, when $A''$ stops it finds the correct item.
We claim the following: Let $i\in [k]$ be arbitrary.
\begin{enumerate}
\item If $N$ is the number of times $A''$ runs $A$, $\PP_{k,i}(N<\infty)=1$, that is, $A''$ stops with probability one;
\item Letting $Q$ be the number of queries used by $A''$,
\begin{align}
\E_{k,i}[Q]=\E_{k,i}[N] q_{k,i} \le \frac{q_{k,i}}{p}\,.
\label{eq:wald}
\end{align}
\end{enumerate}
If the above two claims are established,
it follows that $A$  is a randomized algorithms which always finds the correct entry.
Thus, by the first problem on homework $0$, for some $i\in [k]$,
\begin{align*}
\frac{k+1}{2}-\frac{1}{k} \le \E_{k,i}[Q]\,.
\end{align*}
Putting this together with \eqref{eq:wald} gives
$p(\frac{k+1}{2}-\frac{1}{k}) \le q_{k,i}$.
Thus,
\begin{align*}
\max_{i\in [k]} q_{k,i} \ge p \left(\frac{k+1}{2}-\frac{1}{k} \right)\,,
\end{align*}
finishing the proof.

It remains to establish the above two claims.
Fixing $k,i$ allows us to reduce clutter by writing
$\E$ in place of $\E_{k,i}$ and $\PP$ in place of $\PP_{k,i}$.

To prove the claims,
introduce $(Q_t,S_t)$ as the pair where $Q_t$ is the number of queries used in call $t\ge 1$ of algorithm $A$ and where $S_t\in \{0,1\}$ indicates whether this call was successful.
By construction, $( (Q_t,S_t) )_{t\ge 1}$ is an i.i.d. sequence, with common distribution $P_{k,i}$.
Also, by definition,
\begin{align*}
N = \min \{ n\ge 1\,:\, S_n =1 \}\,.
\end{align*}
As is well known, $N$ has a geometric distribution with parameter $p_{k,i}$:
$\PP(N=n)=p_{k,i}(1-p_{k,i})^{n-1}$ and $\PP(N\ge n) = (1-p_{k,i})^{n-1}$.
As $\PP(N<\infty)= 1-\lim_{n\to\infty} \PP(N\ge n) =1$, establishing the first claim.

As to the second claim, note that by definition,
\begin{align*}
Q = \sum_{n=1}^N Q_n\,.
\end{align*}
We intend to use Wald's identity to get our desired result.
To be able to use this identity, we need to check that the following are satisfied:
\begin{enumerate}
\item $(Q_n)_{n\ge 1}$ share the same finite-mean;
\item $\E[N]<\infty$;
\item $\E[ Q_n \one{N\ge n} ] = \E[Q_n] \PP(N\ge n)$ for all $n\ge 1$;
\item $\sum_{n=1}^\infty \E[ |Q_n| \one{ N \ge n } ]<\infty$.
\end{enumerate}
If these conditions hold, Wald's identity gives
\begin{align*}
\E[Q] = \E[N] \E[Q_1]\,.
\end{align*}
Then, using that $\E[Q_1] = q_{k,i}$
and that, as is well known,
\begin{align}
\E[N] = \sum_{n\ge 1} \PP(N\ge n)=\frac{1}{p_{k,i}}\,,
\label{eq:nex}
\end{align}
combined with \eqref{eq:plb} gives
\begin{align*}
\E[Q] \le \frac{q_{k,i}}{p}
\end{align*}
as required.

It remains to verify the stated conditions.
The first condition follows from the definitions (the common mean is $q_{k,i}$).
For the second condition, we already noted that $\E[N]=1/p_{k,i}$ which is finite.
For the third condition, note that $\{N\ge n \} = \{S_1=0,\dots,S_{n-1}=0\}$ whose indicator is independent of $Q_n$ (since $Q_n$ and $(S_1,\dots,S_{n-1})$ are independent). Hence,
\begin{align*}
\E[ Q_n \one{N\ge n} ]
& = \E[ Q_n \one{S_1=0,\dots,S_{n-1}=0} ]
    = \E[ Q_n ]  \E[ \one{S_1=0,\dots,S_{n-1}=0}  ]
= \E[Q_n] \PP(N\ge n)\,,
\end{align*}
as required.
The fourth condition follows from the third:
$\sum_{n\ge 1} \E[ |Q_n|\one{N\ge n}] = \sum_{n\ge 1} \E[ Q_n \one{N\ge n }]
= \sum_{n\ge 1} \E[Q_n] \PP(N\ge n) = q_{k,i}\EE{N}<\infty$.

\bigskip
\bigskip
\noindent \underline{Solution 2}:
Let $\Perm([k])$ denote the permutations on $[k]$.
WLOG we may restrict ourselves to randomized algorithms
that query the entries in a random order, say $P\in \Perm([k])$, querying first $P(1)$, then $P(2)$, etc.
Indeed, as argued in homework 0, algorithms that query entries twice or more, are dominated.
Similarly, we may assume that the algorithm stops whenever it receives $1$ as the response
or when it queried $k-1$ entries.
In general, an algorithm may also decide to stop after $M\in [k-1]$ queries were issued:
In this case, again, WLOG, we may assume
that it outputs a random element $R$ from the entries not yet queried:
$R\in \{ P(M+1),\dots,P(k)\}$.
Thus, an arbitrary, non-dominated randomizing algorithm is fully described by the joint distribution of $(P,M,R)$.

Fix now such an algorithm.
Let $C$ be the output (entry returned by the algorithm).
Further, let $Q$ be the number of queries the algorithm uses.
Thus, on instance $i\in [k]$, $C=i$ if $P^{-1}(i)\le M$,
otherwise $C=R$. (Note that $P^{-1}(i)\le M$ is equivalent to $i\in \{P(1),\dots,P(M)\}$.)
Further, on instance $i$, $Q=\min(P^{-1}(i),M)$.
Let $I\in [k]$ be a random index that is uniformly chosen, independently of the choice of $(P,M,R)$.

Let $\PP_i$ be the probability distribution induced on $(C,Q,I)$ by running algorithm on instance $i$.
Further, let $\PP$ be the probability distribution induced on $(C,Q,I)$ by running the algorithm on a random index $I\in [k]$ with a uniform distribution.
As $\PP_i(\cdot) = \PP(\cdot|I=i)$ and $I$ is uniformly distributed,
$\PP = \frac1k \sum_{i=1}^k \PP_i$. We denote by $\E_i$ the expectation operator underlying $\PP_i$, and by $\E$ the expectation operator underlying $\PP$.

Assume that the expected query cost of the algorithm is ``small'':
\begin{align*}
\max_i \E_i[Q] \le c k
\end{align*}
for $c>0$ to be chosen later,
while the algorithm is guaranteed to return the correct answer with ``high probability'':
\begin{align*}
\min_i \PP_i(C=i)\ge 0.91\,.
\end{align*}
Fix $i\in [k]$. By Markov's inequality,
\begin{align*}
\PP_i(Q>100ck)\le \frac{\E_i[Q]}{100ck} \le \frac{1}{100}\,.
\end{align*}
Hence,
\begin{align*}
\PP_i(C=i,Q\le 100ck) \ge \PP_i(C=i)-\PP_i(Q>100ck)\ge 0.91-0.01=0.9\,.
\end{align*}
Taking the average over $i=1,\dots,k$, it follows that
\begin{align*}
\PP(C=I,Q\le 100ck) \ge 0.9\,.
\end{align*}
By the tower rule,
$\PP(C=I,Q\le 100ck) = \EE{ \PP(C=I,Q\le 100ck | P,M,R) } \ge 0.9$,
from which it follows that for some $p\in \Perm([k])$, $m\in [k-1]$, $r\in [k]$
with
\begin{align*}
r\in \{ p(m+1),\dots,p(k)\}\,,
\end{align*}
it holds that
\begin{align*}
\PP(C=I,Q\le 100ck | P=p,M=m,R=r) \ge 0.9\,.
\end{align*}
Now,
\begin{align*}
\MoveEqLeft
\PP(C=I,Q\le 100ck | P=p,M=m,R=r)
 \\
& \le \PP( p^{-1}(I)\le 100ck | P=p,M=m,R=r)
+
\PP( p^{-1}(I)> 100ck,C=I,Q\le 100ck  | P=p,M=m,R=r)\\
&\le 100 c +
\PP( p^{-1}(I)> 100ck,C=I,Q\le 100ck  | P=p,M=m,R=r)\,,
\end{align*}
where the second inequality used that $I$ and $P,M,R$ are independent and that $\lceil 100ck \rceil \le 100ck$.
Considering the last term note that if $p^{-1}(I)>100ck\ge Q$ then $Q=\min(p^{-1}(I),m)=m$
and thus $C=r$.
Thus,
\begin{align*}
\MoveEqLeft
\PP( p^{-1}(I)> 100ck,C=I,Q\le 100ck  | P=p,M=m,R=r)\\
& \le
\PP( p^{-1}(I)> 100ck,I=r   | P=p,M=m,R=r)
\le
\frac{k-\lceil 100ck+1 \rceil}{k}
\le
\frac{(k- 100ck)}{k}
= 1-100c\,,
\end{align*}
where we used again the independence of $I$ and $P,M,R$.
Choosing $c=0.002$ we see that
\begin{align*}
0.9\le
\PP(C=I,Q\le 100ck | P=p,M=m,R=r)
\le 0.8\,,
\end{align*}
which is a contradiction.
Hence, with this choice of $c$ there is no algorithm with the above two properties.

\qed\par\smallskip\hrule
\end{solution*}


%\begin{question}
%\end{question}
%\begin{solution*}
%\qed\par\smallskip\hrule
%\end{solution*}


\bigskip
\bigskip

\noindent
\textbf{
Total for all questions: \arabic{DocPoints}}.
% Of this, $15$ are bonus marks (i.e., $90$ marks worth $100\%$ on this problem set).
Of this, up to 20 can be bonus marks. You can receive bonus marks by asking/upvoting questions, for a total of 20 bonus marks!
You must ask at least one question in one of the Lecture Discussion Threads by the Assignment 3 deadline to receive 12 bonus marks.
You can also receive 2 bonus marks for upvoting at least one question before 8am on the day of each lecture, for a maximum of 2 marks x 4 lectures = 8 marks for upvoting.
Your assignment will be marked out of \arabic{DocPoints} minus the bonus marks you received.

\end{document}
