% !TEX encoding = UTF-8 Unicode
\documentclass{article}
\newcommand{\hwnumber}{4}

\newcommand{\norm}[1]{\| #1 \|}
\newcommand{\abs}[1]{| #1 |}


\usepackage{fullpage,amsthm,amsmath,amssymb}
\usepackage{algorithm,algorithmic}
\usepackage{mathtools}
\usepackage{bbm,bm}
\usepackage{enumerate}
\usepackage{cancel}
\usepackage{xspace}
\usepackage[textsize=tiny,
%disable
]{todonotes}
\newcommand{\todot}[1]{\todo[color=blue!20!white]{T: #1}}
\newcommand{\todoc}[1]{\todo[color=orange!20!white]{Cs: #1}}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=black]{hyperref}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\usepackage[capitalize]{cleveref}


\usepackage{comment}

\newcommand{\cP}{\mathcal{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\cX}{\mathcal{X}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator*{\Exp}{\mathbb{E}}
\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator*{\1}{\mathbbm{1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\bbP}{\mathbb P}
\newcommand{\V}{\mathbb V}
\renewcommand{\P}[1]{P\left\{ #1 \right\}}
\newcommand{\Prob}[1]{\mathbb{P}( #1 )}
\newcommand{\real}{\mathbb{R}}
\renewcommand{\b}[1]{\mathbf{#1}}
\newcommand{\EE}[1]{\E[#1]}
\newcommand{\bfone}{\1}
\newcommand{\one}[1]{\mathbb{I}\{#1\}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\0}{\mathbf{0}}
\usepackage{xifthen}

% total number of points that can be collected
\newcounter{DocPoints} % counter reset to zero upon creation

% counting points per question (not user facing)
\newcounter{QuestionPoints} % counter reset to zero upon creation

% Points for a subquestion of a question; 
% Adds the points to the total for the question and the document.
\newcommand{\points}[1]{%
	\par\mbox{}\par\noindent\hfill {\bf #1 points}%
	\addtocounter{DocPoints}{#1}
	\addtocounter{QuestionPoints}{#1}
}
% Points for a question; call with no params if the question
% had subquestions. In this case it prints the total for the question (see \points).
% Otherwise call with the points that the question is worth.
% In this case, the total is added to the document total.
% It is a semantic error to call this with a non-empty parameter
% when a question had subquestions with individual scores.
\newcommand{\tpoints}[1]{        %
	\ifthenelse{\isempty{#1}}%
	{%
	}%
	{%
		\addtocounter{DocPoints}{#1}
		\addtocounter{QuestionPoints}{#1}
	}													 %
	\par\mbox{}\par\noindent\hfill {Total: \bf \arabic{QuestionPoints}\xspace points}\par\mbox{}\par\hrule\hrule
	\setcounter{QuestionPoints}{0}
}
\newcommand{\tpoint}[1]{
	\tpoints{#1}
}

\theoremstyle{definition}
\newtheorem{question}{Question}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem*{assumption*}{Assumption}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem*{remark*}{Remark}
\newtheorem{solution}{Solution}
\newtheorem*{solution*}{Solution}

\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\excludecomment{solution}
%\excludecomment{solution*}

\newcommand{\hint}{\noindent \textbf{Hint}:\xspace}


\usepackage{hyperref}

\newcommand{\epssub}{\delta}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\sA}{\mathcal{A}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}


\begin{document}

\begin{center}
{\Large \textbf{CMPUT 605: Theoretical Foundations of Reinforcement Learning, Winter 2023\\ Homework \#\hwnumber}}
\end{center}

\section*{Instructions}
\textbf{Submissions}
You need to submit a single PDF file, named {\tt p0\hwnumber\_<name>.pdf} where {\tt <name>} is your name.
The PDF file should include your typed up solutions (we strongly encourage to use pdf\LaTeX). 
Write your name in the title of your PDF file.
We provide a \LaTeX template that you are encouraged to use.
To submit your PDF file you should send the PDF file via private message to Vlad Tkachuk on Slack before the deadline.

\textbf{Collaboration and sources}
Work on your own. You can consult the problems with your classmates, use books
or web, papers, etc.
Also, the write-up must be your own and you must acknowledge all the
sources (names of people you worked with, books, webpages etc., including class notes.) 
Failure to do so will be considered cheating.  
Identical or similar write-ups will be considered cheating as well.
Students are expected to understand and explain all the steps of their proofs.

\textbf{Scheduling}
Start early: It takes time to solve the problems, as well as to write down the solutions. Most problems should have a short solution (and you can refer to results we have learned about to shorten your solution). Don't repeat calculations that we did in the class unnecessarily.

\vspace{0.3cm}

\textbf{Deadline:} March 26 at 11:55 pm

\newcommand{\cM}{\mathcal{M}}
\newcommand{\nS}{\mathrm{S}}
\newcommand{\nA}{\mathrm{A}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ip}[1]{\langle #1 \rangle}
\newcommand{\N}{\mathbb{N}}
\newcommand{\cG}{\mathcal{G}}

\section*{Large action set query lower bound}


We recall a few definitions and results from 
\href{https://rltheory.github.io/lecture-notes/planning-in-mdps/lec9/}{Lecture 9}.
For a featurized MDP $(M,\phi)$, let
\begin{align}
\varepsilon^*(M,\Phi) : = \sup_{\pi \text{ memoryless}} \inf_{\theta\in \mathbb{R}^d} \| \Phi \theta - q^\pi \|_\infty\,.
\label{eq:polerr}
\end{align}

\begin{definition}
An online planner is $(\delta,\varepsilon)$-sound if
for any finite discounted MDP $M=(\mathcal{S},\mathcal{A},P,r,\gamma)$
and feature-map $\varphi:\mathcal{S}\times \mathcal{A}\to \mathbb{R}^d$ such that
$\varepsilon^*(M,\Phi)\le \varepsilon$,
when interacting with $(M,\varphi)$, the planner induces a $\delta$-suboptimal policy of $M$.
\end{definition}

\noindent The following was proven in the said lecture:

\begin{theorem}[Query lower bound: large action sets]
\label{thm:qlb}
\label{thm:melesslb}
For any $\varepsilon>0$, $0<\delta\le 1/2$, positive integer $d$
and
for any $(\delta,\varepsilon)$-sound online planner $\mathcal{P}$
there exists a featurized-MDP $(M,\varphi)$ with rewards in $[0,1]$ with $\varepsilon^*(M,\Phi)\le \varepsilon$ such that
when interacting with a simulator of $(M,\varphi)$,
the expected number of queries used by
$\mathcal{P}$ 
is at least $\Omega(f(d,\varepsilon,\delta))$ where
\begin{align*}
f(d,\varepsilon,\delta) = \exp\left( \frac{1}{32} \left(\frac{\sqrt{d}\varepsilon}{\delta}\right)^2 \right)\,.
\end{align*}
\end{theorem}

\begin{question}
  The lecture notes provide a proof sketch for this theorem. Formally prove this theorem, explicitly explain each step of your proof. 
\tpoints{20}
\end{question}
\begin{solution*}
First, recall the JL feature matrix construction:
\begin{proposition}[JL feature matrix]
For any $\tau>0$, integers $d,k>0$  such that
\begin{align}
d\le k \le \exp\left(\frac{d\tau^2}{8}\right)\,,
\label{eq:krange}
\end{align}
there exists a matrix $\Phi \in \mathbb{R}^{k\times d}$ such that
for any $i\in[k]$,
\begin{align}
\max_{i\in [k]} \inf_{\theta\in \mathbb{R}^d} \|\Phi \theta - e_i \|_\infty \le \tau\,,
\label{eq:featjl}
\end{align}
where
$e_i$ is the $i$th basis vector of standard Euclidean basis of $\mathbb{R}^k$,
and in particular if $\varphi_i^\top$ is the $i$th row of $\Phi$,
$\|\Phi \varphi_i - e_i\|_\infty \le \tau$
holds.
\end{proposition}


\newcommand{\send}{s_{\text{end}}}
Fix the planner $\mathcal{P}$ with the said properties.
Let $k$ be a positive integer to be chosen later.
We construct a feature map $\varphi:\mathcal{S}\times \mathcal{A}\to \mathbb{R}^d$
and $k$ MDPs $M_1,\dots,M_k$
that share $\mathcal{S}=\{s,\send\}$ and $\mathcal{A}=[k]$ as state- and action-spaces, respectively.
Here $s$ will be chosen as the initial state where the planners will be tested from and
 $\send$ will be an absorbing state with zero reward. 
The MDPs share the same deterministic transition dynamics: 
All actions in $s$ end up in $\send$ with probability one 
and all actions taken in $\send$ end up in $\send$ with probability one.
The rewards for actions taken in $\send$ are all zero.
Finally, we choose the reward of MDP $M_i$ in state $s$
to be 
\begin{align*}
r_a^{(i)}(s)=\mathbb{I}(a=i) r^*\,,
\end{align*}
where the value of $r^*\in [0,1]$ is left to be chosen later. 

Fix $i\in [k]$. Let $\mathbb{P}$ be the probability distribution induced by the interconnection of planner $\mathcal{P}$ and MDP $M_i$. While this depends on $i$ (since planners use observed rewards to plan and the reward distributions between the MDPs are different), this dependence is suppressed to minimize clutter.
Let $\mathbb{E}$ be the corresponding expectation operator.

Let $A$ be the action that is chosen by planner $\mathcal{P}$ when fed with initial state $s$. 
Let $\bar r = \mathbb{E}[r_A^{(i)}(s)]$.
By the MDPs construction, the value of the policy induced by the planner in state $s$ and MDP $M_i$ is $v := \bar r$.
Note that the optimal value in state $s$ is $r^*$.
By our assumption, $\mathcal{P}$ is $(\delta,\varepsilon)$-sound.
Hence, provided that we can construct an appropriate feature map so that $M_i$ satisfies $\varepsilon^*(M_i,\Phi)\le \varepsilon$, we must have

\begin{align}
\bar r \ge r^*-\delta\,.
\label{eq:brrs}
\end{align}

Now, choose
\begin{align*}
r^* = 2 \delta
\end{align*}

which makes the right-hand side of \eqref{eq:brrs} $r^*/2$. Note that $r^*\in [0,1]$ 
since we assumed that $\delta\le 1/2$.
Thus, we have $\bar r \ge r^*/2$. By construction, all the rewards are zero except the reward of action $i$. Hence, $\bar r = \mathbb{P}(A=i) r^*$ and thus we get that

\begin{align*}
\mathbb{P}(A=i)\ge 1/2\,.
\end{align*}

Hence, for $i$ when the planner $\mathcal{P}$
is interconnected with the simulator of $M_i$, it
returns action $i$ with at least probability $1/2$.
As a result, we can use the planner to search any binary array of length $k$ for the single nonzero entry in the array. Indeed, assume that we want to use the planner to search in the array $b\in \{0,1\}^n$.
Then, when the planner queries $(s,a)$ with $a\in [k]$,
we issue a query to the array to get the value of $b_a$. Then we feed the planner
with $s$ (as the next state) and the reward $b_a r^*$. Clearly,
if and only if $b$ is such that $b_i=1$,
this way we simulate the data that would be generated if the planner was interconnected with MDP $M_i$. Finally, when the planner stops and outputs $A$, we return $A$. By our previous argument, $\mathbb{P}(A=i)\ge 1/2$. Hence, by the high-probability needle lemma, the expected number of queries used by $\mathcal{P}$ on at least one of the $k$ MDPs is at least $\Omega(k)$.

It remains to choose $k$ and the feature-map. For this, we use JL feature matrix.
First. note that the action-value functions of the memoryless policies in any of the $k$ MDPs belong to the set

\begin{align*}
\{ q_{i}:\mathcal{S}\times \mathcal{A}\to \mathbb{R}\,:\,
  q_i(\send,\cdot) = \boldsymbol{0}\,,
 q_{i}(s,a) = r^* \mathbb{I}(a=i), a\in \mathcal{A}, i\in [k] \}.
\end{align*}

Hence, we need a feature-map that approximates the functions in this set uniformly well up to an $\varepsilon$ accuracy.
Fix $\tau>0$ to be chosen later.
Take the JL feature matrix
$\Phi \in \mathbb{R}^{k\times d}$
such that Eq. \eqref{eq:featjl} holds
for this $\tau$.
Let the rows of $\Phi$ be $\varphi_1,\dots,\varphi_k$.
We let 
\begin{align*}
\phi(s,k) = \varphi_k\,,\quad  \phi(\send,k)=\boldsymbol{0}\,, \qquad  k\in [A]\,.
\end{align*}
Then, to approximate the function $q_{i}$ with some $i\in [k]$,
we set $\theta = r^* \varphi_i$.
Clearly, $q_i(\send,\cdot) = \phi(\send,\cdot)^\top \theta = \boldsymbol{0}$.
For $s$ and 
for $a\in [k]$ we have

\begin{align*}
|\varphi(s,a)^\top\theta -q_{i}(s,a)|
 =
| r^* \varphi_a^\top \varphi_i-q_{i}(s,a)|
 \le
\begin{cases}
r^* \tau\,, & \text{if } a\ne i\,;\\
0\,, & \text{otherwise}\,.
\end{cases}
\end{align*}


To control the error of approximating $q_{i}$,
it suffices to choose $\tau$ so that
$ r^* \tau \le \varepsilon$. Recalling
 $r^* \tau = 2\delta \tau$,
we choose $\tau$ so that $2\delta \tau = \varepsilon$.
Finally, to choose the value of $k$ we plug in the value of $\tau$
into Eq. \eqref{eq:krange} and get
\begin{align*}
k =\left\lfloor\exp\left(\frac{d (\frac{\varepsilon}{2\delta})^2}{8}\right)\right\rfloor\,.
\end{align*}

The proof is finished by recalling that the expected
number of queries made by $\mathcal{P}$ is at least $\Omega(k)$ on at least one of $M_1,\dots,M_k$.
\if0
We prove that
\begin{align*}
n^*(d,\varepsilon,\delta/2,\lceil H_{\gamma,\delta/2} \rceil ) 
= \Omega(f(d,\varepsilon,\delta))\,,
\end{align*}
which is clearly equivalent to the desired statement, by substituting $\delta/2$ into both sides of \cref{eq:nmlb}. 

Fix $\delta>0$, $\varepsilon>0$, $d>0$ and let
\begin{align*}
k =\left\lfloor f(d,\varepsilon,\delta) \right\rfloor\,.
\end{align*}
The following lemma can be extracted from the proof of \cref{thm:qlb}
\begin{lemma}
\label{lem:mlp}
There exists $\Phi \in \mathbb{R}^{k\times d}$ and MDPs $M_1, \dots, M_k$ such that the following holds:
\begin{enumerate}
\item All MDPs have a single state, $k$ actions and MDP $M_i$'s reward function is $r^{(i)}_a(s) = 2\delta(1-\gamma) \one{a=i}$;
\item For $1\le i \le k$, $(M_i,\Phi)\in \cM_{\varepsilon}$;
\item If $\cP'$ is a memoryless local planner that induces a $\delta$-suboptimal policy when interconnected with any of $(M_i,\Phi)$ then $\max_{1\le i \le k} n(\cP',M_i,\Phi,1)\ge k/4$.
\end{enumerate}
\end{lemma}
\noindent Note that the local planner $\cP'$ in Part 3 need not be $(\delta,\varepsilon)$-sound.

Let now $\cP$ be any local planner that is $(\delta/2,\varepsilon)$-sound.
Let $h = \lceil H_{\gamma,\delta/2} \rceil$. We will show that
\begin{align}
\max_{1\le i \le k} n(\cP,M_i,\Phi,h) \ge k/4-h\,.
\label{eq:plb}
\end{align}
Then, since $(M_i,\Phi)\in \cM_\varepsilon$,
\begin{align*}
\sup_{(M,\phi) \in \cM_\varepsilon} n(\cP,M,\Phi,h) \ge k/4 - h \,.
\end{align*}
Taking the infimum of both sides over $\cP\in \cP_{\varepsilon,\delta/2}$, we get
\begin{align*}
n^*(d,\varepsilon,\delta/2,h) \ge k/4-h\,.
\end{align*}
Now, since $h$ does not depend on $d$,
as $d\to \infty$,
$n^*(d,\varepsilon,\delta/2,h)= \Omega(f(d,\varepsilon,\delta))$.

It remains to show \cref{eq:plb}.
For this, we will resort to \cref{lem:mlp}. Since this result is for memoryless planners, we convert $\cP$ into a memoryless local planner $\cP'$ as follows:
$\cP'$ calls $\cP$ consecutively $h$ times.
At the end, $\cP'$ also checks with at most $h$ queries the rewards corresponding to the 
$h$ actions,
 $A_0,\dots,A_{h-1}$ obtained from $\cP$ and returns any of $A_0,\dots,A_{h-1}$
 for which the observed reward is nonzero.
 When there is no such action, $\cP'$ returns action $1$.
 
We claim that the following holds:

\noindent \underline{Claim}: Planner $\cP'$ is $\delta$-suboptimal 
on any of the featurized MDPs $(M_1,\Phi),\dots,(M_k,\Phi)$.

Note that if this claim holds true, \cref{eq:plb} follows.
Indeed, by \cref{lem:mlp}, 
for some $1\le i \le k$, $n(\cP',M_i,\Phi,1)\ge k/4$. Take such an $i$.
Clearly, $n(\cP',M_i,\Phi,1) = n(\cP,M_i,\Phi,h)+h$.
Thus, $n(\cP,M_i,\Phi,h)=n(\cP',M_i,\Phi,1)-h \ge k/4-h$, showing that \cref{eq:plb} holds.

It remains to show the claim.
For this, we need some extra notation.
For $i\in [k]$, 
let $\pi_i$ denote the policy induced by $\cP$ when using it in MDP $M_i$
and let $\PP_i$ be the probability distribution induced by interconnecting $\cP$ and $M_i$.
Let $\E_i$ be the corresponding expectation operator.

The optimal value in MDP $M_i$ is $2\delta$.
Since $\cP$ is $(\delta/2,\varepsilon)$-sound, denoting by $v_i^{\pi_i}$ the value function of $\pi_i$ in $M_i$, we have
\begin{align*}
\frac{3}{2}\delta = v^*_i(s) - \frac{\delta}{2}  & \le v^{\pi_i}_i(s) \\
& \le 2\delta(1-\gamma) \E_i[ \sum_{t=0}^{h-1}  \gamma^t \one{A_t=i} ]
		+ \frac{\gamma^h}{1-\gamma} \\
&\le  2\delta(1-\gamma) \frac{1-\gamma^{h}}{1-\gamma} \E_i[  \max_{0\le t \le h-1} \one{A_t=i} ]+ \delta/2\\
& \le 2\delta  \PP_i( i\in \{A_0,\dots,A_{h-1}\} )  + \delta/2\,,
\end{align*}
where the second inequality follows from the definition of rewards and the choice of $h$,
the third follows from the summation formula for geometric series, 
the fourth follows from $\gamma^h\ge 0$.
Reordering gives
\begin{align}
 \PP_i( i\in \{A_0,\dots,A_{h-1}\} )  \ge \frac{1}{2}\,.
 \label{eq:hp}
\end{align}

Let $\PP_i'$ be the probability distribution induced by the interconnection of $\mathcal{P}'$ and $M_i$,
and let $\pi_i'$ denote the policy induced by $\mathcal{P}'$ when it used in $M_i$.
Then,
\begin{align*}
v_i^{\pi_i'} (s)
= 2\delta \PP_i'(A=i) 
= 2\delta \PP_i(A=i)
\ge 2\delta \PP_i(i \in \{A_0,\dots,A_{h-1}\}) \ge \delta
\end{align*}
where the first equality follows from the definition of $M_i$ and $\pi_i'$, the second
follows because, by the construction of $\mathcal{P}'$, $\PP_i = \PP_i'$, the first inequality follows from the definition of $A$ and last inequality follows from 
\eqref{eq:hp}.
Hence,
$v^*(s)-v_i^{\pi_i'} (s) = 2\delta-\delta \le \delta$, finishing the proof of the claim.
\fi
\qed\par\smallskip\hrule
\end{solution*}


\section*{Fixed-horizon fundamental theorem}
The same lecture stated the fundamental theorem for fixed-horizon problems, which we copy here for convenience. For the definitions of the quantities used in the theorem, see the lecture notes.
\begin{theorem}[Fixed-horizon fundamental theorem]
\label{thm:fhft}
We have $v_0^*\equiv \boldsymbol{0}$ and for any $h\ge 0$, $v_{h+1}^* = T v_h^*$. Furthermore,
for any $\pi_0^*,\dots,\pi_h^*, \dots$ such that for $i\ge 0$,
$\pi_i^*$ is greedy with respect to $v_i^*$,
for any $h>0$ it holds that
$\pi=(\pi_{h-1}^*,\dots,\pi_0^*, \dots)$ (i.e., the policy which in step $0$ uses $\pi_{h-1}^*$, in step $1$ uses $\pi_{h-2}^*$, $\dots$, in step $(h-1)$ uses $\pi_0^*$, after which it continues arbitrarily) is $h$-step optimal:
\begin{align*}
v^{\pi}_h = v_h^*\,.
\end{align*}
\end{theorem}
In the lecture notes we did not give a proof.
\begin{question}
Prove \cref{thm:fhft}.
\hint 
Use induction and mimic the previous proofs.
\tpoints{50}
\end{question}
\begin{solution*}
We follow the advice by mimicking the proofs seen before.
We need some definitions. 
For a policy $\pi$ and for $(s,a)$, $i\ge 0$ arbitrary, let
$\nu_{\mu,i}^\pi(s,a) = \PP_\mu^\pi(S_i=s,A_i=a)$. By abusing notation, we also 
let $\nu_{\mu,i}^\pi(s) = \PP_\mu^\pi(S_i=s)$.
First, we show the following:

\noindent \underline{Claim 1}: For any $\pi  = (\pi_0,\pi_1,\dots)$ policy, $\mu \in \cM_1(\cS)$, 
there is a nonstationary memoryless policy $\pi'=(\pi_0',\pi_1',\dots)$ such that 
for any $i\ge 0$, $(s,a)\in \cS \times \cA$, $\nu_{\mu,i}^\pi(s,a) = \nu_{\mu,i}^{\pi'}(s,a)$
(here, $\pi_0',\pi_1',\dots$ are memoryless policies).

Fix $\pi$. The policy $\pi'$ is defined by $(\pi_0',\pi_1',\dots)$ where for $i\ge 0$,
$\pi_i'$ is defined using 
\begin{align*}
\pi_i'(a|s) = \frac{\nu^\pi_{\mu,i}(s,a)}{\nu^\pi_{\mu,i}(s)}\,, \qquad (s,a)\in \cS \times \cA\,.
\end{align*}
Clearly, $\pi_i'$ is a memoryless policy.
We now claim that for any $s$, $\nu_{\mu,i}^\pi(s) = \nu_{\mu,i}^{\pi'}(s)$.
We prove this by induction on $i$.
For $i=0$, the claim is clearly true as 
$\nu_{\mu,0}^\pi(s) = \mu(s)=\nu_{\mu,0}^{\pi'}(s)$.
Assume that the claim holds for $i\ge 0$.
Then, applying the law of total probability
and using definitions of $\PP_\mu^\pi$ and $\PP_\mu^{\pi'}$,
\begin{align*}
\nu_{\mu,i+1}^\pi(s')
&= \sum_{s,a} \nu_{\mu,i}^\pi(s,a) \PP_\mu^\pi(S_{i+1}=s'|S_i=s,A_i=a)\\
&= \sum_{s,a} \nu_{\mu,i}^\pi(s,a) P_a(s,s')\\
&= \sum_{s,a} \nu_{\mu,i}^{\pi'}(s,a) P_a(s,s') \tag{\text{by the I.H.}}\\
&= \sum_{s,a} \nu_{\mu,i}^{\pi'}(s,a) \PP_\mu^{\pi'}(S_{i+1}=s'|S_i=s,A_i=a)\\
& = \nu_{\mu,i+1}^{\pi'}(s')\,.
\end{align*}
It also immediately follows that for any $i\ge 0$ and any $s,a$,
\begin{align*}
\nu_{\mu,i}^\pi(s,a) 
& = \frac{\nu_{\mu,i}^\pi(s,a)}{\nu_{\mu,i}^\pi(s)} \nu_{\mu,i}^\pi(s) \\
& = \pi'_i(a|s) \nu_{\mu,i}^{\pi'}(s) \\
& 
\stackrel{(*)}{=} 
\frac{\nu_{\mu,i}^{\pi'}(s,a)}{\nu_{\mu,i}^{\pi'}(s)}\nu_{\mu,i}^{\pi'}(s) \\
& = \nu_{\mu,i}^{\pi'}(s,a)\,,
\end{align*}
where the equality marked by $(*)$ follows because $\pi'$ is a sequence of memoryless policies and the $i$th policy in $\pi'$ is exactly $\pi_i'$. This finishes the proof of the claim.

From this follows our second claim:

\noindent \underline{Claim 2}: For any $\pi  = (\pi_0,\pi_1,\dots)$ policy, $s\in \cS$ there is a nonstationary memoryless policy $\pi'=(\pi_0',\pi_1',\dots)$ such that 
for any $i\ge 0$, $v_i^{\pi}(s) = v_i^{\pi'}(s)$.

Fix $\pi$, $s\in \cS$. Let $\pi'$ be as in the previous claim. By abusing notation, let $r(s,a) = r_a(s)$ so that for $\nu \in \cM_1(\cS\times\cA)$, $\nu r = \sum_{s,a} \nu(s,a)r(s,a)$ is well-defined.
Then, $v_i^\pi(s) = \sum_{t=0}^{i-1} \E_s^\pi[ r_{A_t}(S_t) ]
= \sum_{t=0}^{i-1}  \nu_{\mu,t}^\pi r = \sum_{t=0}^{i-1}  \nu_{\mu,t}^{\pi'} r = v_i^{\pi'}(s)$, finishing the proof.

Denote by $\text{NML}$ the set of nonstationary memoryless policies.
Hence, for $i\ge 0$,
\begin{align}
v_i^*(s) = \sup_{\pi} v_i^\pi(s) = \sup_{\pi \in \text{NML}} v_i^{\pi}(s)\,.
\label{eq:nmle}
\end{align}

We prove the statement by induction on $i$, but we first recall the statement itself.
Let us start by recalling the definition of $\pi_i^*$: $\pi_i^*$ is greedy with respect to $v_i^*$:
\begin{align*}
T_{\pi^*_i} v_i^* = T v_i^*\,.
\end{align*}
Now, for $i>0$, let $\tilde\pi_i =(\pi_{i-1}^*,\pi_{i-2}^*,\dots,\pi_0^*,\dots)$.
The following needs to be proven:
\begin{enumerate}
\item $v_0^* = \boldsymbol{0}$;
\item $v_{h}^* = T v_{h-1}^*$ holds for $h\ge 1$;
\item $v^{\tilde\pi_h}_h = v_h^*$ holds for $h\ge 1$;
\end{enumerate}
First, let $h=0$. By definition, for any policy $\pi$, $v_0^\pi = \boldsymbol{0}$. Hence, $v^*_0 = \boldsymbol{0}$.
We prove the second two statements together, by induction on $h$. For the base case let $h=1$.
We need to prove that
\begin{align*}
v_1^* = v_1^{\tilde\pi_1} = T \boldsymbol{0}\,.
\end{align*}
Take any nonstationary memoryless policy $\pi = (\pi_0,\pi_1,\dots)$.
It is easy to see that for any $h\ge 1$,
\begin{align}
v_h^\pi = T_{\pi_0} \dots T_{\pi_{h-1}} \boldsymbol{0}\,. \label{eq:fhpe}
\end{align}
In particular, for $h=1$,
\begin{align*}
v_1^\pi = T_{\pi_0} \boldsymbol{0}\,.
\end{align*}
Then,
\begin{align*}
v_1^\pi \le T \boldsymbol{0} = T_{\pi_0^*} \boldsymbol{0} = v_1^{\tilde\pi_1} \le v_1^*\,.
\end{align*}
Now fix $s$.
Taking the supremum over $\pi$, by \cref{eq:nmle},
\begin{align*}
v_1^*(s) 
= \sup_{\pi\in \text{NML}} v_1^\pi(s) 
\le (T \boldsymbol{0})(s) 
= (T_{\pi_0^*} \boldsymbol{0})(s) 
= v_1^{\tilde\pi_1}(s)\le v^*_1(s)\,,
\end{align*}
hence equality holds everywhere above. 
Since $s$ is arbitrary,
\begin{align*}
v_1^* = T \boldsymbol{0} = v_1^{\tilde\pi_1}
\end{align*}
as required.

Now let $h>1$ and assume that the statement has been proven up to $h-1$.
Again, fix an arbitrary nonstationary memoryless policy $\pi$. Then, by \cref{eq:fhpe},
\begin{align*}
v_h^\pi 
& = T_{\pi_0} T_{\pi_1} \dots T_{\pi_{h-1}} \boldsymbol{0} \\
& \le T_{\pi_0} v_{h-1}^*  \tag{$T_{\pi_0}$ monotone, $T_{\pi_1} \dots T_{\pi_{h-1}} \boldsymbol{0} =v_{h-1}^{(\pi_1,\pi_2,\dots)} \le v_{h-1}^*$ } \\
& \le T v_{h-1}^*  \tag{$T_{\pi_0}\le T$}\\
& = T_{\pi_{h-1}^*} v_{h-1}^* \tag{def. of $\pi_{h-1}^*$}\\
& = T_{\pi_{h-1}^*} v_{h-1}^{\tilde \pi_{h-1}}  \tag{induction hypothesis}\\
& = T_{\pi_{h-1}^*} \dots T_{\pi_0^*} \boldsymbol{0} \tag{def. of $\tilde \pi_{h-1}$, \cref{eq:fhpe}}\\
& = v_h^{\tilde \pi_h}  \tag{def. of $\pi_h$, \cref{eq:fhpe}} \\
& \le v^*_h\,. \tag{def. of $v^*_h$}
\end{align*}
As before, fixing a state, taking the supremum over $\pi$, by \cref{eq:nmle},
\begin{align*}
v_h^* \le T v_{h-1}^* = v_h^{\tilde \pi_h} \le v_h^*\,,
\end{align*}
and hence we have equality everywhere. This finishes the inductive step and thus the proof.
\qed\par\smallskip\hrule
\end{solution*}


\section*{Statisticians also have limits}
Let $\cX$ be a subset of a Euclidean space equipped with the usual Borel $\sigma$-algebra, 
$\cP\subset \cM_1(\cX)$ a set of probability distributions over $\cX$.
Let $f:\cP \to \RR$ be a fixed function.
We consider statistical estimation problems where a random ``data'' $X\in \cX$ is observed
from an unknown $P\in \cP$
and the job of the statistician is to produce an estimate of $f(P)$.

That is, the statistician needs to design an estimator; for simplicity we assume that the estimators are not randomizing (an extension to randomizing estimators is trivial).
A non-randomizing estimator maps the data to a real; thus, any such estimator is a map $g:\cX \to \RR$.
We assume that $g$ is measurable so that we can talk about the probability of errors.

In particular, for $\delta\in [0,1]$ and $\varepsilon>0$, 
we say that $g$ is \textbf{$(\delta,\varepsilon)$-sound} for the problem specified by $(\cP,f)$ if for any $P\in \cP$,
\begin{align}
P( |g(X)-f(P)|>\varepsilon)\le \delta\,.
\label{eq:sstat}
\end{align}
Here, $X:\cX \to \cX$ is treated as the identity map, as usual: $X(x) = x$, $x\in \cX$.
Thus, the above probability is the probability assigned by $P$ to the set 
\[
\{ x\in \cX\,:\, |g(x)-f(P)|>\varepsilon \}
\]
and condition \eqref{eq:sstat} has the equivalent form that for any $P\in \cP$,
\begin{align*}
P(\{ x\in \cX\,:\, |g(x)-f(P)|>\varepsilon \} )\le \delta\,.
\end{align*}
It is just shorter and more elegant to write \cref{eq:sstat}, hence, we will stick to this usual form.

For two probability measures, $P,Q$, over the same measurable space $(\Omega,\cF)$, we define their \textbf{relative entropy} by
\begin{align*}
D(P,Q) =
\begin{cases}
 \int \log \frac{dP}{dQ}(\omega) \, dP(\omega) \,, & \text{ if } P\ll Q\;\\
 +\infty\,, & \text{otherwise}\,.
 \end{cases}
\end{align*}
The relative entropy is also known as the Kullback-Leibler divergence between $P$ and $Q$ (see Chapter 14 in the 
\href{https://tor-lattimore.com/downloads/book/book.pdf}{bandit book} for an explanation of its origin and some examples).

The following result, which is Theorem 14.12 in that book, will be useful:
\newcommand{\KL}{D}
\begin{theorem}[Bretagnolle--Huber inequality]
\label{thm:pinskerhp}\index{Bretagnolle-Huber inequality|textbf}
Let $P$ and $Q$ be probability measures on the same measurable space $(\Omega, \cF)$, and let $A \in \cF$ be an arbitrary event. Then,
\begin{align}\label{eq:pinskerhp}
P(A) + Q(A^c) \geq \frac{1}{2} \exp\left(-\KL(P, Q)\right)\,,
\end{align} 
where $A^c = \Omega \setminus A$ is the complement of $A$.
\end{theorem} 


\begin{question}
\label{q:lbcore}
Show that if there is an $(\delta,\varepsilon)$-sound estimator for $(\cP,f)$ then
\begin{align*}
\log\left(\frac{1}{4\delta}\right) \le \inf \{ D(P_0,P_1) \,:\, P_0,P_1\in \cP \text{ s.t. }
|f(P_0)-f(P_1)|>2\varepsilon \} \,.
\end{align*}

In words, distributions whose $f$-values are separated by $2\varepsilon$ cannot be too close to each other  if a $(\delta,\varepsilon)$-sound estimator exist. This should be quite intuitive.
\tpoints{20}
\end{question}

\begin{solution*}
Let $\Omega = \cX$ and $\cF$ be the corresponding Borel $\sigma$-algebra.
Let $g$ be a $(\delta,\varepsilon)$-sound estimator for $(\cP,f)$.
Pick $P_0,P_1\in \cP$ such that 
\begin{align}
|f(P_0)-f(P_1)|>2\varepsilon 
\label{eq:setcs}
\end{align}
Let $A = \{ |g(X)-f(P_0)|>\varepsilon \}$. 
From \cref{eq:pinskerhp},
\begin{align*}
D(P_0,P_1)\ge \log\left( \frac{1}{2( P_0(A)+P_1(A^c) )} \right)\,.
\end{align*}
Hence, it suffices to show that $ P_0(A)+P_1(A^c)\le 2\delta$.

By definition,
\[
\delta \ge P_0(A)\,.
\]
Also by definition and by \cref{eq:setcs}, 
\begin{align*}
\delta \ge P_1( |g(X)-f(P_1)|>\varepsilon ) \ge P_1( |g(X)-f(P_0)|\le \varepsilon) = P_1(A^c)\,.
\end{align*}
In particular, the second inequality holds because for any $x$ such that $|g(x)-f(P_0)|\le \varepsilon$, 
by \cref{eq:setcs}, $|g(x)-f(P_1)|>\varepsilon$.
Putting things together, $P_0(A)+P_1(A^c)\le 2\delta$ and thus $D(P_0,P_1) \ge \log(1/(4\delta))$.
Taking the infimum over $P_0$ and $P_1$ gives the result.
\qed\par\smallskip\hrule
\end{solution*}

In what follows, we will deal with Bernoulli random variables. The relative entropy between Bernoulli distributions has special properties which we will find useful. The next problem asks you to prove some of these properties.

\newcommand{\Ber}{\text{Ber}}
Let $\Ber(p)$ denote the Bernoulli distribution with parameter $p\in [0,1]$. 
As it is well known (and not hard to see from the definition),
\begin{align*}
D(\Ber(p),\Ber(q)) = d(p,q)
\end{align*}
where $d(p,q)$ is the so-called \textbf{binary relative entropy function}, which is defined as
\begin{align*}
d(p,q) = p\log(p/q) + (1-p) \log( (1-p)/(1-q))\,.
\end{align*}

\begin{question}
\label{q:ber}
Show that for $p,q\in (0,1)$, defining $p^*$ to be $p$ or $q$ depending on which is further away from $1/2$,
\begin{align}
d(p,q) \le \frac{(p-q)^2}{2p^*(1-p^*)}\,.
\label{eq:dpq}
\end{align}
\hint 
Notice that $d(p,q)=D_R( (p,1-p), (q,1-q))$, where $D_R$ is Bregman divergence with respect to our old friend, the unnormalized negentropy $R$ over $[0,\infty)^2$. Then use Theorem 26.12 from the bandit book.
\tpoints{20}
\end{question}
\begin{solution*}
Let $R$ be the unnormalized negentropy over $[0,\infty)^2$. Then, by Theorem 26.12, for any $x,y\in (0,\infty)^2$,
\begin{align*}
D_R(x,y) = \frac{1}{2}\| x- y\|_{\nabla R(z)}^2
\end{align*}
for some $z$ on the line segment connecting $x$ to $y$.
We have $R(z) = z_1 \log(z_1) + z_2 \log(z_2) - z_1 - z_2$. Hence, $\nabla R(z) = [\log(z_1),\log(z_2)]^\top$ and $\nabla R(z) = \text{diag}(1/z_1,1/z_2)$, both defined for $z\in (0,\infty)^2$.
Thus,
\begin{align*}
D_R(x,y) = \frac{(x_1-y_1)^2}{2 z_1} + \frac{(x_2-y_2)^2}{2 z_2}\,.
\end{align*}
Now choosing $x=(p,1-p)$, $y=(q,1-q)$, we see that $x,y\in (0,\infty)^2$ if $p,q\in (0,1)$.
In this case, with some $\alpha\in [0,1]$,
 $z = \alpha x + (1-\alpha) y 
= (\alpha p + (1-\alpha) q,  \alpha(1-p)+(1-\alpha)(1-q))^\top
= (\alpha p + (1-\alpha) q,  1-(\alpha p+(1-\alpha)q) )^\top$.
Hence, $z_2 = 1-z_1$ and
\begin{align*}
d(p,q) = \frac{(p-q)^2}{2z_1} + \frac{(p-q)^2}{2(1-z_1)} = \frac{(p-q)^2}{2 z_1(1-z_1)}\,.
\end{align*}
Now, $z_1(1-z_1)\ge p^*(1-p^*)$ (the function $z\mapsto z(1-z)$ has a maximum at $z=1/2$ and is decreasing on ``either side'' of the line $z=1/2$).
Putting things together, we get 
\begin{align*}
d(p,q) = \frac{(p-q)^2}{2 z_1(1-z_1)} \le \frac{(p-q)^2}{2 p^*(1-p^*)}\,.
\end{align*}
\qed\par\smallskip\hrule
\end{solution*}

Now, for $n>0$ let $\Ber^{\otimes n}(p)$ denote the $n$-fold product of $\Ber(p)$ with itself, 
so that if $X\sim \Ber^{\otimes n}(p)$ then $X = (X_1,\dots,X_n)$ where $X_i \sim \Ber(p)$ and $(X_1,\dots,X_n)$ is an independent sequence.

Take $\cX = \{0,1\}^n$ and $\cP_n = \{ \Ber^{\otimes n}(p) \,:\, p\in [0,1] \}$.
Let $f:\cP_n \to [0,1]$ be defined by $f(\Ber^{\otimes n}(p)) = p$.
The problem specified by $(\cP_n,f)$ is the problem of estimating the parameter 
of a Bernoulli distribution given $n$ independent observations from the said, unknown distribution.

\begin{question}
Show that for the Bernoulli estimation problem described above,
for $\delta\in [0,1]$ and $0\le \varepsilon^2<1/32$ fixed, there is no $(\delta,\varepsilon)$-sound estimator
of the common mean, unless
 $n\ge \frac{\log(1/(4\delta))}{16 \varepsilon^2} $.
 %Slud's inequality, see Lemma 5.1 from Anthony and Bartlett, improves this by a factor of two. The improvement is because not the answer to Question 4 is used; but in fact one can show that the 1/2 there is unnecessary if we can set p=1/2.
 
\hint Use that $D(P^{\otimes n},Q^{\otimes n}) = n D(P,Q)$ and the statements from the previous two problems.
\tpoints{20}
\end{question}
\begin{solution*}
Assume that there is a $(\delta,\varepsilon)$-sound estimator for the said problem.
By Question~\ref{q:lbcore},
\begin{align*}
\log\left(\frac{1}{4\delta}\right) \le \inf \{ D(P_0,P_1) \,:\, P_0,P_1\in \cP_n \text{ s.t. }
|f(P_0)-f(P_1)|>2\varepsilon \}\,.
\end{align*}
Let $P_i = \Ber^{\otimes n}(p_i)$, $i\in \{0,1\}$.
By the hint and Question~\ref{q:ber}, for $p_0=1/2$, $p_1 = 1/2+\varepsilon'$, $p^* = p_1$ and $p^*(1-p^*) = 1/4-(\varepsilon')^2$, hence
\begin{align*}
D(P_0,P_1) = n d(p_0,p_1) \le n \frac{ (p_0-p_1)^2}{ 2p^*(1-p^*)} \le \frac{2 n (\varepsilon')^2}{1-4 (\varepsilon')^2}\,.
\end{align*}
Using that $f(P_i) = p_i$, we get 
\begin{align*}
\log\left(\frac{1}{4\delta}\right) 
\le 2n \inf \left\{ \frac{ (\varepsilon')^2}{1-4 (\varepsilon')^2} \,:\, \varepsilon'>2\varepsilon \right\}
= \frac{2 n (2\varepsilon)^2}{1-4 (2\varepsilon)^2}\,,
\end{align*}
where the equality follows because $x \mapsto x^2/(1-4x^2)$ is increasing on $[0,1/2)$.
Reordering, we get that
\begin{align*}
n 
\ge \frac{\log\left(\frac{1}{4\delta}\right) }{8\varepsilon^2} (1-16 \varepsilon^2)
\ge \frac{\log\left(\frac{1}{4\delta}\right) }{16\varepsilon^2} 
\,,
\end{align*}
where the last inequality follows from $\varepsilon^2\le 1/32$,
finishing the proof.
\qed\par\smallskip\hrule
\end{solution*}

Now consider the problem when the definition of $f$ is changed to 
\begin{align}
f_\gamma(\Ber^{\otimes n}(p)) = \frac{1}{1-\gamma p}\,,
\label{eq:disf}
\end{align}
where $0<\gamma<1$.

\begin{question}
Show that for the Bernoulli estimation problem described above with $f=f_\gamma$ as in \cref{eq:disf},
with some constants $\gamma_0>0$ and $c_0,c_1>0$, 
for $\delta\in [0,1]$, $\varepsilon\le c_0/(1-\gamma)$, $\gamma\ge \gamma_0$,
the necessary condition for the existence of 
 $(\delta,\varepsilon)$-sound estimator for $(\cP_n,f_\gamma)$ is 
that
 $n\ge c_1 \frac{\log(1/(4\delta))}{(1-\gamma)^3 \varepsilon^2}$.
 
\hint Use the same strategy as in the solution of the previous exercise.
\tpoints{40}
\end{question}
\begin{solution*}
Similarly to the previous calculations,
\begin{align*}
\log\left(\frac{1}{4\delta}\right) \le n \inf \left\{ \frac{(p_0-p_1)^2}{2 p^*(1-p^*)} \,:\, |f(p_0)-f(p_1)|>2\varepsilon, p_0,p_1\in (0,1)\right\}\,.
\end{align*}
Let
\begin{align*}
p_0 = \frac{4}{3} - \frac{1}{3\gamma}\,.
\end{align*}
It will be useful to note that
\begin{align}
1-\gamma p_0 = \frac{4}{3}(1-\gamma)\,,
\label{eq:gp1}
\end{align}
and
\begin{align}
1- p_0 = \frac{1-\gamma}{3\gamma}\,.
\label{eq:gp2}
\end{align}

Choose $\gamma_0$ so that for any $\gamma \ge \gamma_0$, $p_0\ge 1/2$. 
We calculate
\begin{align*}
f'(p) = \frac{\gamma}{(1-\gamma p)^2}\,.
\end{align*}
Note that both $f$ and $f'$ are increasing.
Now, for $p_1>p_0$, for some $z\in [p_0,p_1]$, we have
\begin{align*}
f(p_1) = f(p_0) + f'(z)(p_1-p_0) \ge f(p_0) + f'(p_0)(p_1-p_0)
\end{align*}
and thus $f(p_1)>f(p_0)+2\varepsilon$ if
$f(p_0) + f'(p_0)(p_1-p_0) >f(p_0)+2\varepsilon$,
or, equivalently,
$p_1-p_0>2\varepsilon/f'(p_0)$.
Note that 
\begin{align}
p_0':=p_0+2\varepsilon/f'(p_0)< 1
\label{eq:p1ub}
\end{align}
provided that
\begin{align}
\varepsilon < \frac{1}{2}\, \frac{3}{32(1-\gamma)}\,.
\label{eq:epsr}
\end{align}
Indeed, from the expression for $f'$ and \cref{eq:gp1},
\begin{align*}
f'(p_0) = \left(\frac34\right)^2 \frac{\gamma}{(1-\gamma)^2}\,,
\end{align*}
and thus
\begin{align*}
p_0'-1 = p_0+2\varepsilon/f'(p_0) -1 
& = \frac{4}{3} - \frac{1}{3\gamma} + \left(\frac43\right)^2 \frac{2\varepsilon}{\gamma} (1-\gamma)^2-1
 = \frac{4\gamma -1 +\frac{32}{3} \varepsilon (1-\gamma)^2 - 3\gamma}{3\gamma}\\
& = \frac{1}{3\gamma} \left[\frac{32\varepsilon}{3} (1-\gamma)^2 - (1-\gamma)\right] \\
& = \frac{(1-\gamma)}{3\gamma} \left[\frac{32\varepsilon}{3} (1-\gamma) - 1\right] \\
& \le  -\frac{(1-\gamma)}{6\gamma} < 0
\numberthis \label{eq:p0ex} 
\,,
\end{align*}
provided that \cref{eq:epsr} holds.

Since $p_1\ge p_0 \ge 1/2$, $p^* = p_1$. Putting things together,
\begin{align*}
\log\left(\frac{1}{4\delta}\right) 
& 
\le
n \inf \left\{ \frac{(p_0-p_1)^2}{2 p_1(1-p_1)} \,:\, p_1\in (0,1)
\text{ s.t. } p_1-p_0>2\varepsilon/f'(p_0)
\right\} \\
%& 
%\le
%n \inf \left\{ \frac{(p_0-p_1)^2}{2 p_1(1-p_1)} \,:\, p_1\in (0,1)
%\text{ s.t. } p_1-p_0>\left(\frac43\right)^2 \frac{2\varepsilon }{\gamma}(1-\gamma)^2
%\right\} \\
& = 
n \frac{\left(\frac43\right)^4 \frac{4 \varepsilon^2 }{\gamma^2}(1-\gamma)^4}{2 p_0'(1-p_0')}
\tag{assuming \cref{eq:epsr} so that \cref{eq:p1ub} holds}\\
& \le
n \frac{\left(\frac43\right)^4 \frac{4 \varepsilon^2 }{\gamma^2}(1-\gamma)^4 6\gamma}{2 p_0(1-\gamma)} 
\tag{by \cref{eq:p0ex} and $p_0'\ge p_0$}\\
& \le
n \,6\left(\frac43\right)^4 \frac{4 \varepsilon^2 }{\gamma_0}(1-\gamma)^3 \,.
\tag{since $p_0\ge 1/2$ and $\gamma\ge \gamma_0$}
\end{align*}
Reordering gives the result.
\qed\par\smallskip\hrule
\end{solution*}


%\begin{question}
%\end{question}
%\begin{solution*}
%\qed\par\smallskip\hrule
%\end{solution*}


\bigskip
\bigskip

\noindent
\textbf{
Total for all questions: \arabic{DocPoints}}.
Of this, up to 70 can be bonus marks. You can receive bonus marks by asking/upvoting questions, for a total of 70 bonus marks!
You must ask at least one question in one of the Lecture Discussion Threads by the Assignment 4 deadline to receive 50 bonus marks.
You can also receive 5 bonus marks for upvoting at least one question before 8am on the day of each lecture, for a maximum of 5 marks x 4 lectures = 20 marks for upvoting.
Your assignment will be marked out of \arabic{DocPoints} minus the bonus marks you received.

% Total for all questions: \arabic{DocPoints}}.
% Of this, $100$ are bonus marks (i.e., $100$ marks worth $100\%$ on this problem set).


\end{document}




