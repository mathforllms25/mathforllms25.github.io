\documentclass{article}
\newcommand{\hwnumber}{1}

\newcommand{\norm}[1]{\| #1 \|}
\newcommand{\abs}[1]{| #1 |}

\usepackage{fullpage,amsthm,amsmath,amssymb}
\usepackage{algorithm,algorithmic}
\usepackage{mathtools}
\usepackage{bbm,bm}
\usepackage{enumerate}
\usepackage{xspace}
\usepackage[textsize=tiny,
%disable
]{todonotes}
\newcommand{\todot}[1]{\todo[color=blue!20!white]{T: #1}}
\newcommand{\todoc}[1]{\todo[color=orange!20!white]{Cs: #1}}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=black]{hyperref}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\usepackage{comment}

\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator*{\Exp}{\mathbf{E}}
\DeclareMathOperator*{\1}{\mathbbm{1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\E}{\mathbb E}
\newcommand{\V}{\mathbb V}
\renewcommand{\P}[1]{P\left\{ #1 \right\}}
\newcommand{\Prob}[1]{\mathbb{P}( #1 )}
\newcommand{\real}{\mathbb{R}}
\renewcommand{\b}[1]{\mathbf{#1}}
\newcommand{\EE}[1]{\E[#1]}
\newcommand{\bfone}{\1}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\cF}{\mathcal{F}}
\usepackage[capitalize]{cleveref}
\usepackage{xifthen}

% total number of points that can be collected
\newcounter{DocPoints} % counter reset to zero upon creation

% counting points per question (not user facing)
\newcounter{QuestionPoints} % counter reset to zero upon creation

% Points for a subquestion of a question; 
% Adds the points to the total for the question and the document.
\newcommand{\points}[1]{%
	\par\mbox{}\par\noindent\hfill {\bf #1 points}%
	\addtocounter{DocPoints}{#1}
	\addtocounter{QuestionPoints}{#1}
}
% Points for a question; call with no params if the question
% had subquestions. In this case it prints the total for the question (see \points).
% Otherwise call with the points that the question is worth.
% In this case, the total is added to the document total.
% It is a semantic error to call this with a non-empty parameter
% when a question had subquestions with individual scores.
\newcommand{\tpoints}[1]{        %
	\ifthenelse{\isempty{#1}}%
	{%
	}%
	{%
		\addtocounter{DocPoints}{#1}
		\addtocounter{QuestionPoints}{#1}
	}													 %
	\par\mbox{}\par\noindent\hfill {Total: \bf \arabic{QuestionPoints}\xspace points}\par\mbox{}\par\hrule\hrule
	\setcounter{QuestionPoints}{0}
}
\newcommand{\tpoint}[1]{
	\tpoints{#1}
}

\theoremstyle{definition}
\newtheorem{question}{Question}
\newtheorem{assumption}{Assumption}
\newtheorem*{assumption*}{Assumption}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem*{remark*}{Remark}
\newtheorem{solution}{Solution}
\newtheorem*{solution*}{Solution}

\excludecomment{solution}
%\excludecomment{solution*}

\newcommand{\hint}{\noindent \textbf{Hint}:\xspace}

\usepackage{hyperref}

\newcommand{\epssub}{\delta}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\sA}{\mathcal{A}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}


\begin{document}

\begin{center}
{\Large \textbf{CMPUT 653: Theoretical Foundations of Reinforcement Learning, Winter 2022\\ Midterm}}
\end{center}

\section*{Instructions}
\noindent \textbf{Submissions}
You need to submit a zip file, named {\tt midterm\_<name>.zip} 
or {\tt midterm\_<name>.pdf} 
where {\tt <name>} is your name.
The zip file should include a report in PDF, typed up (we strongly encourage to use pdf\LaTeX) and the code that we asked for. Write your name on your solution.
I provide a template that you are encouraged to use.
You have to submit the zip file on the eclass website of the course.

\noindent \textbf{Collaboration and sources}
Work on your own. No consultation, etc.
Students are expected to understand and explain all the steps of their proofs.

\noindent \textbf{Scheduling}
Start early: It takes time to solve the problems, as well as to write down the solutions. Most problems should have a short solution (and you can refer to results we have learned about to shorten your solution). Don't repeat calculations that we did in the class unnecessarily.

\vspace{0.3cm}

\noindent \textbf{Deadline:} February 25 at 11:55 pm

\newcommand{\cM}{\mathcal{M}}
\newcommand{\nS}{\mathrm{S}}
\newcommand{\nA}{\mathrm{A}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ip}[1]{\langle #1 \rangle}

\section*{Undiscounted infinite horizon problems}


Let $M = (\cS,\cA,P,r)$ be a finite MDP as usual, but this time consider the infinite horizon undiscounted total reward criterion. In this setting, the value of policy $\pi$ (memoryless or not) is
\begin{align*}
v^\pi(s) = \E^{\pi}_s\left[ \sum_{t=0}^\infty r_{A_t}(S_t) \right]\,.
\end{align*}
To guarantee that this value exist we make the following assumption on the MDP $M$:

\newcommand{\term}{s^{\star}}
\begin{assumption}[All policies proper]\label{ass:app}
Assume that the MDP $M$ has a state $\term$ such that the following hold:
\begin{enumerate}
\item For all actions $a\in \cA$, $P_a(\term,\term)=1$ (and thus, $P_a(\term,s')=0$ for any $s'\ne \term$ state of the MDP);
\item For all actions $a\in \cA$, $r_a(\term)=0$;
\item The rewards are all nonnegative;
\item For any policy $\pi$ of the MDP (memoryless or not), 
and for any $s\in \cS$,
$\sum_{t\ge 0}\PP^{\pi}_s(S_t \ne \term)<\infty$.
\end{enumerate}
\end{assumption}
{\color{red} In this section we assume that \cref{ass:app} holds even if this is not explicitly mentioned.}

\newcommand{\eR}{\bar \R}
\newcommand{\BB}{\mathbb{B}}
\newcommand{\one}[1]{\mathbb{I}\{#1\}}
\begin{question}
\label{q:ex}
Show that  the value of any policy $\pi$ can indeed be ``well-defined'' in the following sense:
Let
$(\Omega,\cF)$ be the measurable space that holds the random variables $(S_t,A_t)_{t\ge 0}$.
\begin{enumerate}
\item If we take $R=\sum_{t=0}^\infty r_{A_t}(S_t)$, this is well-defined as an \emph{extended real random variable} from the measurable space $(\Omega,\cF)$ to $(\eR,\BB(\eR))$ where $\eR = \R\cup\{-\infty,+\infty\}$ is the set of \emph{extended reals} 
and $\BB(\eR)$ is the ``natural'' Borel $\sigma$-algebra over $\eR$ 
defined using $\BB(\eR) =\sigma( \{[-\infty,x]\,:\, x\in \eR \})$ (i.e., the smallest $\sigma$-algebra generated by the set system in the argument of $\sigma$).
\points{5}
\item For any policy $\pi$ and state $s\in \cS$, 
under $\PP_s^\pi$,
the expectation of $R$ exists and is finite.
\points{20}
\end{enumerate}
\hint For Part 1, recall the closure properties of the collection of extended real random variables (e.r.r.v.). 
Start your argument with showing that $r_{A_t}(S_t)$ is a random variable and build up things from there.
For Part 2, recall that the expected value of a nonnegative e.r.r.v is equal to the limit of expected values assigned to simple functions below it provided that the limit of these simple functions converges to the e.r.r.v. 
For Part 2, see Prop 2.3.2 and for Part 1 see Prop 2.1.5 in (for example)
this book
\href{https://www.dropbox.com/s/3gi7k35j3jgcftp/2006_Book_MeasureTheoryAndProbabilityThe.pdf}{here}.%
\footnote{
Krishna B. Athreya and Soumendra N. Lahiri. Measure Theory and Probability Theory. Springer, 2006.}
\tpoints{}
\end{question}
\begin{solution*}
Part 1:
By definition, $S_t,A_t$ are random variables.
By a slight abuse of notation, let the map $r: \cS \times \cA \to \R$ be defined by $r(s,a) = r_a(s)$.
Taking the discrete topology on $\cS \times \cA$ makes $r$ a continuous map (any map from a discrete topological space to any other topological space is continuous).
Hence, $r_{A_t}(S_t)$ is a random variable. Since the sum of finitely many random variables is again a random variable, it follows that for any
$t\ge 0$, $R_t = \sum_{s=0}^t r_{A_t}(S_t)$ is also a random variable.
Finally, $R = \sup_{t\ge 0} R_t$, since $R_t$ is a nondecreasing sequence by our assumptions on the rewards. Since $R$ is the supremum of a countable collection of random variables, it is also a random variable.

Part 2:
Fix policy $\pi$ and state $s\in \cS$. 
Since $R$ is a nonnegative extended real random variable, its expectation is well-defined.
Furthermore, the expected value can be obtained by taking \emph{any} sequence of simple functions $f_n$ from $(\Omega,\cF)$ to the set of reals that approaches $R$ from below
and calculating $\lim_{n\to\infty} E_s^\pi[f_n]$.
We choose $f_n = R_n$. 
To argue that $R_n$ is a simple function write it as 
\begin{align}
R_n = \sum_{t=0}^n \sum_{s\ne \term} \sum_a \mathbb{I}_{\{U_{t,s,a}\}} r_a(s)\,,
\label{eq:rnrepr}
\end{align}
where for $t\ge 0$, $(s,a)\in \cS\times \cA$, $U_{t,s,a} = \{ S_t=s, A_t = a\}$.
Here, we restrict the sum to $s\ne \term$: Despite this, the equality holds 
since $r_a(\term)=0$ for any action $a\in \cA$. This restriction is useful for the next step.

Now, as noted before, 
since the rewards are nonnegative, $R_n \le R$ and thus $\sup_{n} R_n = \lim_{n\to\infty} R_n = R$.
Hence, 
\begin{align*}
\E_s^{\pi}[R]= \lim_{n\to\infty} \E_s^{\pi}[R_n]\,.
\end{align*}
Thus, to show that $\E_s^{\pi}[R]$ is finite, it suffices to show that the limit on the right-hand side is finite.
From \eqref{eq:rnrepr},
\begin{align*}
\E_s^\pi[R_n] 
& = \sum_{t=0}^n \sum_{s\ne \term} \sum_a r_a(s) \PP_s^\pi( S_t=s, A_t = a )\\
&\le \max_{s,a} r_a(s) \,  \sum_{t=0}^n \sum_{s\ne \term}  \PP_s^\pi( S_t=s ) \\
&= \max_{s,a} r_a(s) \,  \sum_{t=0}^n \PP_s^\pi( S_t\ne \term ) \\
&\le \max_{s,a} r_a(s) \,  \sum_{t=0}^\infty \PP_s^\pi( S_t\ne \term ) < +\infty\,,
\end{align*}
where the last inequality used that, by assumption, 
$\sum_{t=0}^\infty \PP_s^\pi( S_t\ne \term ) <\infty$.
\qed\par\smallskip\hrule
\end{solution*}

The last part of the previous problem allows us to define the value of $\pi$ in state $s$ using the usual formula
\begin{align*}
v^\pi(s) = \E_s^\pi[ R ]
\end{align*}
and note that regardless of $\pi$ and $s$, these values are always finite.

For a memoryless policy $\pi$ and $s,s'\ne \term$, 
define $P_{\pi}(s,s') = \sum_{a\in \cA} \pi(a|s) P_a(s,s')$, i.e., the usual way.
We can also view $P_{\pi}$, as usual, an $(\nS-1)\times (\nS-1)$ matrix by identifying $\cS$ with $\{1,\dots,\nS\}$, $\term=\nS$.

\begin{question}[Transition matrices]
Show that for any $s,s'\in \cS$, $s,s'\ne \term$, and $t\ge 1$, $(P_\pi^t)_{s,s'} = \PP_s^\pi(S_t=s')$.
\tpoints{10}
\end{question}
\begin{solution*}
The solution is the same as the solution of Q2 on Assignment 1 with a few changes, which are marked by {\color{red} red}.
Fix any $t\ge 0$. Fix also $\pi$ and $s_0\in \cS$. We abbreviate $\PP_{s_0}^\pi$ to $\PP$ in what follows (we changed $s$ to $s_0$ so that there is no clash with indexing of states below while we can reduce clutter).
Detto for $\E_{s_0}^\pi$ and $\EE$.
Recall that $H_t = (S_0,A_0,\dots,S_{t-1},A_{t-1},S_t)$.
Fix $s'\in \cS$.
By the tower rule of conditional expectations (applied twice),
\begin{align*}
\PP(S_{t+1}=s')
=\EE{ \EE{ \PP(S_{t+1}=s'|H_t,A_t)  | H_t } }\,.
\end{align*}
For the innermost expectation (probability, actually) we have
\begin{align*}
\PP(S_{t+1}=s'|H_t,A_t) = P_{A_t}(S_t,s')
\end{align*}
by the construction of $\PP$.
Now,
\begin{align*}
\EE{ P_{A_t}(S_t,s') | H_t } = \sum_{a\in \cA} \pi_t(a|H_t) P_a(S_t,s')
\end{align*}
and since $\pi$ is memoryless, $\pi_t(a|H_t) = \pi(a|S_t)$. 
Hence, the expression in the right-hand side is $P_\pi(S_t,s')$ {\color{red} when $S_t\ne \term$.
When $S_t=\term$, $P_{A_t}(S_t,s')=0$.
}
Plugging this in, using the law of total expectations,
\begin{align*}
\EE{ P_{\pi}(S_t,s') } =
 \sum_{{\color{red} s \ne \term}} P_{\pi}(s,s') \PP(S_t=s)\,.
\end{align*}
Putting everything together we see that
\begin{align*}
\PP(S_{t+1}=s') = \sum_{{\color{red} s \ne \term}} P_{\pi}(s,s') \PP(S_t=s)
\end{align*}
which, together with $\PP(S_0=s)=\delta_{s_0}(s)$ implies the desired statement.
Indeed, for $t=0$ we get
\begin{align*}
\PP(S_{1}=s') = P_{\pi}(s_0,s') = P_{\pi}(s_0,s')\,,
\end{align*}
and hence, by induction,
\begin{align*}
\PP(S_{t+1}=s') = \sum_{{\color{red} s \ne \term}}  P_\pi^t(s_0,s) P_{\pi}(s,s')
= P_\pi^{t+1}(s_0,s')\,.
\end{align*}
\qed\par\smallskip\hrule
\end{solution*}

\begin{question}
Prove that for any memoryless policy $\pi$, defining $r_\pi(s) = \sum_a \pi(a|s)r_a(s)$, as usual,
we have
$v^\pi = \sum_{t\ge 0} P_\pi^t r_\pi$, where when viewed as vectors, $v^\pi$ and $r_\pi$ are restricted to $s\ne \term$ (i.e., they are $(\nS-1)$-dimensional).

\hint You may want to reuse the result of the previous exercise.
\tpoints{10}
\end{question}
\begin{solution*}
Let $s,s'\ne \term$, $\PP = \PP_s^\pi$.
By the result of the previous exercise, for $t\ge 1$, $\PP(S_t=s') = P_\pi^t(s,s')$.
By the tower rule and Lebesgue's dominated convergence theorem,
\begin{align*}
v^\pi(s) =\sum_{t\ge 0}  \EE{ \EE{ r_{A_{t}}(S_{t}) | H_t } }\,.
\end{align*}
For the innermost expectation we have
\begin{align*}
\EE{ r_{A_{t}}(S_{t}) | H_t }
= \sum_{a\in \cA} \pi_t(a|H_t) r_a(S_t)
= \sum_{a\in \cA} \pi(a|S_t) r_a(S_t) = r_\pi(S_t)\,,
\end{align*}
because $\pi$ is memoryless.
Plugging this in and using the law of total expectations we get
\begin{align*}
 \EE{ \EE{ r_{A_{t}}(S_{t}) | H_t } }  = \sum_{{\color{red} s' \ne \term}} \PP(S_t=s') r_\pi(s')\,,
\end{align*}
{\color{red} where we used that for $s'=\term$, $r_\pi(s')=0$, hence the sum can be restricted to $s\ne \term$.}
Since for $t=0$, $\PP(S_t=s') = 1$ iff $s'=s$, this together with the result of the previous problem gives
that
\begin{align*}
v^\pi(s) = \sum_{t\ge 0}  \sum_{{\color{red} s' \ne \term}} P_\pi^t(s,s') r_\pi(s')
\end{align*}
(recall that $A^0$ is the identity matrix for any square matrix $A$).
Using a matrix-vector notation we can write the above display as 
\begin{align*}
v^\pi = \sum_{t\ge 0}  P_\pi^t r_\pi\,.
\end{align*}
\qed\par\smallskip\hrule
\end{solution*}

\begin{question}[Policy evaluation fixed-point equation]
\label{q:pefp}
Show that for $s\ne \term$, $v^\pi$ satisfies
\begin{align*}
v^\pi(s) = r_\pi(s) + \sum_{s'\ne \term} P_\pi(s,s') v^\pi(s')\,.
\end{align*}
\tpoints{2}
\end{question}
\begin{solution*}
By the previous problem $v^\pi = r_\pi + \sum_{t\ge 1} P_\pi^t r_\pi = r_\pi + P_\pi (\sum_{t\ge 0} P_\pi^t r_\pi) = r_\pi + P_\pi v^\pi$.
\qed\par\smallskip\hrule
\end{solution*}

Define now the $w(s)$ as the total expected reward incurred under $\pi$ when it is started from $s$ and \emph{in each time step the reward incurred is one} until $\term$ is reached (that is, $r_a(s)$ is replaced by $1$ for $s\ne \term$, while the zero rewards are kept at $\term$).
By our previous result, $w$ is well-defined.
Furthermore,
\begin{align*}
w(s)\ge 1\,, \qquad s\ne \term
\end{align*}
as for $s\ne \term$, in the zeroth period, a reward of one is incurred and in all subsequent periods the rewards incurred are nonnegative.

Introduce now the weighted norm, $\norm{\cdot}_w$: For $x\in \R^{\nS-1}$,
\begin{align*}
\norm{x}_w = \max_{s\in [\nS-1]} \frac{|x_s|}{w(s)}\,.
\end{align*}

When the dependence on $\pi$ is important, we will use $w_\pi$.

\begin{question}[Contractions]
\label{q:contr}
Show that $P_\pi$ is a contraction under $\norm{\cdot}_w$, that is,
there exists $0\le \rho <1$ such that
for any $x,y\in \R^{\nS-1}$,
\begin{align*}
\norm{P_\pi x- P_\pi y}_w \le \rho \norm{x-y}_w\,.
\end{align*}
\tpoints{15}
\end{question}
\begin{solution*}
Since $P_\pi$ is linear,
$P_\pi x- P_\pi y= P_\pi(x-y)$.
Hence, it suffices to show that for any $u\in \R^{\nS-1}$,
\begin{align*}
\norm{P_\pi u}_w \le \rho \norm{u}_w\,.
\end{align*}
Fix any $u\in \R^{\nS-1}$ and $i\in [\nS-1]$.
Then, using that $w\ge 1$ and hence is positive,
\begin{align}
\left\vert \frac{(P_\pi u)(i)}{w(i)} \right\vert
& \le
\frac{1}{w(i)}
 \sum_{j=1}^{\nS-1} P_\pi(i,j) w(j) \left\vert \frac{u(j)}{w(j)}\right\vert
 \le
\norm{u}_w \frac{\sum_{j=1}^{\nS-1} P_\pi(i,j) w(j) }{w(i)} \,.
\label{eq:gub}
\end{align}
Now, recall that by the solution to Question~\ref{q:pefp},
\begin{align*}
w(i)=1+\sum_{j=1}^{\nS-1} P_\pi(i,j) w(j)\,.
\end{align*}
Hence,
\begin{align*}
\frac{\sum_{j=1}^{\nS-1} P_\pi(i,j) w(j) }{w(i)} 
=
\frac{w(i)-1 }{w(i)} 
\le
\max_{1\le i \le \nS-1} \frac{w(i)-1 }{w(i)} =:\rho\,.
\end{align*}
and thanks to $0\ge w(i)-1 <w(i)$ and since $\nS$ is finite, $0\le \rho <1$.
Plugging this into \eqref{eq:gub}, we get 
\begin{align*}
\norm{P_\pi u}_w = \max_i \left\vert \frac{(P_\pi u)(i)}{w(i)} \right\vert
 \le
\rho \norm{u}_w 
\end{align*}
finishing the proof.
\qed\par\smallskip\hrule
\end{solution*}

We can define occupancy measures as before: For $s\ne \term$, policy $\pi$ and initial state distribution $\mu$ defined over $\term\not\in \cS':= \{1,\dots,\nS-1\}$,
\begin{align*}
\nu_\mu^\pi(s, a) = \sum_{t=0}^\infty  \mathbb{P}_\mu^\pi (S_t = s, A_t = a).
\end{align*}
Clearly, this is well-defined under our standing assumption (by Question~\ref{q:ex}).
Noting that rewards from $\term$ are all zero, we have
\begin{align*}
v^\pi(\mu) = \ip{\nu_\mu^\pi,r}\,.
\end{align*}


\begin{question}
\label{q:occ}
Show that for any policy $\pi$ and distribution $\mu \in \cM_1(\cS')$ there is a memoryless policy $\pi'$ such that $\nu_\mu^\pi = \nu_\mu^{\pi'}$.
\tpoints{10}
\end{question}

\begin{solution*}
The solution is the almost verbatim copy of the solution to Question 8 on Assignment 1. Discount factors need to be dropped. Other changes in the solution are indicated by {\color{red} red}.
For arbitrary $\mu,\pi$, $s\in {\color{red} \cS'}$,
define
\begin{align*}
\tilde \nu_\mu^\pi(s) = \sum_{a\in \cA} \nu_\mu^\pi(s,a)\,,
\end{align*}
the `marginal' of $\nu_\mu^\pi\in \cM_1(\cS'\times \cA)$ over the states.
Clearly, 
\begin{align*}
\tilde \nu_\mu^\pi(s)  = \sum_{t\ge 0} \PP_\mu^\pi(S_t=s)\,.
\end{align*}
When $\pi$ is a memoryless policy, since $\PP_\mu^\pi(S_t=s) = \mu P_\pi^t e_s$, 
\begin{align} \label{eq:discstateocmem}
\tilde \nu_\mu^\pi =  \mu \sum_{t\ge 0} P_\pi^t\,.
\end{align}

Now fix $\mu,\pi$ as in the theorem 
and pick an arbitrary distribution $\pi_0\in \cM_1(\cA)$ over the actions. 
Define $\pi'$ as follows: For $s\in \cS'$,
\begin{align*}
\pi'(a|s) = 
\begin{cases}
\frac{\nu_\mu^\pi(s,a)}{{\tilde{\nu}}_\mu^\pi(s)}\,, & \text{if } \tilde \nu_\mu^\pi(s)\ne 0
{\color{red} \text{ and } s\ne \term}\,;\\
\pi_0(a)\,, & \text{otherwise}\,.
\end{cases}
\end{align*}
We will now argue that this policy is indeed suitable. In particular, we will show that 
\begin{align}
\label{eq:statemargeq}
\tilde \nu_\mu^\pi = \mu +  \tilde \nu_\mu^\pi P_{\pi'},
\end{align}
which implies the result since viewing this as a (linear) 
equation in $\tilde \nu_\mu^\pi$, the unique solution to this equation is $\tilde \nu_\mu^{\pi'}$, the  occupancy measure of $\pi'$ over the states.
{\color{red} Indeed,
by \cref{eq:discstateocmem}, $\tilde \nu_\mu^{\pi'}$ is indeed a solution and by
 the solution to Question~\ref{q:contr}, since $P_\pi'$ is a contraction, by Banach's fixed point theorem, it is also a unique solution.}
Thus, $\tilde \nu_\mu^\pi = \tilde\nu_\mu^{\pi'}$ and thus, {\color{red} for $s\ne \term$,}
\begin{align*}
\nu^\pi_\mu(s,a) = \tilde \nu_\mu^\pi(s) \pi'(a|s) = \tilde \nu_\mu^{\pi'}(s) \pi'(a|s) = \nu_\mu^{\pi'}(s,a)\,,
\end{align*}
where the last equality follows from the definitions of $\tilde \nu_\mu^{\pi'}$ and $\nu_\mu^\pi$ and the fact that $\pi'$ is memoryless.

It remains to show that \eqref{eq:statemargeq} holds. For this, we have
\begin{align*}
\tilde \nu_\mu^\pi(s)
& = \sum_{t\ge 0} ^t \PP_\mu^\pi(S_t=s) \\
& = \mu(s) +  \sum_{t\ge 0} \PP_\mu^\pi(S_{t+1}=s) \\
& = \mu(s) +  \sum_{s_{\text{prev}},a} \underbrace{\sum_{t\ge 0}  \PP_\mu^\pi(S_{t}=s_{\text{prev}},A_t=a)}_{\nu_\mu^\pi(s_{\text{prev}},a)} P_{a}(s_{\text{prev}},s) \\
& = \mu(s) +  \sum_{s_{\text{prev}}} \tilde \nu_\mu^\pi(s_{\text{prev}}) \sum_a \pi'(a|s_{\text{prev}}) P_{a}(s_{\text{prev}},s) \\
& = \mu(s) +  \sum_{s_{\text{prev}}} \tilde \nu_\mu^\pi(s_{\text{prev}}) P_{\pi'}(s_{\text{prev}},s)\,,
\end{align*}
which is equivalent to \eqref{eq:statemargeq}. Here, the last equality follows from 
the definition of $P_{\pi'}$.
\qed\par\smallskip\hrule
\end{solution*}

Define $v^*(s) = \sup_{\pi} v^\pi(s)$ and define 
$T: \R^{\nS-1} \to \R^{\nS-1}$ by
$(T v)(s) = \max_a r_a(s) + \ip{P_a(s),v}$, $s\ne \term$.
For a memoryless policy, we also let $T_\pi v = r_\pi + P_\pi v$ (using vector notation).
Greediness is defined as usual: $\pi$ is greedy w.r.t. $v\in \R^{\nS-1}$, if $T_\pi v = T v$.

\begin{question}[The Fundamental Theorem for Undiscounted Infinite-Horizon MDPs]
Show that  the fundamental theorem still holds:
\begin{enumerate}
\item The optimal value function $v^*$ is well-defined (i.e., finite);
\points{20}
\item Any policy  that is greedy with respect to $v^*$  is optimal: $v^\pi = v^*$;
\item It holds that $v^* = Tv^*$.
\points{10}
\end{enumerate}
\tpoints{}
\end{question}
\begin{solution*}
By Question~\ref{q:occ},
for any policy $\pi$ and a start state distribution $\mu \in \mathcal{M}_1(\{1,\dots,\nS-1\})$, there exists a memoryless policy $\pi'$ such that
\begin{align*}
\nu_\mu^{\pi'} = \nu_\mu^{\pi}.
\end{align*}

We copy the proof given in Lecture 2, with modifications, shown, again in {\color{red} red}.
The proof would be easy if we only considered memoryless policies when defining $v^*$. 
In particular, letting $\text{ML}$ stand for the set of memoryless policies of the given MDP, define
\begin{align*}
\tilde{v}^*(s) = \sup_{\pi \in \text{ML}} v^\pi(s) \quad \text{for all } s \in \mathcal{S}\,.
\end{align*}
{\color{red} Because the supremum is over a smaller set, $\tilde{v}^* \le v^*$.}

As we shall see soon, it is not hard to show the theorem just with $v^*$ replaced everywhere with $\tilde{v}^*$.
That is:
\begin{enumerate}
\item {\color{red} $\tilde{v}^*$ is well-defined;}
\item Any policy $\pi$ that is greedy with respect to $\tilde{v}^*$ satisfies $v^\pi = \tilde{v}^*$;
\item It holds that $\tilde{v}^* = T \tilde{v}^*$.
\end{enumerate}

This is what we will show in Part 1 of the proof, while in Part 2 we will show that 
{\color{red} $\tilde{v}^* \ge v^*$ and thus $v^*$ is also well-defined and}
$\tilde{v}^*=v^*$.
Clearly, the two parts together establish the desired result.

\noindent \textbf{Part 1}:
{\color{red} We start by establishing that $\tilde{v}^*$ is well-defined.
For this, fix $s\in \cS$. We want to show that $\tilde{v}^*(s)<\infty$.
Let $(\pi_k)_k$ be a sequence of memoryless policies such that $\lim_{k\to\infty} v^{\pi_k}(s) = \tilde{v}^*(s)$ (which could be infinite).
By possibly considering a subsequence, we may assume that $(\pi_k)_k$ itself is convergent. This is because we can view $\pi_k\in \Delta_1(\cA)^{\cS}$, where $\Delta_1$ is the set of probability vectors over $\cA$ and thus $\pi_k$ takes values in a compact subset of a Euclidean space, hence, it has a convergent subsequence.
Now, let $\pi(a|s) = \lim_{k\to\infty} \pi_k(a|s)$, $(s,a)\in \cS \times \cA$ (the pointwise limit of $\pi_k$).
Then, by Question \ref{q:ex}, $v^\pi(s)<+\infty$. 
We now claim that $v^{\pi_k}(s) \to v^{\pi}(s)$ as $k\to\infty$.
Indeed, $v^{\pi_k} - v^{\pi} = (I-P_{\pi_k})^{-1}[T_{\pi_k} v^{\pi} - v^{\pi} ]$
and $T_{\pi_k} v^{\pi} - v^{\pi} = T_{\pi_k} v^{\pi} - T_\pi v^{\pi} = r_{\pi_k}-r_\pi + (P_{\pi_k}-P_{\pi}) v^\pi$. Let $w = w_{\pi}$.
Then, $\norm{v^{\pi_k} - v^{\pi}}_w = 
\norm{ (I-P_{\pi_k})^{-1} }_w \left( \norm{ r_{\pi_k} -r_{\pi}}_w + \norm{ P_{\pi_k}-P_{\pi}}_w \norm{v^\pi}_w \right)\to 0$ as $k\to\infty$ since, by a calculation
similar to that given in the solution of Question~\ref{q:contr},
one can show that for $k$ large enough, $\norm{P_{\pi_k}}_w \le \frac{1+\rho}{2}<1$ where $\rho$ is the 
contraction coefficient for $P_{\pi}$ defined in the solution of that problem, and hence
for $k$ large enough,
$\norm{ (I-P_{\pi_k})^{-1} }_w = \norm{\sum_{t\ge 0} (P_{\pi_k})^t }_w \le \frac{1}{1-\frac{1+\rho}{2}}=\frac{2}{1-\rho}<\infty$ and
$\norm{ r_{\pi_k} -r_{\pi}}_w\to 0$
and $\norm{ P_{\pi_k}-P_{\pi}}_w\to 0$ by the continuity of $\norm{\cdot}_w$ while
$\norm{v^\pi}_w<\infty$ because $v^\pi$ is finite valued and there are finitely many states.
Hence, $\tilde{v}^*(s) = \lim_{k\to\infty} v^{\pi_k}(s) = v^{\pi}(s)<+\infty$.
}

The idea of the proof {\color{red} then} is  to first show that
\begin{align}
\tilde{v}^*\le T \tilde{v}^*
\label{eq:suph}
\end{align}
and then show that for any greedy policy $\pi$, $v^\pi \ge \tilde{v}^*$.

The displayed equation follows by noticing that
$v^\pi \le \tilde{v}^*$ holds for all memoryless policies $\pi$ by definition. Applying $T_\pi$ on both sides, using $v^\pi = T_\pi v^\pi$, we get $v^\pi \le T_\pi \tilde{v}^*$. Taking the supremum of both sides over $\pi$ and noticing that $T v = \sup_{\pi \in \text{ML}} T_\pi v$ for any $v$, together with the definition of $\tilde{v}^*$ gives $\eqref{eq:suph}$.

Now, take any memoryless policy $\pi$ that is greedy w.r.t. $\tilde{v}^*$. Thus, $T_\pi \tilde{v}^* = T \tilde{v}^*$.

Combined with \eqref{eq:suph}, we get
\begin{align}
\label{eq:start}
T_\pi \tilde{v}^* \ge \tilde{v}^*\,.
\end{align}
Applying $T_\pi$ on both sides and noticing that $T_\pi$ keeps the inequality intact (i.e., for any $u,v$ such that $u\le v$ we get $T_\pi u \le T_\pi v$), we get
\begin{align*}
T_\pi^2 \tilde{v}^* \ge T_\pi \tilde{v}^* \ge \tilde{v}^*\,,
\end{align*}
where the last inequality follows from $\eqref{eq:start}$. With the same reasoning we get that for any $k\ge 0$,
\begin{align*}
T_\pi^k \tilde{v}^* \ge T_\pi^{k-1} \tilde{v}^* \ge \dots \ge \tilde{v}^*\,,
\end{align*}

Now, 
{\color{red} by the solution to Question~\ref{q:contr}, $T_\pi$ is easily seen to be a weighted norm-contraction with an appropriate weighted norm defined in that problem and thus}
the fixed-point iteration $T_\pi^k \tilde{v}^*$ converges to $v^\pi$. 
Hence, taking the limit above, we get
\begin{align*}
v^\pi \ge \tilde{v}^*.
\end{align*}
This, together with $v^\pi \le \tilde{v}^*$ shows that $v^\pi = \tilde{v}^*$.
Finally, $T \tilde{v}^* = T_\pi \tilde{v}^* = T_\pi v^\pi = v^\pi = \tilde{v}^*$.

\noindent \textbf{Part 2}:
It remains to be shown that $\tilde{v}^* = v^*$.
Let $\Pi$ be the set of all policies.
Because $\text{ML}\subset \Pi$, $\tilde{v}^*\le v^*$.
Thus, it remains to show
that
\begin{align}
\label{eq:mlbigger}
v^* \le \tilde{v}^*\,.
\end{align}

To show this, we will use 
{\color{red} that by Question~\ref{q:occ},}
for any state-distribution $\mu {\color{red} \in \cM_1(\cS')}$ and policy $\pi$ (memoryless or not) we can find a memoryless policy, which we will call for now $\text{ML}(\pi)$, such that $\nu_\mu^\pi = \nu_\mu^{\text{ML}}$. Fix a state $s\in {\color{red} \cS'}$. 
Applying this result with $\mu = \delta_s$ {\color{red} with $s\in \cS'$}, we get
\begin{align*}
v^\pi(s)
& = \langle \nu_s^\pi, r \rangle \\
& = \langle \nu_s^{\text{ML}(\pi)}, r \rangle \\
& \le \sup_{\pi'\in \text{ML}} \langle \nu_s^{\pi'}, r \rangle \\
& = \sup_{\pi'\in \text{ML}} v^{\pi'}(s) = \tilde{v}^*(s)\,.
\end{align*}

Taking the supremum of both sides over $\pi$, we get
$v^*(s)= \sup_{\pi\in \Pi} v^\pi(s) \le \tilde{v}^*(s)$. Since $s\in{\color{red} \cS'}$ was arbitrary
{\color{red} and for $s=\term$, $v^*(\term)=v^\pi(\term)$ for any policy $\pi$,}
 we get $v^*\le \tilde{v}^*$, finishing the proof. 

\qed\par\smallskip\hrule
\end{solution*}


\begin{question}
\label{q:nonneg}
Imagine that \cref{ass:app} is changed such that all immediate rewards are nonpositive  (at $\term$ the rewards are still zero).
What do you need to change in your answer to the previous questions? Just give a short summary of the changes.
\tpoints{3}
\end{question}
\begin{solution*}
Recall that expectations of nonpositive random variables are defined through taking their negation. 
Hence, we need to consider $-R$, but this brings us back to the nonnegative case.
Nothing else changes.
\qed\par\smallskip\hrule
\end{solution*}

%\begin{question}
%Imagine that \cref{ass:app} is changed such that all immediate rewards are nonpositive (at $\term$ the rewards are still zero).
%Prove that the claims made in Question \ref{q:ex} remain true (and hence all the other results are remain true). It suffices to describe the changes to the solution of Question \ref{q:ex}.
%\tpoints{2}
%\end{question}
%\begin{solution*}
%Duplicate question:) 
%See answer to Question \ref{q:nonneg}.
%\qed\par\smallskip\hrule
%\end{solution*}


\begin{question}
Imagine that \cref{ass:app} is changed such that there is no sign restriction on the rewards, they can be positive, or negative. 
Something will go wrong with the claims made in Question \ref{q:ex}. Explain what.
%Are the claims made in Question \ref{q:ex} still true? If yes, explain why.
%If not, explain why and then try to change the claims so that value functions are still well-defined (and match the positive or negative case) and the claims in the rest of the questions are still true? 
\tpoints{3}
\end{question}
\begin{solution*}
%This is a bit trickier:
%the claims made in Question \ref{q:ex} are not true anymore.
A simple example is when there are two actions, $\cA = \{1,2\}$ and for some state $s\ne \term$, $r_1(s)=+1$ and $r_2(s)=-1$. 
Then, $R$ is not well-defined on the event $\{S_0=s,A_0=1,S_1=s,A_1=2,S_2=s,A_2=1,\dots \}$,
that is, when the actions are alternating between action one and action two.
%
%However, one can show that for any $s\in \cS$, policy $\pi$,
%\begin{enumerate}
%\item $R$ is well-defined on event $U=\{\exists t_0\ge 0 \text{ such that } \forall t\ge t_0\,: S_t = \term \}$;
%\item $\P_s^\pi(U)=1$ and hence $R$ is $\P_s^\pi$-almost surely well-defined;
%\item $R' = R \mathbb{I}_{U}$ is well-defined as an e.r.r.v.
%\end{enumerate}
%we need to take positive and negative parts; but we can do that with $R_n$ as well and still get everything working. In particular, to come up with the ``right'' simple functions $f_n$ for dealing with, say, $R_+$, first write $(R_n)_+$ as
%\begin{align*}
%(R_n)_+ = \sum_{s_0\ne \term ,a_0,\dots,s_n\ne \term ,a_n} (r_{a_0}(s_0)+\dots+r_{a_n})_+ \mathbb{I}_{\{S_0=s_0,A_0=a_0,\dots,S_n=s_n,A_n=a_n\}}
%\end{align*}
%Then let
%\begin{align*}
%aa
%\end{align*}
%
\qed\par\smallskip\hrule
\end{solution*}

%\section*{Fixed horizon undiscounted problems}
%Let $M = (\cS,\cA,P,r)$ be a finite MDP.
%In the fixed horizon undiscounted setting, the value of a policy $\pi$ (memoryless or not) 
%in state $s$ is defined as
%\begin{align*}
%v_H^\pi(s) =\E_s^\pi\left[ \sum_{t=0}^{H-1} r_{A_t}(S_t) \right]\,,
%\end{align*}
%where $H>0$ is the horizon of the problem.
%
%Define $v_H^*(s) = \sup_{\pi} v_H^\pi(s)$ and define 
%$T: \R^{\nS} \to \R^{\nS}$ by
%$(T v)(s) = \max_a r_a(s) + \ip{P_a(s),v}$.
%For a memoryless policy, we also let $T_\pi v = r_\pi + P_\pi v$ (using vector notation).
%Greediness is defined as usual: $\pi$ is greedy w.r.t. $v\in \R^{\nS}$, if $T_\pi v = T v$.
%
%Since the criterion only involves $H$ steps, it suffices to restrict attention to policies that consists of $H$ steps. Thus, in this setting, a generic policy takes the form $\pi = (\pi_0,\pi_1,\dots,\pi_{H-1})$ where $\pi_i$ maps histories of form $(S_0,A_0,\dots,S_{i-1},A_{i-1},S_i)$ to distributions over the actions.
%By a slight abuse of notation, for $\pi_0,\pi_1,\dots,\pi_{H-1}$ memoryless policies, we let $\pi = (\pi_0,\pi_1,\dots,\pi_{H-1})$ stand for the policy that uses $\pi_0$ on state $S_0$, uses $\pi_1$ on state $S_1$, etc.
%
%\begin{question}[The Fundamental Theorem for Undiscounted Fixed-Horizon MDPs]
%Fix $H\ge 1$.
%Let $v_0^*$ defined to be the identically zero function: $v_0^* = \boldsymbol{0}$.
%Show that the following version of the fundamental theorem holds:
%\begin{enumerate}
%\item Any policy $\pi = (\pi_0,\dots,\pi_{H-1})$ such that
%$\pi_i$ is greedy with respect to $v_{H-1-i}^*$  is optimal: $v^\pi_H = v^*_H$;
%\item For $1\le h \le H$, $v^*_h = Tv^*_{h-1}$.
%\end{enumerate}
%\tpoints{10}
%\end{question}
%\begin{solution*}
%Assume first that all the rewards are nonnegative.
%Define a new MDP $\tilde M = (\tilde \cS,\cA,\tilde P,\tilde r)$ as follows:
%$\tilde \cS = \{\term\} \cup \cS \times \{0,1,\dots,H-1\}$ and 
%for $a\in \cA$, $s,s'\in \cS$, $0\le i \le H-2$
%$\tilde P_a( (s,i), (s',i+1) )=P_a(s,s')$ 
%and $\tilde P_a( (s,i), v )=0$ otherwise, while $\term$ is an absorbing state:
%$P_a( \term,\term )=1$.
%Furthermore, for $s\in \cS$, $P_a( (s,H-1), \term )=1$ (last stage states exit to the absorbing state).
%Finally, let the rewards be defined by $\tilde r_a( (s,i)) = r_a(s)$ and $r_a(\term)=0$.
%It is easy to see that \cref{ass:app} holds for $M'$. 
%Hence, the fundamental theorem holds for the new MDP.
%
%Let $\tilde v^*$ be the optimal value function of $\tilde M$ and $\tilde T$ be the underlying Bellman optimality operator.
%We claim that for $s\in \cS$, $0\le i \le H-1$,
%\begin{align*}
%\tilde v^*( (s,i) ) = v^*_{H-i}(s)\,.
%\end{align*}
%
%
%Now, if the rewards were not all nonnegative, 
%we can shift them by a sufficiently large constant so that the new rewards are nonnegative. This shifts all the value functions by a constant, including the optimal value functions. For the ``shifted'' MDP, the conclusion holds. But then it also holds for the nonshifted MDP.
%
%\qed\par\smallskip\hrule
%\end{solution*}

\section*{Approximate Policy Iteration}

\begin{question}
In the context of the analysis of approximate policy iteration analysis it was suggested that the following identity holds:
\begin{align*}
P_{\pi'} - P_{\pi^*} + \gamma P_{\pi'}(I-\gamma P_{\pi'})^{-1} (P_{\pi'}-P_{\pi} )
=
P_{\pi'} (I-\gamma P_{\pi'})^{-1} (I-\gamma P_{\pi}) - P_{\pi^*}\,.
\end{align*}
Show that this identity holds, actually, regardless the choice of the memoryless policies 
$\pi$, $\pi'$ and $\pi^*$.
\tpoints{10}
\end{question}
\begin{solution*}
For an arbitrary memoryless policy $\pi$ introduce the notation $A_{\pi} = (I-\gamma P_{\pi})$.
We have
\begin{align*}
\MoveEqLeft
P_{\pi'} - P_{\pi^*} + \gamma P_{\pi'}(I-\gamma P_{\pi'})^{-1} (P_{\pi'}-P_{\pi} ) \\
& =
P_{\pi'} - P_{\pi^*} + \gamma P_{\pi'} A_{\pi'} (P_{\pi'}-P_{\pi} ) \\
& =
P_{\pi'} - P_{\pi^*} + \gamma P_{\pi'} A_{\pi'} (P_{\pi'}-P_{\pi} ) A_{\pi} A_{\pi}^{-1} \\
& =
P_{\pi'} - P_{\pi^*} + P_{\pi'} (A_{\pi'}-A_\pi) A_{\pi}^{-1} \\
& =
P_{\pi'} - P_{\pi^*} + P_{\pi'} A_{\pi'}A_{\pi}^{-1}  - P_{\pi'} A_\pi A_{\pi}^{-1} \\
& =
P_{\pi'} - P_{\pi^*} + P_{\pi'} A_{\pi'}A_{\pi}^{-1}  - P_{\pi'}  \\
& =
P_{\pi'} A_{\pi'}A_{\pi}^{-1}  - P_{\pi^*} \,.
\end{align*}
Here, the third equality comes from that
\begin{align*}
A_\pi^{-1} - A_{\pi'}^{-1} = (I-\gamma P_\pi) - (I-\gamma P_{\pi'}) = \gamma (P_{\pi'}-P_{\pi})\,,
\end{align*}
and thus, multiplying from the right with $A_\pi$ and multiplying from the left with $A_{\pi'}$ we get
\begin{align*}
A_{\pi'}-A_\pi = \gamma A_{\pi'} (P_{\pi'}-P_{\pi} ) A_{\pi} \,.
\end{align*}

An alternate solution starts with noting that
for any memoryless policy $\pi$,
\begin{align*}
\gamma P_{\pi} A_{\pi} = \sum_{i\ge 1} (\gamma P_{\pi})^i = A_\pi - I\,.
\end{align*}
Hence,
\begin{align*}
P_{\pi'} + \gamma P_{\pi'} A_{\pi'} (P_{\pi'} - P_\pi) 
&= 
P_{\pi'} + (A_{\pi'}-I) (P_{\pi'} - P_\pi) \\
&= 
P_{\pi'} + A_{\pi'}(P_{\pi'} - P_\pi) - (P_{\pi'} - P_\pi) \\
&= 
A_{\pi'}(P_{\pi'} - P_\pi) + P_\pi\,.
\end{align*}
Subtracting $P_{\pi^*}$ from both sides gives the result.
\qed\par\smallskip\hrule
\end{solution*}


\begin{question}
Prove the following.
Assume that the rewards lie in the $[0,1]$ interval.
Let $(\pi_k)_{k\ge 0}$ be a sequence of memoryless policies and $(q_k)_{k\ge 0}$ be a sequence of functions over the set of state-action pairs such that for $k\ge 1$,
$\pi_k$ is greedy with respect to $q_{k-1}$. Further,
let $\varepsilon_k = \max_{0\le i \le k} \| q^{\pi_i} - q_i\|_\infty$.
Then, for any $k\ge 1$,
\begin{align*}
\norm{ q^* - q^{\pi_k} }_\infty 
& \le 
\frac{\gamma^k}{1-\gamma}  + 
\frac{2\gamma}{(1-\gamma)^2} \varepsilon_{k-1}\,,
\end{align*}
and policy $\pi_{k+1}$ is $\delta$-optimal where
\begin{align*}
\delta 
\le 
\frac{2}{1-\gamma} \left( 
\frac{\gamma^k}{1-\gamma}  + \frac{2}{(1-\gamma)^2} \varepsilon_k  \right)\,.
\end{align*}
How does this result compare to the Approximate Policy Iteration Corollary from Lecture 8 notes? 

\noindent \textbf{Hint:} You can use the following geometric progress lemma for action-value functions without proof. 
\begin{align*}
\norm{ q^* - q^{\pi_k} }_\infty 
\le 
\gamma \norm{ q^* - q^{\pi_{k-1}} }_\infty  + \frac{2\gamma}{1-\gamma} \norm{ q^{\pi_{k-1}}-q_{k-1}}_\infty\,.
\end{align*}


\tpoints{15}
\end{question}

\begin{solution*}
From the geometric progress lemma we have 
\begin{align*}
\norm{ q^* - q^{\pi_k} }_\infty 
\le 
\gamma \norm{ q^* - q^{\pi_{k-1}} }_\infty  + \frac{2\gamma}{1-\gamma} \norm{ q^{\pi_{k-1}}-q_{k-1}}_\infty\,.
\end{align*}
Iterating this gives
\begin{align*}
\norm{ q^* - q^{\pi_k} }_\infty 
& \le 
\gamma^k \norm{ q^* - q^{\pi_{0}} }_\infty  + 
\frac{2\gamma}{(1-\gamma)^2} \max_{0 \le i \le k-1} \norm{ q^{\pi_{i}}-q_{i}}_\infty \\
& \le 
\frac{\gamma^k}{1-\gamma}  + 
\frac{2\gamma}{(1-\gamma)^2} \varepsilon_{k-1}\,,
\end{align*}
where in the last inequality we used the definition of $\varepsilon_{k-1}$ 
and that both $q^*$ and $q^{\pi_0}$ take values in $[0,1/(1-\gamma)]$, hence 
$\norm{ q^* - q^{\pi_{0}} }_\infty \le 1/(1-\gamma)$.


To get the second result note that policy $\pi_{k+1}$ is greedy with respect to $q_k$. Hence, we first bound the distance between $q_k$ and $q^*$:
\begin{align*}
\norm{ q^* - q_{k}}_\infty
& \le 
\norm{ q^* - q^{\pi_k} }_\infty
+
\norm{ q^{\pi_k} - q_k }_\infty \\
& \le 
\frac{\gamma^k}{1-\gamma}  + 
\frac{2\gamma}{(1-\gamma)^2} \varepsilon_{k-1}
+
\varepsilon_k \\
& \le 
\frac{\gamma^k}{1-\gamma}  + \frac{2}{(1-\gamma)^2} \varepsilon_k \,,
\end{align*}
where the last inequality used that 
$\varepsilon_k \ge \varepsilon_{k-1}$ and
$\frac{2\gamma}{(1-\gamma)^2}+1 = \frac{2\gamma+1-2\gamma+\gamma^2}{(1-\gamma)^2} \le \frac{2}{1-\gamma}$.

Now, by the ``policy error bound I.'' 
from Lecture 6, $\pi_{k+1}$ is $\delta$-optimal with
\begin{align*}
\delta 
\le \frac{2\norm{q_k-q^*}_\infty}{1-\gamma} 
\le 
\frac{2}{1-\gamma} \left( 
\frac{\gamma^k}{1-\gamma}  + \frac{2}{(1-\gamma)^2} \varepsilon_k  \right)\,.
\end{align*}
\qed\par\smallskip\hrule
\end{solution*}

%\begin{question}
%semi-Markov MDP? \tpoints{10}
%\end{question}
%\begin{solution*}We follow the hint.\end{solution*}
%
%
%\begin{question}
%Something about LPs?
%\end{question}
%\begin{solution*}We follow the hint.\end{solution*}
%
%Nonnegative dynamic programming/negative dynamic programming
%
%Deterministic MDPs
%
%
%Shortest path problems; graph algorithms and relationship to our DP algorithms; Dijkstra's algorithm, A$^*$, .. all pairs, single source, all source SP
%
%Stochastic shortest path and weighted max-norms
%
%Weighted max-norms and infinite MDPs
%
%Weighted L2 norms and policy evaluation
%
%Not all feature-maps are born equal.
%E.g., for state-aggregation, the blow-up factor is $1$.
%Prove this.
%Or when you have a few core states.
%


\bigskip
\bigskip

\noindent
\textbf{
\noindent
Total for all questions: \arabic{DocPoints}}.
Of this, $23$ are bonus marks (i.e., $110$ marks worth $100\%$ on this problem set).

\end{document}




