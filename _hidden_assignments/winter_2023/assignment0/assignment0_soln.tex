\documentclass{article}
\newcommand{\hwnumber}{1}

\usepackage{fullpage,amsthm,amsmath,amssymb}
\usepackage{algorithm,algorithmic}
\usepackage{bbm,bm}
\usepackage{enumerate}
\usepackage{xspace}
\usepackage[textsize=tiny,
disable
]{todonotes}
\newcommand{\todot}[1]{\todo[color=blue!20!white]{T: #1}}
\newcommand{\todoc}[1]{\todo[color=orange!20!white]{Cs: #1}}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=black]{hyperref}


\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator*{\Exp}{\mathbf{E}}
\DeclareMathOperator*{\1}{\mathbbm{1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\E}{\mathbb E}
\newcommand{\V}{\mathbb V}
\renewcommand{\P}[1]{P\left\{ #1 \right\}}
\newcommand{\Prob}[1]{\mathbb{P}( #1 )}
\newcommand{\Probg}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\real}{\mathbb{R}}
\newcommand{\RR}{\mathbb{R}}
\renewcommand{\b}[1]{\mathbf{#1}}
\newcommand{\EE}[1]{\E[#1]}
\newcommand{\argmax}{\arg\max}

\usepackage{xifthen}

% total number of points that can be collected
\newcounter{DocPoints} % counter reset to zero upon creation

% counting points per question (not user facing)
\newcounter{QuestionPoints} % counter reset to zero upon creation

% Points for a subquestion of a question; 
% Adds the points to the total for the question and the document.
\newcommand{\points}[1]{%
	\par\mbox{}\par\noindent\hfill {\bf #1 points}%
	\addtocounter{DocPoints}{#1}
	\addtocounter{QuestionPoints}{#1}
}
% Points for a question; call with no params if the question
% had subquestions. In this case it prints the total for the question (see \points).
% Otherwise call with the points that the question is worth.
% In this case, the total is added to the document total.
% It is a semantic error to call this with a non-empty parameter
% when a question had subquestions with individual scores.
\newcommand{\tpoints}[1]{        %
	\ifthenelse{\isempty{#1}}%
	{%
	}%
	{%
		\addtocounter{DocPoints}{#1}
		\addtocounter{QuestionPoints}{#1}
	}													 %
	\par\mbox{}\par\noindent\hfill {Total: \bf \arabic{QuestionPoints}\xspace points}\par\mbox{}\par\hrule\hrule
	\setcounter{QuestionPoints}{0}
}
\newcommand{\tpoint}[1]{
	\tpoints{#1}
}

\newtheorem*{solution*}{Solution}

\newcommand{\hint}{\noindent \textbf{Hint}:\xspace}

\usepackage{hyperref}

\begin{document}

\begin{center}
{\Large \textbf{CMPUT 605: Theoretical Foundations of Reinforcement Learning, Winter 2023\\ Homework \#0}}
\end{center}

\textbf{Instructions:}
You need to submit a single PDF file, named {\tt p0\hwnumber\_<name>.pdf} where {\tt <name>} is your name.
The PDF file should include your typed up solutions (we strongly encourage to use pdf\LaTeX). 
Write your name in the title of your PDF file.
We provide a \LaTeX template that you are encouraged to use.
To submit your PDF file you should send the PDF file via private message to Vlad Tkachuk on Slack before the deadline.

\textbf{Collaboration and sources}
Work on your own. You can consult the problems with your classmates, use books
or web, papers, etc.
Also, the write-up must be your own and you must acknowledge all the
sources (names of people you worked with, books, webpages etc., including class notes.) 
Failure to do so will be considered cheating.  
Identical or similar write-ups will be considered cheating as well.
Students are expected to understand and explain all the steps of their proofs.

\textbf{Scheduling}
Start early: It takes time to solve the problems, as well as to write down the solutions. Most problems should have a short solution (and you can refer to results we have learned about to shorten your solution). Don't repeat calculations that we did in the class unnecessarily.

\vspace{0.3cm}

\textbf{Deadline:} January 15 at 11:55 pm

\section{In search of ...}
\newcommand{\epssub}{\delta}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\sA}{\mathcal{A}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}

Consider the following simple learning problem: 
Fix $\delta>0$ and an integer $k>0$.
Here, $k$ will be the number of actions available. Each action $i\in [k]:=\{1,\dots,k\}$ is assigned a value, say, $\mu_i$, which is a real number. The learner does not know these values.
The problem of the learner is to find an action that is $\delta$ close to the best action out of $k$ actions (higher values are better).
The learner can try any action in any order and can even randomize when choosing the next action.
When choosing an action, the learner receives in response the value of the action (without noise!).
When choosing an action, the learner can take into account all the past observations and choices: It has no limit of what it can remember, or on the precision of its memory.
At one point, the learner needs to stop and return an action.

A \emph{problem instance} 
that the learner interacts with is fully characterized by $\mu = (\mu_1,\dots,\mu_k)\in \R^k$, the $k$ values assigned to the actions.
The learner is called \emph{$\delta$-sound} for a set of instances $\cH\subset \R^k$ if no matter the problem instance taken from $\cH$, when the learner interacts with the aforementioned way with the chosen problem instance, the learner always stops and returns an action that is strictly less than $\delta$ away from the optimal action on that instance. If the instance was $\mu\in \R^k$, the learner chose $A\in [k]$, with probability one, it has to hold that $\mu_A> \max_{i\in [k]}\mu_i-\delta$ (note the strict inequality).
For a learner $\cA$, let $q(\cA,\mu)$ be the expected value of the number of rounds that the learner spends with interacting with problem instance $\mu$ (we need the expected value, because learners may randomize).
By slightly abusing notation, let $q(\cA,\cH) = \max_{\mu\in \cH} q(\cA,\mu)$ be the expected number of rounds that the learner $\cA$ will spend on the instance in $\cH$ which makes it the ``slowest''.

Let $\cS(\cH,\delta)$ be the set of $\delta$-sound learners for $\cH$.
Further, let $\cB = \{e_1,\dots,e_k\}\subset \R^k$ 
where $e_i$ is the $i$th standard basis vector: $e_{ij}=0$ if $j\ne i$ and $e_{ii}=1$.


\paragraph{Question 1} 
Describe a \emph{deterministic} learner $\cA\in \cS(\cB,1)$ such that the learner stops on any instance in $\cB$ after at most \textcolor{red}{$k-1$} queries: $q(\cA,\cB)\le k-1$. 
\tpoints{5}

\begin{solution*}
Loop through the actions, starting from action $1$ and finishing with action $k-1$:
In the $i$th step of this loop, ask for the feedback (response) for action $i$.
If the feedback is $1$, return $i$. If the loop finishes, return $k$.
\qed\par\bigskip\par\hrule
\end{solution*}

\paragraph{Question 2} 
Formally prove that for the learner $\cA$ you described $\cA\in \cS(\cB,1)$ holds.
\tpoints{5}

\begin{solution*}
It suffices to prove that for any $i\in [k]$, when $\cA$ interacts with the environment $e_i$, it will return index $i$ because this implies that for the returned action $A\in [k]$, $\mu_A=1>\max_{j\in [k]} e_{ij} - 1 = 1-1=0$.
Fix $i\in [k]$, arbitrarily.
There are two cases: If $i\le k-1$, the learner in the main loop will get to ask for the feedback for action $i$.
This is because for all earlier queries, it receives the value of $0$ ($e_{ij}=0$ for $j<i$).
When asking for the feedback for action $i$, it receives the value of $1$ ($e_{ii}=1$)
and thus quits the loop and returns $i$, which is the correct output.
The second case is if $i=k$. In this case, by the previous argument, the loop finishes without quitting early.
The learner then returns $i=k$, which is again the correct output.
Since no matter the value of $i$, the learner's output is correct, $\cA \in \cS(\cB,1)$ holds.
\qed\par\bigskip\par\hrule
\end{solution*}

\paragraph{Question 3} 
Formally prove that for the learner $\cA$ you described $q(\cA,\cB)\le k-1$ holds.
\tpoints{5}

\begin{solution*}
No matter the instance that the learner interacts with, 
the learner's loop lasts for at most $k-1$ steps. In each step of the loop one query is submitted. Therefore, the number of queries the learner submits is at most $k-1$.
\qed\par\bigskip\par\hrule
\end{solution*}

\paragraph{Question 4} 
Show that any deterministic learner $\cA\in \cS(\cB,1)$ needs at least $k-1$ queries 
in the worst case for instances in $\cB$.
\medskip
\tpoints{10}

\begin{solution*}
Take a deterministic algorithm $\cA$.
Let the queries issued by $\cA$ on the all-zero input be $s_1,s_2,\dots\in [k]$: 
This is a finite sequence if the algorithm stops querying (e.g., returns), otherwise it is an infinite sequence.
If the length of this sequence is at least $k-1$ then on $e_i$ with $i\in [k]\setminus \{s_1,\dots,s_{k-1}\}$ (which exist because $[k]\setminus \{s_1,\dots,s_{k-1}\}$ is non-empty), $\cA$ will issue at least $k-1$ queries because on this instance, the feedbacks corresponding to the first $k-1$ queries of $\cA$ are zero, so the first $k-1$ queries of $\cA$ will be $s_1,\dots,s_{k-1}$.
Thus, in this case there is nothing to be proven.

Now assume that the length $\ell$ of this sequence is strictly smaller than $k-1$. We show that in this case $A\in \cS(\cB,1)$ cannot hold and thus from $A\in \cS(\cB,1)$ it follows that we cannot be in this case.
Thus, the proof will be finished once we show that $A\not\in \cS(\cB,1)$ proved that $\ell\le k-2$.

Take $i,j\in [k]\setminus \{s_1,s_2,\dots,s_{\ell}\}$ such that $i\ne j$.
Such numbers exist because $|\{s_1,s_2,\dots,s_{\ell}\}|\le \ell \le k-2$ and thus
$[k]\setminus \{s_1,s_2,\dots,s_\ell\}$ has at least $k-(k-2)=2$ elements.
Since $\cA$ never queries either action $i$ or action $j$, $\cA$ will receive the same feedbacks on both $e_i$ and $e_j$. 
Now, $\cA$, being deterministic, will return the same answer $A\in [k]$ on both $e_i$ and $e_j$.
However, either $A\ne i$ or $A\ne j$, hence $\cA$ returns the index of an action with payoff zero on at least one of $e_i$ and $e_j$. Hence, $\cA\not\in \cS(\cB,1)$.
\qed\par\bigskip\par\hrule
\end{solution*}


\paragraph{Question 5} 
Describe a learner $\cA\in \cS(\cB,1)$ (deterministic or not) such that the learner stops on any instance in $\cB$ after at most \textcolor{red}{$(k+1)/2 - 1/k$} queries on expectation: \textcolor{red}{$q(\cA,\cB)\le (k+1)/2 - 1/k$}. 
Formally prove that the learner is indeed sound and it indeed stops after at most \textcolor{red}{$(k+1)/2 -1/k$} queries on expectation, on every problem instance in $\cB$.
\tpoints{15}

\begin{solution*}
Consider an algorithm that queries each action in a random order, stopping as soon as it receives a nonzero feedback, at which point the algorithm can return the index of the action just queried.
Clearly, this algorithm is sound. 
For calculating the expected number of queries let
$S\in \mathrm{Perm}([k])$ be a (uniform) random permutation of $[k]$: This is the order in which the algorithm queries the actions (i.e., $S(1)$ is queried first, followed by $S(2)$, etc.).

Fix $i\in [k]$ and let $T = \min\{ t\ge 1\,:\, S(t) = i \}$ be the number of queries when $\mu=e_i$. 
Note that $T=S^{-1}(i)$ and as such that $\Prob{T=t}=\Prob{S(t)=i}=1/k$ for any $t\in [k]$, i.e., $T$ is uniformly distributed over $\{1,\dots,k\}$.
Now consider the slightly improved algorithm, 
which, using the principle of elimination, queries at most $k-1$
items. The runtime of this algorithms on instance $\mu=e_i$ is $\tilde T = \min(T,k-1)$ and we have
$\E[\tilde T] = \sum_{t=1}^{k-1} t\, \Prob{ \tilde T = t } = \sum_{t=1}^{k-1} t \,\Prob{T=t} + (k-1) \Prob{T=k}$. Plugging in $\Prob{T=t}=1/k$, we have
\begin{align}
\E[\tilde T] = \frac{1}{k} \left(-1+ \underbrace{\sum_{t=1}^{k} t}_{k(k+1)/2} \right) 
=  \frac{k+1}{2}-\frac{1}{k}\,. 
\label{eq:calc}
\end{align}
\qed\par\bigskip\par\hrule
\end{solution*}

\paragraph{Question 6} 
Show that any learner $\cA\in \cS(\cB,1)$ needs at least \textcolor{red}{$(k+1)/2-1/k$} queries 
on expectation in the worst case for instances in $\cB$.
That is, prove that for any \textcolor{red}{$\cA\in \cS(\cB,1)$, 
$q(\cA,\cB)\ge (k+1)/2 -1/k$}. 
\medskip

\noindent \textbf{Hint:}
Yao's lemma states
``that the expected cost of a randomized algorithm on the worst-case input is no better than the expected cost for a worst-case probability distribution on the inputs of the deterministic algorithm that performs best against that distribution.'' (source: \href{shorturl.at/cjrU7}{wikipedia}).
Use this lemma.

\tpoints{15}

\begin{solution*}
Following the hint, by Yao's lemma, it suffices to show that $q^*\ge (k+1)/2-1/k$ where
\begin{align*}
q^*=\frac1k \inf_{\cA\in \cS^*(\cB,1)} \sum_{i=1}^k q(\cA,e_i) \,,
\end{align*}
where $\cS^*(\cB,1)$ is $\cS(\cB,1)$ restricted to deterministic algorithms (we chose the uniform distribution over the input instances $e_1,\dots,e_k$).

Take $\cA\in\cS^*(\cB,1)$. 
It suffices to show that 
\begin{align}\label{eq:lb}
\frac1k \sum_{i=1}^k q(\cA,e_i)  \ge \frac{k+1}{2}-\frac{1}{k}\,.
\end{align}
Assume that $\cA$ sometimes repeats some queries.
Define $\cA'$ as follows: in each round of interaction $\cA'$ first calls $\cA$, checks whether $\cA$ issued a query that was issued before and if so feed $\cA$ with the answer received earlier, otherwise issues the query to the environment and return the feedback received back to $\cA$. This algorithm $\cA'$  works identically on all inputs as $\cA$ and never issue a query twice. In particular, for all $i\in [k]$, $q(\cA,e_i)\ge q(\cA',e_i)$ and so \eqref{eq:lb} will follow if we show it with $\cA:=\cA'$.
Thus, in what follows, without loss of generality, we assume that $\cA$ does not repeat any queries.
Similarly, we can assume without loss of generality that $\cA$ stops as soon as it receives a nonzero response.

Like in the solution to Question 4,
let the queries issued by $\cA$ on the all-zero input be $s_1,s_2,\dots\in [k]$: 
By the answer to Question 4 the length of this sequence is at least $k-1$.
By our assumption the sequence $(s_1,\dots,s_{k-1})$ does not have repeated elements.
Define $s_k\in [k]$ to be the unique action index which is left out from $\{s_1,\dots,s_{k-1}\}$.
Since  $(s_1,s_2,\dots,s_k)$ does not have repeated elements, it is a permutation of $[k]$.
By assumption, $\cA$ must stop as soon as it receives a nonzero answer
and because $\cA\in \cS(\cB,1)$, it cannot stop earlier.
Let $t(i) = \min\{ t\ge 1\,:\, s_t=i \}$.
It follows that the number of queries issued by $\cA$ when run on $e_i$
 is $\min(k-1,t(i))$. 
Since $(s_i)_i$ is a permutation of $[k]$, so is $(t(i))_{i\in [k]}$.
Let $I\sim P$, i.e., $I$ is uniformly distributed on $\{1,\dots,k\}$.
We have $\Prob{T= t(I)}=1/k$ and thus, with the same calculation as before,
$\EE{\min(k-1,T)} =  (k+1)/2-1/k$ (cf. Eq.~\eqref{eq:calc}).
\qed\par\bigskip\par\hrule
\end{solution*}

\paragraph{Question 7 (for fun)} 
It appears that as far as the expected number of rounds is considered,
randomized learners can do better than deterministic learners.
Is this \emph{real}?
Would \emph{you} implement the randomized learner? Why or why not?
\medskip

\tpoints{0}

\begin{solution*}
The answer depends on the following factors:
\begin{itemize}
\item Is $k$ large?
\item How many times will we run this algorithm?
\item Do we care about the cost of generating a uniform random permutation?
\item Do we care about the simplicity of the code?
\end{itemize}
In particular, if $k$ is large, I expect the runtime on any single instance $e_i$ to be relatively close to the expected runtime (relative to $k$) on any single run, with a ``good chance''. Thus, with high probability, the prediction is that the randomized algorithm will save over the non-randomized one, regardless of the input. Hence, it may be worth the effort to implement it.

On the second item, if the algorithm will be run on many inputs, then over the many inputs, the average runtime is expected to even more ``tightly concentrated'' around the expected runtime. This is a good case for the randomizing algorithm.

If we care about the cost of generating the queries, we need to pay some cost for generating the queries uniformly at random. The Fisher-Yates algorithm has a linear cost, thus it adds negligibly to the total runtime, but sometimes even this little cost could matter. Probably not too often. But if it matters, randomization will be out.

By making a code more complex, we may introduce subtle errors in the code. Thus, if reliability is important, a simpler solution may be preferred. If the fancy solution is to be supported, one needs to carefully verify the correctness of the code. There are also subtleties related to whether a specific algorithm plays well with a random number generators. In general, one should just use a library function if available (almost all modern libraries will have fast algorithms for random shuffling, which is what is needed here). 
\qed\par\bigskip\par\hrule
\end{solution*}

\section{Basic probability questions}

Assume that you are given a randomized algorithm $\cB$ that returns the \emph{unique} correct answer on any problem it is fed with (e.g., computing a shortest path) with probability at least $2/3$.
Fix $\delta>0$. 

\paragraph{Question 8}
Design an algorithm that returns the correct answer with probability at least $1-\delta$.
The algorithm may use $\cB$. 
Describe how your algorithm works (pseudocode preferred).
\tpoints{5}

\begin{solution*}
See solution for Question 9
\qed\par\bigskip\par\hrule
\end{solution*}

\paragraph{Question 9}
Bound the expected runtime of your algorithm as a function of the expected runtime of $\cB$.
In particular, if the expected runtime of $\cB$ on input $x$ is $r(\cB,x)$, then show that the expected runtime of your algorithm on input $x$ is at most $\lceil 72 \ln(1/\delta) \rceil \, r(\cB,x)$.
If needed modify your algorithm (in which case modify your answer to the previous question).

\noindent \textbf{Hint}: 
Use the ``Chernoff lower tail bound'' for independent Bernoulli variables.
This states the following:
Let $S$ be the sum of $n$ independent Bernoulli random variables. 
Let $\mu = \EE{S}$ be the expected value of $S$.
Then, for any $0\le \delta \le 1$, $\Prob{ S\le (1-\delta)\mu } \le \exp( - \mu \delta^2/3)$.
\tpoints{5}

\begin{solution*}
First let us state our algorithm then bound its runtime.
The mode of a list is the item on it with the highest repeat count.

\medskip
\noindent Input: $n \in \mathbb{N}$, subroutine $\mathcal{B}$, $x$
\begin{enumerate}
    \item Initialize list $V = ()$
    \item for $i = 1,2,...,n$ do
    \item $\quad$ $V = V$.append($\mathcal{B}(x)$)
    \item end loop
    \item return mode of $V$.
\end{enumerate}
\newcommand{\one}[1]{\mathbb{I}(#1)}
Thus the question is how to choose $n$ such that our algorithm outputs the correct answer with probability at least $1-\delta$. 

Let $Y_i$ be the value returned by $\mathcal{B}(x)$, on its $i$th call.
Let $y$ be the unique correct answer.
Let $B_i = \one{Y_i=y}$ be the indicator that the $i$th answer is correct and let
$S = \sum_{i=1}^n B_i$. Thus, $S$ counts the number of successes of the calls.

Now, the algorithm is guaranteed to be correct when $S\ge n/2$. This is because the second highest repeat count on $V$ in this case must be strictly less than $n/2$, hence $y$ "wins" (i.e., $Y=y$).
Note that $(B_i)_i$ is an i.i.d. sequence: This follows from the (somewhat sneaky, implicit, but sensible) assumption that $\mathcal{B}(x)$ randomizes independently between the calls to it and that it randomizes the same way.

Hence, $S$ is the sum of $n$ independent, identically distributed Bernoulli variables. 
We are preparing to use the hint to bound the probability of failure from above.
We have $\{ Y \ne y \} = \{ Y = y \}^c$ and since $\{S\ge n/2\} \subset \{Y=y\}$ by the previous argument,
$\{Y=y\}^c \subset \{S<n/2\}$. Hence, $\Prob{Y\ne y} \le \Prob{ S<n/2 }$.
Let $\mu = \EE{S}$ as in the hint. Since $\Prob{Y_i=y}\ge 2/3$ by our assumption, $\mu\ge 2/3 n$.
Thus,
\begin{align*}
\Probg{Y\ne y} 
& \le \Probg{ S<n/2 } 
= \Probg{ S<(1-1/4) \frac{2n}{3} } 
\le \Probg{ S< (1-1/4) \mu }
\le \exp\left(-\mu \frac{2(1/4)^2}{3} \right) \\
& \le \exp\left( - \frac{2}{9} \left(\frac14\right)^2 n \right)\,,
\end{align*}
where in the last inequality we used that $\exp(-x)$ is monotone decreasing and that $\mu\ge 2/3 n$.
Thus, it suffices to choose $n$ large enough so that 
$\exp\left( - \frac{2}{9} \left(\frac14\right)^2 n \right)\le \delta$ holds.

By taking the $\log$ of the last two equation on the right we get that we should run our algorithm at most $n = \lceil 72 \ln(1/\delta) \rceil $. Multiplying by the runtime if $\mathcal{B}$ on input $x$, we can bound the runtime of the algorithm by $\lceil 72 \ln(1/\delta) \rceil \, r(\cB,x)$.
\qed\par\bigskip\par\hrule
\end{solution*}

\paragraph{Question 10}
For the same setting as in Question 8, now consider the case when $\cB$ itself is correct with probability at least $1-\delta$.
Consider the case when $\cB$ is run on $s$ different inputs.
Show that 
for any $s\le 1/\delta$,
the probability that $\cB$ is correct on \emph{all these inputs} is at least $1-s\delta$.
\tpoints{5}

\begin{solution*}
Call a call to $\cB$ a failure when the call fails to produce the correct output.
Call a run a failure when $\cB$ fails on any of its calls.
Let $F_i$ be the event that $\cB$ fails on the $i$th call. Let $F$ be the event that a run fails.
Then the run fails if and only if any of the events $F_i$ holds.
That is, $F = \cup_i F_i$ (if $\omega\in F$, the run fails on $\omega$ then it must that $\omega \in F_i$ for at least one index $i$ and if $\omega\in \cup_i F_i$ then at least for one index $i$, $\omega\in F_i$, which means that $\omega\in F$).
Hence,
\begin{align*}
\Prob{F} = \Prob{\cup_i F_i} \le \sum_{i=1}^s \Prob{F_i} \le s \delta\,.
\end{align*}
Further, $\cB$ is correct on all of its calls, if and only if it does not fail. Hence,
\begin{align*}
\Prob{ \cB \text{ is correct on all of its calls}} = 1- \Prob{F}\ge 1-s\delta\,.
\end{align*}

\noindent \textbf{Note}: The argument presented here is very commonly used.
Oftentimes the situation is so that we can easily prove that an ``algorithm'' (calculation, etc.) works correctly unless \emph{some} failure event holds. If we collect these failure events into the list $\{F_i\}_i$, we can then argue that outside of $F = \cup_i F_i$ the algorithm works as expected. Then one is left with controlling the probability of  $F$ which is done by just noting that $\Prob{F}\le \sum_i \Prob{F_i}$. Since the algorithm may also succeed (by errors canceling, etc.) even on $F$, the event when the algorithm actually fails, $F^*$, could be a subset of $F$: $F^* \subset F$ may happen.
But this does not matter, $\Prob{F^*} \le \Prob{F} \le \sum_i \Prob{F_i}$ still holds, so to control failure, it suffices to control the probabilities of $F_i$, the individual failure events.
The bound $\Prob{F} \le \sum_i \Prob{F_i}$ is called the \textbf{union bound}.
\qed\par\bigskip\par\hrule
\end{solution*}


\section{Basic calculus and such}
\newcommand{\norm}[1]{\|#1\|}
\paragraph{Question 11}
For $x\in \R^d$ and $p\ge 1$ let $\norm{x}_{p} = (\sum_{i=1}^d |x_i|^p)^{1/p}$.
For $p=\infty$, let $\norm{x}_\infty = \max_i |x_i|$.
Show that for any $x$, $1\le p\le q\le \infty$, $\norm{x}_p \ge \norm{x}_q$.
\tpoints{5}

\begin{solution*}
% \paragraph{Solution} See the following \href{https://math.stackexchange.com/questions/218046/relations-between-p-norms}{thread}
See the following \href{https://math.stackexchange.com/questions/69125/inequality-between-ellp-norms}{thread}
\qed\par\bigskip\par\hrule
\end{solution*}


\paragraph{Question 12}
Let $1\le p \le \infty$. 
Show that for any $c\in \R$, $x,y\in \R^d$, the following hold:
\begin{enumerate}
\item $\norm{x}_p\ge 0$ and if $\norm{x}_p=0$ then $x=0$;
\points{5}
\item $\norm{c x}_p = |c| \, \norm{x}_p$;
\points{5}
\item $\norm{x+y}_p \le \norm{x}_p + \norm{y}_p$.
\points{5}
\end{enumerate}
(That is, $\norm{\cdot}_p$ is a norm on $\R^d$.)
\tpoints{}

\begin{solution*}
1)The definition immediately gives that $\norm{x}_p\ge 0$.
Assume now that $\norm{x}_p=0$. If $p=\infty$, this means $|x_1|=\dots = |x_d|=0$, which implies that $x=0$.
If $p<\infty$, we have $0=\norm{x}_p^p = \sum_i |x_i|^p \ge |x_j|^p$ for $j\in [d]$ arbitrary. 
Taking the $p$th root, we find that $x_j=0$. Since $j$ was arbitrary, $x=0$.
\\ 
2) 
\begin{equation}
    \lVert cx \rVert_p = \left(\sum_i |cx_i|^p\right)^{1/p} = (|c|^p)^{1/p}  \left(\sum_i |x_i|^p\right)^{1/p} = |c| \lVert x \rVert_p
\end{equation}
Note if $p = \infty$ then the $\max_i |cx_i| = |c| \max_i |x_i|$
\\
3) See \href{https://en.wikipedia.org/wiki/Minkowski_inequality}{Minkowski Inequality}
\qed\par\bigskip\par\hrule
\end{solution*}


\paragraph{Question 13}
Let $\norm{\cdot}$ be any norm on the vector-space of $d\times d$ matrices (that is, 
$\norm{\cdot}:\R^{d\times d} \to \R$ satisfies the properties of a norm when we treat $\R^{d\times d}$ as a $d^2$-dimensional vector space over the reals).
Further, assume that $\norm{\cdot}$ is submultiplicative: for any $A,B\in \R^{d\times d}$, $\norm{AB}\le \norm{A}\norm{B}$.
Let $I$ be the $d\times d$ identity matrix, $A\in \R^{d\times d}$ arbitrary.
Show that $I-A$ is nonsingular when $\sum_{k=0}^\infty \norm{A}^k<\infty$.

\noindent \textbf{Hint}: Perhaps the inverse of $I-A$ is equal to $\sum_{k\ge 0} A^k$? But is this infinite sum even well-defined?
\tpoints{15}

\begin{solution*}
\textbf{Solution 1:} 
Note that if $\sum_k ||A||^k$ is finite then  the \href{https://en.wikipedia.org/wiki/Spectral_radius}{spectral radius} $\rho(A) < 1$. Thus the eigenvalues of $(I - A)$, which are $\{1-\lambda \,:\, \lambda \text{ is an eigenvalue of } A \}$ have strictly positive real parts, and cannot be zero. Hence, the matrix is invertible. 

\textbf{Solution 2:} Let's just follow the hint.
Let $S_n = \sum_{k=0}^n A^k$.
We claim that $(S_n)_n$ is convergent.
That is, for some $S\in \R^{d\times d}$ for every $(i,j)\in [d]\times [d]$, $S_n(i,j) \to S(i,j)$, where we use $X(i,j)$ to denote the $(i,j)$th entry of a matrix $X$.

To show that $S_n$ is convergent it suffices to show that $S_n$ is convergent in (say) the Frobenius norm for if we find $S$ such that $\norm{S-S_n}_F\to 0$ as $n\to \infty$ then 
\begin{align*}
|S(i,j) - S_n(i,j)|^2 \le \norm{ S - S_n }^2_F \to 0 \text{as } n\to\infty
\end{align*}
and thus $(S_n(i,j))_n$ must also converge to $S$.


Now note that $\RR^{d\times d}$ inherits its completeness from the reals.
Thus, to show that $S_n$ converges, it suffices to show that for any $(i,j)$,
$\limsup_{n\to\infty} \sup_{m\ge n} |S_m(i,j)-S_n(i,j)|$ (i.e., that it is Cauchy).
For this, similarly to the above,
$|S_m(i,j) - S_n(i,j)|^2 \le \norm{ S_m - S_n }^2_F$
and hence
\begin{align*}
|S_m(i,j) - S_n(i,j)| \le \norm{ S_m - S_n }_F \le c \norm{S_m - S_n}\,,
\end{align*}
where in the last inequality $c>0$ is a $d$-dependent constant so that 
$\norm{X}_F\le c \norm{X}$ holds for all $X\in \RR^{d\times d}$.
This constant exist because in finite dimensional vector spaces all norms are equivalent.
Now, $S_m-S_n = \sum_{k=n+1}^m A^k$. Hence, $\norm{S_m-S_n} \le \sum_{k=n+1}^m \norm{A^k}$ by the triangle inequality.
Since $\norm{\cdot}$ is submulitplicative, $\norm{A^k}\le \norm{A}^k$. Hence, $\norm{S_m - S_n} \le \sum_{k=n+1}^m \norm{A}^k$. Since, by assumption $\sum_k \norm{A}^k$ converges, 
$\limsup_{n\to\infty} \sum_{m\ge n} \sum_{k=n+1}^m \norm{A}^k = 0$.
Putting things together, we see that 
\begin{align*}
\limsup_{n\to\infty} \sup_{m\ge n} |S_m(i,j) - S_n(i,j)| 
& \le \limsup_{n\to\infty}  \sup_{m\ge n} \norm{ S_m - S_n }_F \\
& \le c \limsup_{n\to\infty} \sup_{m\ge n}  \norm{S_m - S_n} = 0\,.
\end{align*}
Thus, $S_n$ converges. Let its limit be $S$.
We calculate
\begin{align*}
 S_n  (I-A) =  (I+A+\dots+A^n - (A+A^2 + \dots + A^{n+1})) = I-A^{n+1}\,.
\end{align*}
Taking the limit of $n\to \infty$ of both sides and noting that $\lim_{n\to\infty} (S_n(I-A)) = (\lim_{n\to\infty} S_n)(I-A) = S(I-A)$, we get
\begin{align*}
S (I-A)  = I\,.
\end{align*}
Hence, $I-A$, being square, is invertible and it's inverse is $S$.
\qed\par\bigskip\par\hrule
\end{solution*}


\paragraph{Question 14}
Let now $\norm{\cdot}$ be a norm on $\R^d$.
Call $s: \R^{d\times d} \to \R$ the ``norm induced by $\norm{\cdot}$'' when for any $A\in \R^{d\times d}$, $s(A) = \sup_{x\in \R^d, \norm{x}=1} \norm{ A x}$.
Show that $s$ is a submultiplicative norm on $\R^{d\times d}$, i.e., it satisfies the premise of the previous question. 
\tpoints{5}

\begin{solution*}
\newcommand{\setg}[1]{\left\{#1\right\}}
First note that $s(A) = \sup_{x\ne 0} \frac{\norm{Ax}}{\norm{x}}$.
This is because 
\begin{align*}
\setg{ \frac{\norm{Ax}}{\norm{x}} \,:\, x\ne 0 } = 
\setg{ \norm{Av} \,:\, v\in \R^d, \norm{v}\ne 1 } \,.
\end{align*}
Now, the statement is trivial if either $A=0$ or $B=0$ because in this case $s(AB)=0$.
Otherwise,
\begin{align*}
s(AB) 
& = \sup_{x\ne 0} \frac{ \norm{ AB x} }{\norm{x}} 
 = \sup_{x\ne 0,Bx \ne 0} \frac{ \norm{ AB x} }{\norm{Bx}}   \frac{\norm{Bx}}{\norm{x}}\\
& \le \sup_{Bx \ne 0} \sup_{y\ne 0} \frac{ \norm{ AB x} }{\norm{Bx}}   \frac{\norm{By}}{\norm{y}}\\
& =
 \left(\sup_{Bx \ne 0}  \frac{ \norm{ AB x} }{\norm{Bx}} \right)
 \left(\sup_{y\ne 0}  \frac{\norm{By}}{\norm{y}}\right)
 \\
& =
\left(\sup_{v \ne 0}  \frac{ \norm{ Av} }{\norm{v}} \right) 
\left(\sup_{y\ne 0}  \frac{\norm{By}}{\norm{y}}\right)\\
& = s(A) s(B)\,,
\end{align*}
where the second equality (where we add $Bx \ne 0$ to the condition on $x$ in the supremum) follows because 
\begin{align*}
\setg{ \frac{ \norm{ AB x} }{\norm{x}}\,:\, x \ne 0 }
=
\setg{ 0 } \cup 
\setg{ \frac{ \norm{ AB x} }{\norm{Bx}}   \frac{\norm{Bx}}{\norm{x}}\,:\, x \ne 0, Bx\ne 0 }\,.
\end{align*}
and thus the suprema of the set  on the left-hand side and the second set on the right-hand side agree must agree (the second set on the right-hand side is not empty because $B\ne 0$ and in particular it has nonnegative elements).
%For a more rigorous proof see Chapter 9 of Rudin's Principles of Mathematical Analysis. Specifically Definition 9.6 and Theorem 9.7
\qed\par\bigskip\par\hrule
\end{solution*}


\paragraph{Question 15}
Calculate a closed form expression for $s(A)$ with $A\in \R^{d\times d}$ and $s$ the norm induced on $\R^{d\times d}$ by the vector-space norm $\norm{\cdot}_\infty$.
\tpoints{5}

\begin{solution*}
We show that $s(A) = \max_{i \in [d]} \sum_{j=1}^d |A_{i,j}|$.

Let $x_i$ be defined by $x_{ij} = \sgn(a_{ij})$ (when $a_{ij}=0$ use either $+1$ or $-1$).
Note that $\norm{A x_k}_\infty = \max_i | \sum_j a_{ij} x_{kj} | \ge |\sum_j a_{kj}x_{kj} | = | \sum_j |a_{kj}|| = \sum_{j} |a_{kj}|$.
We thus have
\begin{align*}
s(A)
\ge \max_k \norm{A x_k}_\infty 
\ge \max_k \sum_j |a_{kj}|\,.
\end{align*}
Furthermore, 
$\norm{A x}_\infty
 = \max_k |\sum_j a_{kj} x_j | 
 \le \max_k \sum_j |a_{kj}|\, | x_j | 
 \le (\max_k \sum_j |a_{kj}| ) \norm{x}_\infty$. Hence,
\begin{align*}
s(A) \le \sup_{x:\norm{x}_\infty=1} \max_k  \sum_j |a_{kj}| 
\max_k  \sum_j |a_{kj}| \,.
\end{align*}
finishing the proof.
\qed\par\bigskip\par\hrule
\end{solution*}


\paragraph{Question 16}
Let $T:U\to V$ be a map from a normed vector space $U$ over the reals to another normed vector space $V$ over the reals. Possibly $U\ne V$, so the norms will also be different, but to reduce clutter, we just use $\norm{\cdot}$ to denote the norms on all the vector spaces as from the context it will always be clear which norm we are concerned with.
Fix $L\ge 0$. The map $T$ is called $L$-Lipschitz if $\norm{T(u)-T(u')}\le L \norm{u-u'}$. 
\begin{enumerate}
\item Why can we write $T(u)-T(u')$? What set does the result of this operation belong to? Why?
\points{2}
\item Let $T:U\to V$ be $L$-Lipschitz, $S:V \to W$ be $M$-Lipschitz ($W$ is also a normed vector space and $M\ge 0$).
Prove that $S \circ T$ is $LM$-Lipschitz. Here, $S\circ T$ is the composition of $T$ and $S$. Thus, $S\circ T: U \to W$ and in particular $(S\circ T)(u) = S(T(u))$.
\points{8}
\end{enumerate}
\tpoints{}

\begin{solution*}
1) By definition of a vector space since both $T(u)$ and $T(u')$ both belong to the vector space $V$ so too does their difference $T(u) - T(u')$ \\
2) For an arbitrary $u,u' \in U$,
\begin{align*}
\lVert (S \circ T)(u) - (S \circ T)(u') \rVert = \lVert S(T(u)) - S(T(u')) \rVert \leq M \lVert T(u) - T(u') \rVert \leq ML \lVert u - u' \rVert
\end{align*}
Thus $S \circ T$ is $LM$ or $ML$ Lipschitz.
\qed\par\bigskip\par\hrule
\end{solution*}


\bigskip
\bigskip

\noindent
\textbf{
Total for all questions: \arabic{DocPoints}}


\end{document}
