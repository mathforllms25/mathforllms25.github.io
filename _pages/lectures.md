---
layout: page
title: Lectures
permalink: /lectures/
---

### Lectures & Readings

The course is divided into two main parts. The first part consists of instructor-led lectures to establish foundational concepts. The second part is a student-led seminar where we will critically analyze key research papers.

-----

#### Part I: Foundations (Instructor-Led)

**Week 1: Introduction & A Language for Transformers**

  * *Topics:* Course overview; The Transformer Architecture; Modeling transformer computation with the RASP (Restricted Access Sequence Processing) language.
  * *Readings:*
      * "Attention Is All You Need" (Vaswani et al., 2017) [[Link](https://arxiv.org/abs/1706.03762)]
      * "Thinking Like Transformers" (Weiss et al., 2021) [[Link](https://arxiv.org/abs/2106.06981)]

**Week 2: The Limits of Computation**

  * *Topics:* Computational Universality of Transformers with scaffolding; The Transformer as a circuit and its relation to the complexity class TC0.
  * *Readings:*
      * "Memory Augmented Large Language Models are Computationally Universal" (Schuurmans, 2023) [[Link](https://arxiv.org/abs/2301.04589)]
      * "Autoregressive Large Language Models are Computationally Universal" (Schuurmans et al., 2024) [[Link](https://arxiv.org/abs/2410.03170)]
      * "Saturated Transformers are Constant-Depth Threshold Circuits" (Merrill et al., 2022) [[Link](https://aclanthology.org/2022.tacl-1.49/)]

**Week 3: Theories of Learnability**

  * *Topics:* Formal learnability of natural language (Gold's paradigm); Learnability in the context of modern generative models.
  * *Readings:*
      * "Learning theory and natural language" (Osherson, Stob, & Weinstein, 1984) [[Link](https://doi.org/10.1016/0010-0277(84)90040-4)]
      * "Language Generation in the Limit" (Kleinberg & Mullainathan, 2024) [[Link](https://openreview.net/forum?id=FGTDe6EA0B)]

**Week 4: The Limits of Statistical Learning**

  * *Topics:* Why exact learning is necessary for general intelligence; The fundamental misalignment between statistical learning and sound reasoning.
  * *Readings:*
      * "Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence" 
      (György, Lattimore, Lazić, Szepesvári, 2025) [[Link](https://arxiv.org/abs/2506.23908)]

-----

#### Part II: Student-Led Research Seminar Readings

Student pairs will select from the following papers for their presentations.

  * **Theme: Hardness & Limitations**

      * "Hardness of Learning Fixed Parities with Neural Networks" (Shoshani & Shamir, 2025)
      * "On Limitations of the Transformer Architecture" (Peng, Narayanan, & Papadimitriou, 2024)
      * "Frontier LLMs Still Struggle with Simple Reasoning Tasks" (Malek, Ge, Lazic, Jin, György, Szepesvári, 2025)

  * **Theme: Generalization**

      * "Understanding Generalization in Transformers: Error Bounds and Benign Overfitting" (Li et al., 2025)
      * "What Algorithms can Transformers Learn? A Study in Length Generalization" (Zhou et al., 2024)
      * "Universal Length Generalization with Turing Programs" (Hou et al., 2024)

  * **Theme: Algorithmic Solutions**

      * "From Reasoning to Super-Intelligence: A Search-Theoretic Perspective" (Shalev-Shwartz & Shashua, 2025)
      * "Learning Compositional Functions with Transformers from Easy-to-Hard Data" (Wang et al., 2025)
      * "Learning to Add, Multiply, and Execute Algorithmic Instructions Exactly with Neural Networks" (Frye et al., 2025)
