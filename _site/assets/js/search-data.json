{"0": {
    "doc": "About",
    "title": "About",
    "content": "# CMPUT 654: Theoretical Foundations of Machine Learning F2023 Following Tong Zhang's [book](https://tongzhang-ml.org/lt-book/lt-book.pdf), students will be introduced to the basic tools required to understand foundational results in learning theory and will get them prepared to read and understand a large portion of learning theory papers. The focus is on the the statistical approach to learning: The contents of the first 12 chapters from Tong Zhang's book. ## Pre-requisites Students are expected to follow (and even enjoy) mathematical proofs, deal with expressions involving probabilities and have a working knowledge of calculus and linear algebra. Basic probability, linear algebra and convex optimization is covered in Chapters 2, 3, 5, 7, 26, and 38 of the [Bandit Algorithms book](https://tor-lattimore.com/downloads/book/book.pdf). One very nice book that covers more, but is still highly recommended is [A Second Course in Probability Theory](http://people.bu.edu/pekoz/A_Second_Course_in_Probability-Ross-Pekoz.pdf). The book is available online and also in book format. Chapters 1, 3, 4, and 5 are most useful from here. ## Instructor - [Csaba Szepesv&aacute;ri](https://sites.ualberta.ca/~szepesva) ## Lecture Time Monday and Wednesdays from 3:30 PM - 4:50 PM (MST) in T 1-100. ### Office Hours We use slack for discussion of short questions. If more time is needed, arrange it with the instructor using slack. ## Slack Channel We will use Slack for everything. We have a separate workspace to discuss all topics related for this course. If you would like to join the channel please message the instructor. All announcements will be made on this channel. We strongly encourage all students to ask questions regarding course content on the Slack channel. ## Lectures Notes The lecture notes for this year's class are under the heading **LECTURE NOTES**. **Keywords:** Theory, Machine learning, Statistical learning, Concentration inequalities, Uniform deviation bounds ",
    "url": "/pages/about/",
    
    "relUrl": "/pages/about/"
  },"1": {
    "doc": "The work you do",
    "title": "The work you do",
    "content": "# The Work You Do | Component | Weight | Deadline | PDF and LaTex |:---|:---|:---------|:----| Assignment 1 | 10% | September 24, 2023 11:55pm | [PDF](/documents/assignments/fall_2024/assignment1.pdf) and [LaTex](/documents/assignments/fall_2024/assignment1.tex) | Assignment 2 | 10% | October 8, 2023 11:55pm | [PDF](/documents/assignments/fall_2024/assignment2.pdf) and [LaTex](/documents/assignments/fall_2024/assignment2.tex) | Midterm | 20% | October 15, 2023 11:55pm | [PDF](/documents/assignments/fall_2024/midterm.pdf) and [LaTex](/documents/assignments/fall_2024/midterm.tex)| Project (Proposal) | 10% | October 29, 2023 11:55pm || Assignment 3 | 10% | November 12, 2023 11:55pm | [PDF](/documents/assignments/fall_2024/assignment3.pdf) and [LaTex](/documents/assignments/fall_2024/assignment3.tex) | Assignment 4 | 10%| November 26, 2023 11:55pm |[PDF](/documents/assignments/fall_2024/assignment4.pdf) and [LaTex](/documents/assignments/fall_2024/assignment4.tex) | Project (Presentation) | 10% | December 5 and 7, 2023 (in class)|| Project (Report) | 20% | December 10, 2023 11:55pm || --- # Late Policy Late submissions will NOT be accepted, but to allow for flexibility in case of issues for any reason to submit solutions in time, 10% of the mark can be shifted from any component to the midterm. # Course Project ## Deadlines - Proposal: October 29, 2023 11:55pm - Presentations: December 5 and 7, 2023, in class - Report: December 10, 2023 11:55pm ## Objective and evaluation The project should be done individually or in a group of two (maximum group size of two). The goal is for everyone to get a taste of how it is to work on theoretical aspects of machine learning. In the project, you do not actually need to produce research paper quality results (although if you do, no one will complain!). It is sufficient to demonstrate a **thorough understanding** of some aspect of the theory literature, such as: - What are the interesting questions to ask (and what are less interesting questions?) - What is known about a given topic (and what is not known)? - Sorting out whether some assumption is critical for some result (or not). When evaluating the reports, we will not care that much about originality (new results) than coherence, soundness and the quality of writing. In fact, a typical report is expected to be a readable (and possibly entertaining) summary of a topic in the area. Reports that contain original results are also welcome, just to earn full grade, originality is absolutely not required. {: .text-center} *We strongly recommend to start small: Aim for writing a review of some results of interest. If time permits and as you feel fit, add new results.* {: .text-left} Having said this, if you score a new result early on, it is also OK to start on writing that result down. ## How to choose a topic? 1. Choose a theory paper and rewrite it to make it better. Choose and pick of what you include in your report. It may be better proofs. It may be better exposition of the results. Be critical about assumptions (but not overly critical). It may be putting the results into a perspective. Aim for readable (but technically correct) writeups. 2. Choose a problem that you care about in the area. Ask what is known. Write a summary about it. Be specific about what problems you are writing about. If time permits and with some luck, add new results. Aim for small things, like, such and such is known in topic A but only under condition B. Do these results extend to condition C? What conditions are necessary? How about slightly changing the problem, adding or removing a condition? 3. Choose an open question and try to answer it. When there is an upper bound, ask whether there is a matching lower bound. If not quite, try to reduce the gap. Ditto for lower bounds. Any time you see a bound you can ask: Is this tight? 4. It is a bit more risky, but possibly more rewarding, is to choose a non-theory paper and look at it through the eyes of a theoretician. Are there any hard claims that could be formulated (and possibly proved) in the context of the paper? If the paper is proposing algorithms, are there *any* conditions when the algorithm proposed will work \"well\"? How well? Put the results into the context of what is known. ## Formatting The reports should be typeset in latex and sent as a pdf document. **The template is available [here](/documents/misc_files/project_template.tex)**. The report should be maximum 9 pages long, the proposal maximum 2 pages long. They should have the standard structure: - Introduction (what is the problem studied, why do we study it) - Results (the \"meat\") - Conclusions/summary (what did we learn? what is the short take-away from all of this? what's next if anything?) ## Examples of topics TBA ",
    "url": "/pages/assignments/",
    
    "relUrl": "/pages/assignments/"
  },"2": {
    "doc": "Home",
    "title": "Home",
    "content": "# Welcome This is the homepage of the course: **Theoretical Foundations of Machine Learning** taught by [Csaba Szepesv&aacute;ri](https://sites.ualberta.ca/~szepesva/) at the [University of Alberta](https://www.ualberta.ca/). Additional information and resources can be accessed from the sidepane on the left. The main website pages are under the heading **PAGES** and the course notes for this year are organized under the heading **LECTURE NOTES**. The course notes from previous years are in the headings below. | This will be good! . | . ",
    "url": "/",
    
    "relUrl": "/"
  },"3": {
    "doc": "Lectures",
    "title": "Recorded lectures and scribes",
    "content": "| # | Date | Lecture Videos | Scribes | Status | . | 1. | Sept 5, 2023 | Introduction | pdf | V0.5 | . | 2. | Sept 7, 2023 | Concentration of Measure | pdf | V0.1 | . | 3. | Sept 12, 2023 | Concentration of Measure | pdf | V0.1 | . | 4. | Sept 14, 2023 | PAC-Learnability | pdf | V0.1 | . | 5. | Sept 19, 2023 | PAC-Learnability | pdf | V0.1 | . | 6. | Sept 21, 2023 | Empirical Processes | pdf | V0.1 | . | 7. | Sept 26, 2023 | Empirical Processes | pdf | V0.1 | . | 8. | Sept 28, 2023 | Uniform Bernstein | pdf | V0.1 | . | 9. | Oct 3, 2023 | Symmetrization Lemma | pdf | V0.1 | . | 10. | Oct 5, 2023 | Empirical Covering Numbers | pdf | V0.1 | . | 11. | Oct 10, 2023 | — | pdf | V0.1 | . | 12. | Oct 12, 2023 | — | pdf | V0.1 | . | 13. | Oct 17, 2023 | — | pdf | V0.1 | . | 14. | Oct 19, 2023 | — | pdf | V0.1 | . ",
    "url": "/pages/lectures/#recorded-lectures-and-scribes",
    
    "relUrl": "/pages/lectures/#recorded-lectures-and-scribes"
  },"4": {
    "doc": "Lectures",
    "title": "Lectures",
    "content": " ",
    "url": "/pages/lectures/",
    
    "relUrl": "/pages/lectures/"
  }
}
